[
  {
    "source_id": "doc-en-short",
    "language": "en",
    "text": "Short English paragraph about backlog items and decisions."
  },
  {
    "source_id": "doc-en-long",
    "language": "en",
    "text": "This is a comprehensive English document designed to test the chunking and tokenization capabilities of the embedding pipeline. The document contains multiple paragraphs with varied sentence structures, technical terminology, and different types of content to ensure robust testing across diverse text patterns. In software development, particularly in agile methodologies, maintaining a well-structured backlog is crucial for project success. The backlog serves as a prioritized list of features, user stories, and technical tasks that guide the development team's efforts. Each item in the backlog should be clearly defined with acceptance criteria, estimated effort, and business value. The process of backlog refinement involves regular review sessions where the team collaborates to break down large epics into smaller, manageable user stories. This iterative approach ensures that the development work remains aligned with business objectives and user needs. Technical debt, another important consideration in backlog management, represents the implied cost of additional rework caused by choosing an easy solution now instead of using a better approach that would take longer. Managing technical debt requires careful balance between delivering new features and maintaining code quality. The embedding pipeline we're testing here plays a crucial role in enabling semantic search capabilities across backlog items, allowing teams to quickly find related work items, identify potential duplicates, and discover relevant historical decisions. Vector embeddings capture semantic meaning in high-dimensional space, enabling similarity comparisons that go beyond simple keyword matching. The chunking strategy must handle various text types effectively, from brief user story descriptions to detailed technical specifications and architectural decision records. Token budgets ensure that text segments fit within model constraints while preserving semantic coherence. This comprehensive test document validates that our pipeline can handle realistic content volumes and complexity patterns encountered in actual backlog management scenarios."
  },
  {
    "source_id": "doc-cjk",
    "language": "cjk",
    "text": "你好世界再來一句測試段落。你好世界再來一句測試段落。你好世界再來一句測試段落。"
  },
  {
    "source_id": "doc-mixed",
    "language": "mixed",
    "text": "Decision: Keep local-first approach for backlog management. 你好世界，这是一个混合语言的测试文档。This sentence mixes English and CJK characters to test tokenization. 日本語のテストも含まれています。再來一句中文測試。The embedding pipeline should handle multilingual content gracefully, preserving semantic relationships across language boundaries."
  },
  {
    "source_id": "doc-punct",
    "language": "mixed",
    "text": "Punctuation-heavy text!!! Does it split? Yes... maybe; maybe not: test_case_123."
  },
  {
    "source_id": "doc-code",
    "language": "code",
    "text": "def index_document(source_id: str, text: str, config: PipelineConfig) -> IndexResult:\n    \"\"\"Index a single document through the complete embedding pipeline.\n    \n    Args:\n        source_id: Unique identifier for the document\n        text: Raw text content to index\n        config: Pipeline configuration with chunking, tokenizer, embedding, vector settings\n        \n    Returns:\n        IndexResult with telemetry data\n    \"\"\"\n    if not source_id:\n        raise ValueError(\"source_id must be non-empty\")\n    \n    # Resolve components from config\n    tokenizer = resolve_tokenizer(config.tokenizer.adapter, config.tokenizer.model)\n    embedder = resolve_embedder({\n        \"provider\": config.embedding.provider,\n        \"model\": config.embedding.model,\n        \"dimension\": config.embedding.dimension,\n        **config.embedding.options\n    })\n    \n    # Process the text through chunking and embedding pipeline\n    chunks = chunk_text(source_id, text, config.chunking)\n    results = []\n    for chunk in chunks:\n        embedding = embedder.embed(chunk.text)\n        results.append(VectorChunk(chunk_id=chunk.chunk_id, text=chunk.text, vector=embedding.vector))\n    \n    return IndexResult(chunks_count=len(results), backend_type=config.vector.backend)"
  }
]
