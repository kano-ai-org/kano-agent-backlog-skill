{"adr/ADR-0001_backlog-structure-and-moc":{"title":"Backlog structure: per-type folders and Obsidian MOC","links":["items/task/0000/KABSD-TSK-0001_project-backlog-skill"],"tags":[],"content":"Decision\nAdopt per-type folders for backlog items and use Obsidian-style MOC (manual links),\r\nwith Dataview as a supplemental auto list. Keep index files at the Epic level\r\nonly to reduce file count. Bucket items by ID range (per 100) under each type\r\nto reduce large folder sizes.\nContext\nThe backlog system needs to be readable without heavy tooling and should render\r\nclearly in Obsidian. DataviewJS was not reliable in the current setup.\nLinks\n\nRelated: KABSD-TSK-0001 Create project-backlog skill\n\nOptions Considered\n\nFlat items/ folder + DataviewJS-only index\nPer-type folders + DataviewJS index\nPer-type folders + manual MOC links + Dataview (non-JS) lists\n\nPros / Cons\n\nOption 1: simple path; but JS rendering was unreliable.\nOption 2: clearer organization; still depends on DataviewJS.\nOption 3: stable MOC links (Graph-friendly) and optional Dataview lists; slightly more manual upkeep.\n\nConsequences\n\nEpic index files must be updated when items move or are renamed.\nFeature/UserStory rely on Epic MOC links instead of their own index files.\nItem files live under _kano/backlog/items/&lt;type&gt;/&lt;bucket&gt;/ to avoid large directories.\nGraph view will reflect MOC link structure.\n\nFollow-ups\n\nKeep skills/kano-agent-backlog-skill/SKILL.md aligned with this structure.\n"},"adr/ADR-0002_decisions-as-adr-links":{"title":"Decision handling: ADRs stay in decisions/ with item links","links":["items/feature/0000/KABSD-FTR-0001_local-backlog-system"],"tags":[],"content":"Decision\nKeep decisions as ADR documents under _kano/backlog/decisions/ and link them\r\nfrom related work items via the decisions frontmatter field and a Links entry.\r\nDo not model ADRs as backlog work items for now.\nContext\nWe discussed whether decisions should be treated as work items. The current\r\nbacklog uses ADRs to capture durable rationale without turning them into\r\nworkflow tickets. We want to preserve clarity while avoiding ticket sprawl.\nLinks\n\nRelated: KABSD-FTR-0001 Local-first backlog system\n\nOptions Considered\n\nKeep ADRs as separate docs under decisions/ and link from items.\nTreat decisions as a new work item type under items/ with states.\nHybrid: keep ADRs under decisions/ plus optional lightweight decision tickets.\n\nPros / Cons\n\nOption 1: clear separation and low overhead; requires linking discipline.\nOption 2: full backlog visibility; risks turning decisions into ticket noise.\nOption 3: flexible; extra ceremony and more items to maintain.\n\nConsequences\n\nADRs remain outside the work item state machine.\nWork items should record ADR IDs in decisions: [] and Links.\nDashboards may need a separate decision view if visibility is required.\n\nFollow-ups\n\nUpdate related items to link ADR-0002.\n"},"adr/ADR-0003-appendix_collision-report-cli-spec":{"title":"ADR-0003-appendix_collision-report-cli-spec","links":[],"tags":[],"content":"Collision Report &amp; Resolver CLI\nID collision reporting and resolver CLI tool specifications\nOverview\nProvides two tools:\n\nworkitem_collision_report.py - Scan and report display ID collisions\nworkitem_resolve_ref.py - Interactive reference resolution\n\nworkitem_collision_report.py\nFeatures\nScans all items under _kano/backlog/items/ to find duplicate display id cases.\nUsage\n# Basic report\npython scripts/backlog/workitem_collision_report.py\n \n# JSON output\npython scripts/backlog/workitem_collision_report.py --format json\n \n# Show collisions only\npython scripts/backlog/workitem_collision_report.py --collisions-only\n \n# Specify backlog path\npython scripts/backlog/workitem_collision_report.py --backlog-root _kano/backlog\nOutput example\nID Collision Report\r\n===================\r\nGenerated: 2026-01-06 01:30\r\nScanned: 85 items\r\n\r\nCollisions Found: 1\r\n\r\nID: KABSD-TSK-0100 (2 items)\r\n  1. 019473f2 | Task | Done  | First implementation | items/tasks/0000/...\r\n  2. 01947428 | Task | New   | Second attempt      | items/tasks/0000/...\r\n  Suggestion: Use KABSD-TSK-0100@019473f2 or KABSD-TSK-0100@01947428\r\n\r\nNo other collisions found.\n\nJSON output format\n{\n  &quot;generated&quot;: &quot;2026-01-06T01:30:00&quot;,\n  &quot;total_items&quot;: 85,\n  &quot;collision_count&quot;: 1,\n  &quot;collisions&quot;: [\n    {\n      &quot;id&quot;: &quot;KABSD-TSK-0100&quot;,\n      &quot;items&quot;: [\n        {\n          &quot;uid&quot;: &quot;019473f2-79b0-7cc3-98c4-dc0c0c07398f&quot;,\n          &quot;uidshort&quot;: &quot;019473f2&quot;,\n          &quot;type&quot;: &quot;Task&quot;,\n          &quot;state&quot;: &quot;Done&quot;,\n          &quot;title&quot;: &quot;First implementation&quot;,\n          &quot;path&quot;: &quot;items/tasks/0000/KABSD-TSK-0100_first-impl.md&quot;\n        },\n        {\n          &quot;uid&quot;: &quot;01947428-1234-7abc-5678-def012345678&quot;,\n          &quot;uidshort&quot;: &quot;01947428&quot;,\n          &quot;type&quot;: &quot;Task&quot;,\n          &quot;state&quot;: &quot;New&quot;,\n          &quot;title&quot;: &quot;Second attempt&quot;,\n          &quot;path&quot;: &quot;items/tasks/0000/KABSD-TSK-0100_second-attempt.md&quot;\n        }\n      ]\n    }\n  ]\n}\nworkitem_resolve_ref.py\nFeatures\nResolves reference strings with interactive disambiguation support.\nUsage\n# Resolve reference\npython scripts/backlog/workitem_resolve_ref.py KABSD-TSK-0059\n \n# Use uidshort for exact match\npython scripts/backlog/workitem_resolve_ref.py KABSD-TSK-0100@019473f2\n \n# Interactive mode\npython scripts/backlog/workitem_resolve_ref.py KABSD-TSK-0100 --interactive\n \n# Output format\npython scripts/backlog/workitem_resolve_ref.py KABSD-TSK-0059 --format path   # path only\npython scripts/backlog/workitem_resolve_ref.py KABSD-TSK-0059 --format json   # JSON format\npython scripts/backlog/workitem_resolve_ref.py KABSD-TSK-0059 --format uid    # uid only\nOutput examples\nUnique match:\nResolved: KABSD-TSK-0059\r\n\r\n  UID:      019473f2-79b0-7cc3-98c4-dc0c0c07398f\r\n  ID:       KABSD-TSK-0059\r\n  Type:     Task\r\n  State:    Done\r\n  Title:    ULID vs UUIDv7 comparison document\r\n  Path:     items/tasks/0000/KABSD-TSK-0059_ulid-vs-uuidv7-comparison.md\r\n  Created:  2026-01-06\r\n  Updated:  2026-01-06\n\nMultiple matches (interactive mode):\nAmbiguous: 2 items match &quot;KABSD-TSK-0100&quot;\r\n\r\n  # | UID (short) | Type | State | Title\r\n  --|-------------|------|-------|------\r\n  1 | 019473f2    | Task | Done  | First implementation\r\n  2 | 01947428    | Task | New   | Second attempt\r\n\r\nEnter number to select (or &#039;q&#039; to quit): 1\r\n\r\nSelected: KABSD-TSK-0100@019473f2\r\nPath: items/tasks/0000/KABSD-TSK-0100_first-impl.md\n\nImplementation notes\nShared module\n# scripts/backlog/lib/index.py\n \nclass BacklogIndex:\n    def __init__(self, backlog_root: str):\n        self.items = self._scan_items(backlog_root)\n        self._build_indexes()\n    \n    def get_by_uid(self, uid: str) -&gt; Optional[BacklogItem]: ...\n    def get_by_uidshort(self, prefix: str) -&gt; List[BacklogItem]: ...\n    def get_by_id(self, display_id: str) -&gt; List[BacklogItem]: ...\n    def get_collisions(self) -&gt; Dict[str, List[BacklogItem]]: ...\nIntegration with existing skill scripts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExisting scriptIntegration approachworkitem_update_state.pyUse resolve_ref to allow id@uidshort parametersworkitem_create.pyAuto-generate UUIDv7 uidview_generate.pyOptionally display uidshort\nDeliverables\n\n scripts/backlog/workitem_collision_report.py\n scripts/backlog/workitem_resolve_ref.py\n scripts/backlog/lib/index.py (shared module)\n Unit tests\n"},"adr/ADR-0003-appendix_id-resolver-spec":{"title":"ADR-0003-appendix_id-resolver-spec","links":[],"tags":[],"content":"ID Resolver Specification\nResolveRef() function specification - for resolving backlog item references\nOverview\nPer ADR-0003, all reference resolution must go through the Resolver to support:\n\nFull uid exact match\nuidshort prefix match\nDisplay id match (may return multiple items)\n\nResolveRef() function specification\nSignature\ndef resolve_ref(\n    ref: str,\n    index: BacklogIndex,\n    interactive: bool = False\n) -&gt; ResolveResult:\n    &quot;&quot;&quot;\n    Resolve a reference to one or more backlog items.\n    \n    Args:\n        ref: Reference string (uid, uidshort, id, or id@uidshort format)\n        index: Backlog index instance\n        interactive: If True, prompt user for disambiguation\n        \n    Returns:\n        ResolveResult with matched item(s) or error\n    &quot;&quot;&quot;\nInput formats\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFormatExampleDescriptionFull uid019473f2-79b0-7cc3-98c4-dc0c0c07398f36 chars, hyphensuidshort019473f28 hex charsDisplay idKABSD-TSK-0059Project prefix + type + numberid@uidshortKABSD-TSK-0059@019473f2Human-friendly format\nResolution logic\ndef resolve_ref(ref: str, index: BacklogIndex) -&gt; ResolveResult:\n    # 1. Check if ref is full uid (36 chars with hyphens)\n    if is_full_uid(ref):\n        item = index.get_by_uid(ref)\n        if item:\n            return ResolveResult(matches=[item], exact=True)\n        return ResolveResult(error=f&quot;UID not found: {ref}&quot;)\n    \n    # 2. Check if ref contains @uidshort (e.g., KABSD-TSK-0059@019473f2)\n    if &quot;@&quot; in ref:\n        id_part, uidshort = ref.split(&quot;@&quot;, 1)\n        matches = index.get_by_id(id_part)\n        matches = [m for m in matches if m.uid.startswith(uidshort)]\n        if len(matches) == 1:\n            return ResolveResult(matches=matches, exact=True)\n        elif len(matches) &gt; 1:\n            return ResolveResult(matches=matches, exact=False, \n                error=&quot;Multiple matches even with uidshort&quot;)\n        return ResolveResult(error=f&quot;No match for {ref}&quot;)\n    \n    # 3. Check if ref is uidshort (8 hex chars)\n    if is_uidshort(ref):\n        matches = index.get_by_uidshort(ref)\n        if len(matches) == 1:\n            return ResolveResult(matches=matches, exact=True)\n        elif len(matches) &gt; 1:\n            return ResolveResult(matches=matches, exact=False)\n        return ResolveResult(error=f&quot;uidshort not found: {ref}&quot;)\n    \n    # 4. Assume ref is display id\n    matches = index.get_by_id(ref)\n    if len(matches) == 1:\n        return ResolveResult(matches=matches, exact=True)\n    elif len(matches) &gt; 1:\n        return ResolveResult(matches=matches, exact=False)\n    \n    return ResolveResult(error=f&quot;ID not found: {ref}&quot;)\nOutput structure\n@dataclass\nclass ResolveResult:\n    matches: List[BacklogItem] = field(default_factory=list)\n    exact: bool = False\n    error: Optional[str] = None\n    \n@dataclass\nclass BacklogItem:\n    uid: str\n    id: str\n    uidshort: str  # derived from uid[:8]\n    type: str\n    title: str\n    state: str\n    path: str\n    created: str\n    updated: str\nIndex requirements\nResolver requires the following index query capabilities:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQueryMethodDescriptionuid -&gt; itemget_by_uid(uid)Unique matchuidshort -&gt; [items]get_by_uidshort(prefix)Prefix matchid -&gt; [items]get_by_id(id)May return multiple items\nIndex Schema (SQLite)\nCREATE TABLE items (\n    uid TEXT PRIMARY KEY,\n    id TEXT NOT NULL,\n    uidshort TEXT NOT NULL,  -- first 8 hex chars\n    type TEXT,\n    title TEXT,\n    state TEXT,\n    path TEXT UNIQUE,\n    created TEXT,\n    updated TEXT\n);\n \nCREATE INDEX idx_id ON items(id);\nCREATE INDEX idx_uidshort ON items(uidshort);\nDisambiguation\nWhen exact=False and multiple matches exist, output candidate list:\nMultiple matches for &quot;KABSD-TSK-0100&quot;:\r\n\r\n  # | ID              | UID (short)  | Type | State | Title\r\n---------------------------------------------------------------\r\n  1 | KABSD-TSK-0100 | 019473f2     | Task | Done  | First task\r\n  2 | KABSD-TSK-0100 | 01947428     | Task | New   | Second task\r\n\r\nEnter number to select, or use: KABSD-TSK-0100@019473f2\n\nCLI integration\n# Resolve and show item details\npython scripts/backlog/workitem_resolve_ref.py KABSD-TSK-0059\n \n# Resolve with uidshort hint\npython scripts/backlog/workitem_resolve_ref.py KABSD-TSK-0100@019473f2\n \n# Interactive mode\npython scripts/backlog/workitem_resolve_ref.py KABSD-TSK-0100 --interactive\n \n# Output format\npython scripts/backlog/workitem_resolve_ref.py KABSD-TSK-0059 --format json\npython scripts/backlog/workitem_resolve_ref.py KABSD-TSK-0059 --format path\nError handling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaseError messageUID not foundError: UID not found: {uid}ID not foundError: ID not found: {id}Multiple matchesAmbiguous: {count} items match &quot;{ref}&quot;. Use id@uidshort format.Invalid formatError: Invalid reference format: {ref}"},"adr/ADR-0003-appendix_migration-plan-uid":{"title":"ADR-0003-appendix_migration-plan-uid","links":[],"tags":[],"content":"Migration Plan: Add uid to Existing Items\nA migration plan to add a UUIDv7 uid field to existing backlog items.\nOverview\nExisting backlog items currently only have id (display ID). Per ADR-0003, each item needs a uid (UUIDv7) as the immutable primary key.\nKey Decisions (ADR-0003):\n\nuid format: UUIDv7 (RFC 9562)\nFilenames remain unchanged (&lt;id&gt;_&lt;slug&gt;.md)\nuid is added only in frontmatter\n\nMigration Steps\nPhase 1: Preparation\n\n\nBack up the current backlog\ncp -r _kano/backlog _kano/backlog_backup_$(date +%Y%m%d)\n\n\nEnsure a UUIDv7 library is available\n\nPython: uuid6 package or Python 3.12+ built-in\nInstall: pip install uuid6\n\n\n\nPhase 2: Migration script\nCreate scripts/backlog/migration_add_uid.py:\n#!/usr/bin/env python3\n&quot;&quot;&quot;\nAdd uid (UUIDv7) to existing backlog items.\n \nUsage:\n    python migration_add_uid.py --dry-run  # Preview changes\n    python migration_add_uid.py --apply    # Apply changes\n&quot;&quot;&quot;\nimport uuid6  # or uuid (Python 3.12+)\n \ndef generate_uid():\n    &quot;&quot;&quot;Generate a UUIDv7 string.&quot;&quot;&quot;\n    return str(uuid6.uuid7())\n \ndef extract_uidshort(uid: str, length: int = 8) -&gt; str:\n    &quot;&quot;&quot;Extract uidshort (first N hex chars, no hyphens).&quot;&quot;&quot;\n    return uid.replace(&quot;-&quot;, &quot;&quot;)[:length]\nScript capabilities:\n\nScan all .md files under _kano/backlog/items/\nParse frontmatter\nIf uid is missing, generate a UUIDv7 and add it\nUpdate the updated field\nWrite changes back to the file\n\nPhase 3: Handle parent/link references\nBackward-compatibility strategy (incremental):\n\nKeep the existing parent field (by display id)\nOptionally add parent_uid\nA Resolver tool maps parent to the actual uid\n\n# Before migration\nparent: KABSD-FTR-0042\n \n# After migration (backward compatible)\nparent: KABSD-FTR-0042\nparent_uid: 019473f2-79b0-7cc3-98c4-dc0c0c07398f  # optional\nPhase 4: Validation\n\nRun a verification script to ensure every item has a uid\nVerify uid format correctness (UUIDv7)\nVerify dashboards render correctly\n\nFrontmatter schema changes\nNew fields\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFieldTypeRequiredNotesuidstringRequired (post-migration)UUIDv7, immutableparent_uidstringOptionalParent item’s uidaliaseslistOptionalLegacy IDs or alternate names\nExample\n---\nid: KABSD-TSK-0059\nuid: 019473f2-79b0-7cc3-98c4-dc0c0c07398f\ntype: Task\ntitle: &quot;ULID vs UUIDv7 comparison document&quot;\nstate: Done\npriority: P3\nparent: KABSD-FTR-0042\nparent_uid: 019473e8-1234-7abc-5678-def012345678  # optional\n# ... rest of frontmatter\n---\nuidshort specification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPropertyValueLength8 charactersSourceFirst 8 hex characters of uid (no hyphens)Example019473f2UsageHuman-friendly reference KABSD-TSK-0059@019473f2\nRisks / Mitigations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRiskMitigationMigration failure causing data corruptionBack up first; provide a dry-run modeuid collisionsExtremely unlikely with UUIDv7; add validation checksTool incompatibilityBackward-compatible: keep parent, add parent_uid\nImplementation order\n\n Create migration_add_uid.py\n Test in sandbox\n Dry-run preview\n Back up and perform migration\n Verify results\n Update related tools to support uid resolution\n"},"adr/ADR-0003-appendix_ulid-vs-uuidv7-comparison":{"title":"ADR-0003-appendix_ulid-vs-uuidv7-comparison","links":[],"tags":[],"content":"ULID vs UUIDv7 Comparison\nTechnical comparison to inform ADR-0003 uid format choice\nSummary\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristicULIDUUIDv7Length128 bits128 bitsString length26 chars (Base32)36 chars (hex + hyphens)Timestamp48 bits (ms)48 bits (ms)Random part80 bits74 bits (minus version/variant)StandardizationCommunity conventionIETF RFC 9562OrderingLexicographically sortableLexicographically sortableReadabilityShorter, no hyphensStandard UUID format\nDetailed comparison\n1. Format and readability\nULID\n01AN4Z07BY79KA1307SR9X4MV3\r\n|----------|----------------|\r\n Timestamp    Randomness\r\n  (10 ch)      (16 ch)\n\n\nUses Crockford’s Base32 (excludes I, L, O, U to avoid confusion)\nUppercase, no hyphens\n26 characters\n\nUUIDv7\n017f22e2-79b0-7cc3-98c4-dc0c0c07398f\r\n|-------|    |  |    |\r\n  time  ver  var  random\n\n\nStandard UUID format (8-4-4-4-12)\nHexadecimal with hyphens\n36 characters\n\n2. Ordering characteristics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAspectULIDUUIDv7Lexicographic ordering✅ Fully supported✅ Fully supportedSame-millisecond orderingMonotonic incrementCounter/randomCross-machine orderingTime precision onlyTime precision only\nBoth reflect time order correctly under lexicographic sort.\n3. Collision safety\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAspectULIDUUIDv7Random entropy80 bits~74 bitsSame-ms collision rate2^-802^-74Theoretical securityExtremely highExtremely high\nIn practice, both have negligible collision probability.\n4. Library support\nULID\n\nPython: python-ulid, ulid-py\nJavaScript: ulid (official)\nGo: oklog/ulid\nCommunity-driven, broad multi-language coverage\n\nUUIDv7\n\nPython: uuid6 (backport for &lt;3.x), built-in planned in Python 3.12+\nJavaScript: uuid@9+\nGo: google/uuid\nIETF standardized (RFC 9562); mainstream UUID libraries are adding support\n\n5. uidshort prefix length guidance\nULID\n\nFirst 10 characters = timestamp part\nRecommended uidshort: 8-10 characters (covers time + partial randomness)\nExample: 01AN4Z07BY → 8 characters 01AN4Z07\n\nUUIDv7\n\nFirst 8 characters (no hyphens) = high bits of timestamp\nRecommended uidshort: 8-12 characters (hex)\nExample: 017f22e2-79b0-7... → 8 characters 017f22e2\n\nRecommendation\nInitial analysis recommendation: ULID\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvantageNotesShorter26 vs 36 characters; cleaner filenamesMore readableNo hyphens; visually cleanerSufficient entropy80-bit randomness; extremely low collision riskMature ecosystemYears of use; stable library supportFilename-friendlyNo special characters; compatible across filesystems\nAdvantages of UUIDv7\n\nStandardization: IETF RFC 9562; strong long-term stability\nCompatibility: Works with existing UUID infrastructure (e.g., database UUID columns)\nFuture-proofing: Python 3.12+ and mainstream libraries add native support\n\n\nFinal decision (2026-01-06)\nAdopt UUIDv7\nChoose UUIDv7 for these reasons:\n\nIETF standardization provides stronger long-term stability\nCompatible with existing UUID ecosystems\nNative support in mainstream languages is arriving\n\nuidshort length: 8 characters (hex prefix)\n\nExample: 017f22e2-79b0-7... → 017f22e2\n\nReferences\n\nULID Spec\nRFC 9562 - UUIDv7\npython-ulid\n"},"adr/ADR-0003_identifier-strategy-for-local-first-backlog":{"title":"Identifier strategy: sortable IDs without centralized allocation","links":["adr/ADR-0003-appendix_ulid-vs-uuidv7-comparison"],"tags":[],"content":"Decision\nKeep the backlog file-first (Markdown files in repo as the source of truth) and avoid requiring a\r\ncentralized server for identifier allocation.\nAdopt a hybrid identifier strategy:\n\nuid is the immutable primary key (globally unique). Format: UUIDv7 (36 chars, hex, 48-bit timestamp + 74-bit random, RFC 9562).\nid is a human-readable display ID (sortable, short), and may collide across machines/branches.\nFilenames must include uidshort to avoid git add/add conflicts when id collides.\nAny reference resolution using id must go through a resolver that can disambiguate.\n\nContext\nA strictly increasing counter (ID from 0..N) is convenient for sorting and scanning, but in a\r\nlocal-first workflow it becomes hard to guarantee uniqueness across machines/agents without either:\n\na centralized allocator (service/server/lock), or\ncoordination rules that prevent concurrent creation.\n\nUsing only UUIDs avoids collisions but loses the natural sortable sequence in filenames and dashboards.\r\nWe want to preserve “human-first” readability while keeping future options open for distributed collaboration.\nRequirements\n\nMultiple agents/machines can create items concurrently without a shared allocator.\nReferences should remain stable across renames/renumbers and across derived indexes (SQLite, embeddings).\nAgents should be able to answer “next / what to do next” and support triage using indexes.\n\nOptions Considered\n\nCentralized ID allocator (server or shared lock file)\nUUID-only IDs (globally unique, not naturally ordered)\nTime-sortable unique IDs (ULID / UUIDv7)\nHybrid IDs: keep sortable id + add immutable uid\n\nPros / Cons\n\nOption 1: strongest uniqueness; introduces infrastructure and single-point-of-failure; violates local-first.\nOption 2: simplest uniqueness; hurts readability and natural ordering.\nOption 3: unique + sortable; still less human-friendly than short sequential IDs; ecosystem differences.\nOption 4: keeps human-friendly filenames while enabling reliable uniqueness for merging/indexing; adds complexity.\n\nConsequences\n\nThe default workflow remains file-first and Obsidian-friendly.\n\nWork item frontmatter (minimum)\nThis is the target schema for distributed-safe identifiers (migration required; not implemented everywhere yet):\n\nRequired:\n\nuid: string (ULID or UUIDv7; immutable; unique)\nid: string (display ID; sortable; allowed to collide)\ntype, title, status/state, priority, created, updated\n\n\nRecommended:\n\ntags\nparent_uid (references use uid to avoid ambiguity)\nlinks_uid (same)\naliases (optional; for legacy IDs or future renumbering)\n\n\n\nFilename and path\nTo avoid git add/add conflicts when two branches create the same display id, filenames should be unique.\nCurrent format (sufficient for most cases):\n\n&lt;id&gt;_&lt;slug&gt;.md\nExample: KABSD-TSK-0100_implement-backlog-indexing.md\n\nSince the slug is derived from the title and describes the item’s purpose, collision risk is minimal in practice. Two items would need identical id AND identical slug to conflict.\nOptional extended format (for high-concurrency scenarios):\n\n&lt;id&gt;__&lt;uidshort&gt;_&lt;slug&gt;.md\nExample: KABSD-TSK-0100__01KE72EH4N_implement-backlog-indexing.md\n\nuidshort is a stable prefix (fixed length) derived from uid:\n\nULID: prefix of the ULID (timestamp portion is convenient)\nUUIDv7: prefix of the hex string (length TBD, e.g. 8-12)\n\nDecision (2026-01-06): Filename format remains unchanged (&lt;id&gt;_&lt;slug&gt;.md). The uid field is added to frontmatter only; renaming existing files is not required.\nResolver semantics (reference handling)\nTools must implement ResolveRef(ref):\n\nIf ref is a full uid → unique match.\nIf ref is a uidshort → resolve via index (uidshort -&gt; uid). If multiple matches, list candidates.\nIf ref is a display id (e.g. KANO-000123) → resolve via index (id -&gt; [uid...]):\n\none match: return it\nmultiple matches: list candidates for human selection (type/status/title/path/created/updated).\n\n\n\nRecommended human-friendly reference format to reduce ambiguity:\n\nKANO-000123@01KE72EH4N (display id + uidshort)\n\nIndex implications\nDerived indexes must support:\n\nuid -&gt; path\nuidshort -&gt; uid\nid -&gt; [uid...] (note: potentially multiple)\n\nWhat-next behavior (agent workflow)\nWhen asked “next / what to do next”:\n\nPrefer continuing items in progress.\nOtherwise pick from ready items (e.g. 3-5), ordered by priority then recency/parent context.\nOutput should include id@uidshort, title, type, status/state, priority, and a first actionable step.\n\nOpen Questions / Follow-ups\n\nChoose ULID vs UUIDv7 (sorting, readability, library support, collision safety, short-prefix length).\n\nResolved (2026-01-06): Use UUIDv7. See Comparison Document.\nRationale: IETF standardized (RFC 9562), native Python 3.12+ support planned, compatible with existing UUID infrastructure.\nuidshort length: 8 characters (hex prefix) recommended.\n\n\nMigration plan for existing id-only items (add uid to frontmatter, filenames unchanged).\nShould we store both parent (id) and parent_uid during migration, or hard cutover to uid?\nAdd a collision report (group by display id) and a resolver UI/CLI for disambiguation.\n"},"adr/ADR-0004_file-first-architecture-with-sqlite-index":{"title":"File-First Architecture with SQLite Index","links":["adr/ADR-0012_workset-db-canonical-schema-reuse","_meta/canonical_schema.sql","_meta/canonical_schema.json"],"tags":[],"content":"Decision\nUse a File-First architecture where Markdown files are the single source of truth, augmented by a local SQLite database acting as a disposable, read-optimized index.\n\nSource of Truth: Markdown files tracked in Git.\nDerived Index: Local SQLite database (_kano/backlog/_index/backlog.sqlite3).\nSync Direction: STRICTLY One-Way (File → DB). The DB is never the System of Record.\nPersistence: The SQLite file is not tracked in Git (should be .gitignored). It can be rebuilt from files at any time.\n\nContext\nAs the backlog grows, scanning hundreds or thousands of Markdown files for every query (e.g., “find all active tasks blocking feature X”) becomes too slow (O(N) IO operations).\nHowever, we want to maintain the benefits of text files:\n\nGit-friendly: Diff, merge, blame work natively.\nHuman-readable: Accessible without special tools.\nPortable: No database server dependency for basic access.\n\nWe need a solution that provides relational query speed (O(log N)) without compromising the file-centric workflow.\nDetailed Design\n1. Index Lifecycle\nThe index is a cache of the file state.\n\nBuild: Scan all .md files, parse frontmatter, insert into DB.\nUpdate: Check file mtime against DB record. Only re-parse modified files.\nRebuild: rm backlog.sqlite3 followed by a full build ensures 100% consistency.\n\n2. Database Schema\nThe SQLite schema is designed for query efficiency, not normalization rules that would apply to a primary store.\nitems Table\nCore metadata for filtering and sorting.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumnTypeDescriptionuidTEXT (PK)UUIDv7 (from frontmatter)idTEXTDisplay ID (e.g., KABSD-TSK-0049)typeTEXTWork item type (Feature, Task, etc.)stateTEXTCurrent state (New, InProgress, etc.)titleTEXTItem titlepathTEXTRelative path to file (unique)mtimeREALFile modification timestamp (for sync logic)content_hashTEXTHash of content (for change detection)frontmatterJSONFull frontmatter blob (flexibility)createdTEXTCreation dateupdatedTEXTLast updated date\nlinks Table\nTracks relationships for graph queries (e.g., “all children of X”).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumnTypeDescriptionsource_uidTEXTLink source (child)target_uidTEXTLink target (parent)typeTEXTLink type (e.g., “parent”, “relates_to”)\nembeddings Table (Future/Optional)\nStores vector embeddings for semantic search.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumnTypeDescriptionuidTEXTFK to items.uidchunk_indexINTSequence number of chunkembeddingBLOBFloat32 vector arraycontentTEXTChunk text content\n3. Sync Logic\nA sync script (e.g., update_index.py) runs:\n\nBefore complex operations (e.g., generate_view).\nPeriodically (if running as a daemon/watcher).\nOn-demand by user.\n\nresolve_ref and other CLI tools currently use an in-memory index (lib/index.py). They should be refactored to query SQLite when available for better scaling.\nTrade-offs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrade-offDescriptionLatencyDB state may lag behind file state until sync runs. Tooling must handle “dirty” reads or force sync.ComplexityMaintaining sync logic (especially incremental updates) adds code complexity vs raw file scan.SpaceDuplicates metadata in DB file (negligible for text backlogs).\nConsequences\n\nTooling Update: All queries (Dashboard generation, Reference resolution) should migrate to use SQLite for reads.\nGitignore: Ensure *.sqlite3 is ignored.\nResilience: Tools must degrade gracefully if DB is corrupt or missing (fallback to file scan or auto-rebuild).\nSchema Reuse: Per ADR-0012, this canonical schema is reused by workset DBs to avoid schema drift and maintain portable context.\n\nRelated\n\nCanonical Schema: See canonical_schema.sql and canonical_schema.json for the complete schema definition.\nWorkset Schema: See ADR-0012 for how workset DBs reuse this canonical schema.\n"},"adr/ADR-0004_per-product-isolated-index-architecture":{"title":"Per-Product Isolated Index Architecture","links":["items/feature/0000/KABSD-FTR-0010_monorepo-platform-migration","items/task/0000/KABSD-TSK-0084_update-indexer-and-resolver-for-product-isolation"],"tags":[],"content":"Context\nIn a multi-product monorepo environment, we needed to decide between two architectural approaches for the SQLite index:\n\nPlatform-level shared index: All products write to a single _kano/backlog/_index/backlog.sqlite3\nPer-product isolated index: Each product owns its own index at products/&lt;name&gt;/_index/backlog.sqlite3\n\nDecision Criteria\nWe evaluated 7 dimensions:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDimensionPer-ProductPlatformWinnerIsolationComplete separationShared namespacePer-ProductConcurrencyNo locking contentionPotential locksPer-ProductPerformancePredictable, isolatedCan degrade with loadPer-ProductScalabilityLinear per productShared bottleneckPer-ProductComplexitySimple, independentComplex coordinationPer-ProductMaintenanceEasy (per-product)Harder (shared state)Per-ProductFuture ExtensibilitySupports embedding DBDifficult to addPer-Product\nOverall Score: Per-Product = 91%, Platform = 43%\nDecision\nImplement per-product isolated SQLite indexes.\nEach product will:\n\nMaintain its own SQLite database at products/&lt;product-name&gt;/_index/backlog.sqlite3\nHave independent schema with product column for self-documentation\nSupport autonomous indexing/rebuilding without affecting other products\nEnable future cross-product aggregation via product-aware queries\n\nImplementation Details\nSchema Design\nAll tables include product column:\nCREATE TABLE items (\n  product TEXT NOT NULL,\n  id TEXT NOT NULL,\n  PRIMARY KEY (product, id),\n  ...\n);\n \nCREATE TABLE item_tags (\n  product TEXT NOT NULL,\n  item_id TEXT NOT NULL,\n  tag TEXT NOT NULL,\n  PRIMARY KEY (product, item_id, tag),\n  FOREIGN KEY (product, item_id) REFERENCES items(product, id)\n);\nIndex Rebuild Process\n\nscripts/indexing/build_sqlite_index.py --product &lt;name&gt;\nScans products/&lt;name&gt;/items/ directory\nExtracts product from file paths\nUpserts into product-specific SQLite database\nMaintains composite key integrity\n\nResolver Behavior\n\nresolve_ref(ref, index, product=None) accepts optional product filter\nIf product not specified, searches across all products\nProduct column enables cross-product queries when needed\n\nRationale\n\nIsolation ensures safety: No possibility of data leakage between products\nPredictable performance: Each product’s index grows independently\nSupports autonomy: Teams can rebuild their product’s index without coordination\nFuture-proof: Enables embedding databases per product or shared aggregation\nSimpler mental model: Products are truly independent\nBackward compatible: Product column facilitates future migrations\n\nAlternatives Considered\nPlatform-level shared index\n\nPros: Unified search across all products\nCons: Complex coordination, shared bottleneck, potential data leakage\nRejected: Architecture does not support team autonomy\n\nHybrid approach (shared metadata + per-product data)\n\nPros: Balanced complexity\nCons: Still requires coordination layer, adds complexity\nRejected: Per-product isolation is simpler and cleaner\n\nFuture Work\nWhen cross-product features are needed:\n\nCreate aggregation layer on top of per-product indexes\nOr implement global embedding database that reads from per-product sources\nProduct column in schema already prepared for this extensibility\n\nReferences\n\nKABSD-FTR-0010_monorepo-platform-migration.md: Monorepo Platform Migration feature\nKABSD-TSK-0084_update-indexer-and-resolver-for-product-isolation.md: Indexer and resolver product isolation\n"},"adr/ADR-0005_product-column-retention-rationale":{"title":"Product Column Retention in Per-Product Indexes","links":["items/feature/0000/KABSD-FTR-0010_monorepo-platform-migration","adr/ADR-0004_file-first-architecture-with-sqlite-index"],"tags":[],"content":"Context\nWhen implementing per-product isolated indexes, a question arose: if each product owns its own SQLite database, why include a product column in the schema?\nArguments for removing it:\n\nRedundant (the file path already indicates product)\nAdds storage overhead\nColumn values are always the same for a given database\n\nArguments for keeping it:\n\nSelf-documentation (data independence from file paths)\nConsistency with composite key patterns\nFuture extensibility (global embedding DB, cross-product queries)\nError detection capability\n\nDecision\nRetain the product column in all schema tables.\nEach product’s schema will include a product column despite the apparent redundancy.\nRationale\n1. Data Self-Documentation\nThe product column makes data self-describing. If a database dump or export is created, the product context is explicit in the data itself, not dependent on file path or metadata.\nSELECT * FROM items WHERE product=&#039;kano-agent-backlog-skill&#039;;\n-- vs.\nSELECT * FROM items;  -- How do you know which product this is from?\n2. Consistency with Composite Key Strategy\nUsing (product, id) as composite primary keys throughout the schema ensures:\n\nAll foreign keys are uniform: FOREIGN KEY (product, item_id)\nConsistent pattern across all tables (items, item_tags, item_links, worklog_entries)\nEasier code generation and migrations\n\n3. Uniform Schema Across Products\nAll products use identical schema structure. A tool that works with KABSD’s index works with any product’s index without modification. This is valuable for:\n\nShared tool development\nSchema migration scripts\nIndex validation and repair tools\n\n4. Future Extensibility: Global Embedding Database\nThe primary long-term value is supporting a future global embedding database:\n-- Global embedding store (future)\nCREATE TABLE embeddings (\n  product TEXT NOT NULL,\n  item_id TEXT NOT NULL,\n  embedding BLOB NOT NULL,\n  PRIMARY KEY (product, item_id),\n  FOREIGN KEY (product, item_id) REFERENCES all_items(product, id)\n);\nThis would aggregate embeddings from all products while maintaining product context. The product column enables this without schema rework.\n5. Error Detection\nIncluding product column in per-product indexes enables validation queries:\n-- Verify database integrity\nSELECT DISTINCT product FROM items;\n-- Should always return single row: kano-agent-backlog-skill\n \n-- Detect misplaced files\nSELECT product FROM items WHERE product != &#039;kano-agent-backlog-skill&#039;;\n-- Should return empty set\n6. Cross-Product Queries (Future)\nWhen analytics or reporting tools are built, they may aggregate across products:\n-- Future: Query across products via union or aggregation\nSELECT product, COUNT(*) as item_count FROM all_items GROUP BY product;\nThe product column is essential for this without painful migrations.\nImplementation\nAll tables maintain the pattern:\nCREATE TABLE items (\n  product TEXT NOT NULL,\n  id TEXT NOT NULL,\n  uid UUID,\n  type TEXT,\n  -- ... other columns\n  PRIMARY KEY (product, id)\n);\n \nCREATE TABLE item_tags (\n  product TEXT NOT NULL,\n  item_id TEXT NOT NULL,\n  tag TEXT NOT NULL,\n  PRIMARY KEY (product, item_id, tag),\n  FOREIGN KEY (product, item_id) REFERENCES items(product, id)\n);\nAlternatives Considered\nRemove product column entirely\n\nPros: Minimal storage, no apparent redundancy\nCons: Breaks future embedding DB design, harder to validate integrity, poor data self-documentation\nRejected: Future extensibility cost is too high\n\nMake product optional/nullable\n\nPros: Allows querying “which product” implicitly\nCons: Makes schema ambiguous, difficult to enforce consistency\nRejected: Product should always be explicit and required\n\nFuture Work\nWhen global embedding database is implemented:\n\nProduct column in schema already prepared\nNo schema migration required\nTools can immediately work with cross-product data\n\nReferences\n\nKABSD-FTR-0010_monorepo-platform-migration.md: Monorepo Platform Migration\nADR-0004_file-first-architecture-with-sqlite-index: Per-Product Isolated Index Architecture\n"},"adr/ADR-0005_skill-versioning-and-release-policy":{"title":"Skill Versioning and Release Policy","links":[],"tags":[],"content":"Decision\nAdopt a SemVer-inspired versioning policy for kano-agent-backlog-skill with a clear pre-1.0 roadmap:\n\nUse Git tags as the source of truth for released versions: vX.Y.Z.\nWhile &lt;1.0.0, treat releases as milestones and allow faster iteration, but still:\n\nuse Z for bugfixes and non-breaking changes,\nuse Y when we introduce intentional breaking changes (schema/CLI/layout).\n\n\nAfter 1.0.0, follow SemVer strictly:\n\nZ patch = backward-compatible bugfix only\nY minor = backward-compatible feature + optional deprecations\nX major = breaking changes (must provide migration guidance)\n\n\n\nContext\nThis repo is a demo host and development environment for an open-source skill. We need a predictable way to:\n\ncommunicate what changed,\ndecide when changes are breaking,\nalign backlog milestones with releases,\nkeep multi-agent usage stable across time.\n\nDefinitions (what counts as breaking)\nBreaking changes include (non-exhaustive):\n\nFrontmatter schema: renaming/removing required keys, changing meaning of state groups, changing defaults that alter workflow rules.\nPath/layout: moving the canonical backlog root (_kano/backlog/**), changing bucket rules, changing decisions/items separation.\nScript CLI: removing/renaming flags, changing required flags, changing default behavior that affects output determinism.\nConfig schema: renaming/removing keys under _kano/backlog/_config/config.json.\nGenerated output contracts: changing canonical dashboard filenames or section/group meaning.\n\nNon-breaking changes include:\n\nadding optional keys or sections,\nadding new scripts (without changing existing CLI),\nstrengthening validation with clearer error messages (unless it blocks previously valid projects).\n\nRelease artifacts (minimum)\nFor each release tag:\n\nUpdate the skill docs (README/REFERENCE) to match reality.\nEnsure canonical scripts work end-to-end:\n\nscripts/backlog/view_refresh_dashboards.py\nscripts/backlog/view_generate_demo.py (demo dashboards)\nscripts/backlog/workitem_update_state.py\n\n\nEnsure demo views are regenerated.\n\nOptional (recommended as we approach 0.1.0+):\n\nCHANGELOG.md in the skill repo (high-level, human readable).\nA short “upgrade notes” section when there is any migration required.\n\nMilestone mapping (demo backlog)\nWe track releases as milestone Epics:\n\nKABSD-EPIC-0002 = v0.0.1 (core demo)\nKABSD-EPIC-0003 = v0.0.2 (indexing + resolver)\n\nFuture guideline:\n\nPatch releases (v0.0.(Z+1)) do not require new Epics; they are small fixes folded into the current milestone Epic.\nMinor bump in pre-1.0 (v0.(Y+1).0) should have a dedicated milestone Epic if it introduces breaking changes.\n\nConsequences\n\nWe will treat “schema/CLI/layout” changes as versioned contracts.\nEach “milestone epic” must have acceptance criteria that match a release outcome (taggable state).\nWhen breaking changes are introduced, the release must include clear migration guidance (script or documented steps).\n"},"adr/ADR-0006_multi-product-directory-structure":{"title":"Multi-Product Directory Structure and Naming Conventions","links":["items/feature/0000/KABSD-FTR-0010_monorepo-platform-migration","items/task/0000/KABSD-TSK-0081_execute-directory-restructuring-for-monorepo-platform","adr/ADR-0016_per-product-isolated-index-architecture"],"tags":[],"content":"Context\nA monorepo containing multiple independent products (skills) needs a consistent directory layout that:\n\nKeeps products isolated from each other\nSupports independent configuration and indexing\nAllows shared tools and metadata at the project level\nScales to many products without directory explosion\nMaintains backward compatibility during migration\n\nCompeting Designs\n\n\nProject + Products model (chosen):\n_kano/backlog/\r\n  products/&lt;product-name&gt;/\r\n    _config/\r\n    items/\r\n    decisions/\r\n    views/\r\n  _shared/\r\n    defaults.json\n\n\n\nFlat product namespacing:\n_kano/backlog/\r\n  items/&lt;product&gt;-&lt;type&gt;/\r\n  decisions/&lt;product&gt;/\r\n  views/&lt;product&gt;/\n\n\n\nSingle root (pre-migration):\n_kano/backlog/\r\n  items/\r\n  decisions/\r\n  views/\n\n\n\nDecision\nImplement Project + Products hierarchical model.\nDirectory structure:\n_kano/backlog/                          # Project root\r\n├── products/                           # Product container\r\n│   ├── kano-agent-backlog-skill/       # First product\r\n│   │   ├── _config/\r\n│   │   │   └── config.json             # Product-specific config\r\n│   │   ├── items/                      # Product&#039;s backlog items\r\n│   │   │   ├── epics/0000/\r\n│   │   │   ├── features/0000/\r\n│   │   │   ├── userstories/0000/\r\n│   │   │   ├── tasks/0000/\r\n│   │   │   ├── tasks/0100/             # Buckets per 100 items\r\n│   │   │   └── bugs/0000/\r\n│   │   ├── decisions/                  # Product&#039;s ADRs\r\n│   │   ├── views/                      # Product&#039;s dashboards\r\n│   │   ├── _index/\r\n│   │   │   └── backlog.sqlite3         # Product-isolated index\r\n│   │   └── _meta/\r\n│   │       ├── schema.md\r\n│   │       ├── conventions.md\r\n│   │       └── indexes.md              # Epic index registry\r\n│   │\r\n│   └── kano-commit-convention-skill/   # Second product\r\n│       └── ... (same structure)\r\n│\r\n├── sandboxes/                          # Isolated test/demo environments\r\n│   ├── kano-agent-backlog-skill/       # Can test schema changes here\r\n│   └── kano-commit-convention-skill/\r\n│\r\n├── _shared/                            # Project-level shared data\r\n│   ├── defaults.json                   # { &quot;default_product&quot;: &quot;...&quot; }\r\n│   └── config_template.json            # Shared config seed\r\n│\r\n├── _meta/                              # Project metadata (if needed)\r\n├── _index/                             # Project index (optional, future)\r\n├── views/                              # Project-level dashboards\r\n│   ├── Dashboard_PlainMarkdown_Active.md\r\n│   ├── Dashboard_PlainMarkdown_New.md\r\n│   └── Dashboard_PlainMarkdown_Done.md\r\n└── _logs/                              # Audit logs (project-level)\r\n    └── agent_tools/\r\n        └── tool_invocations.jsonl\n\nRationale\n1. Isolation and Autonomy\nProducts live under products/&lt;name&gt;/:\n\nTeam A manages products/product-a/\nTeam B manages products/product-b/\nNo namespace collisions, no coordination needed\nClear ownership boundaries\n\n2. Scalability\nHierarchical structure scales linearly:\n\n2 products: 2 directories\n10 products: 10 directories\n100 products: 100 directories\nNo explosion of files at top level\n\n3. Unified Schema Across Products\nEach product has identical internal structure:\n\nAll products use items/, decisions/, views/, _config/, _meta/\nTools can be generic: “for each product, scan items/”\nReduces special-case logic in scripts\n\n4. Backward Compatibility\nExisting KABSD backlog migrates as:\n\nOld: _kano/backlog/items/task/0000/KABSD-TSK-0007.md\nNew: _kano/backlog/products/kano-agent-backlog-skill/items/task/0000/KABSD-TSK-0007.md\n\nPath change is clean; no file modifications required. Git correctly tracks as renames.\n5. Per-Product Isolation in Indexing\nEach product gets:\n\nOwn SQLite database: products/&lt;name&gt;/_index/backlog.sqlite3\nOwn metadata: products/&lt;name&gt;/_meta/indexes.md\nOwn config: products/&lt;name&gt;/_config/config.json\n\nRebuild product A’s index without touching product B.\n6. Flexible Sandboxing\nsandboxes/&lt;product-name&gt;/ allows safe testing:\n\nDevelop schema changes on test data\nRun migration scripts without affecting production backlog\nEasy cleanup: just delete sandbox directory\n\n7. Project-Level Aggregation (Future)\n_shared/ and _index/ support future features:\n\nGlobal embedding database\nCross-product analytics dashboards\nUnified search index (optional, opt-in)\n\nProducts remain independent; project layer is additive.\nImplementation\nPath Resolution\nAll scripts use context.py for product-aware resolution:\nfrom context import get_product_root, get_items_dir\n \nproduct_name = args.product or os.getenv(&quot;KANO_PRODUCT&quot;) or defaults[&quot;default_product&quot;]\nproduct_root = get_product_root(product_name)  # _kano/backlog/products/&lt;name&gt;\nitems_dir = get_items_dir(product_name)        # products/&lt;name&gt;/items\nConfiguration\n\nProject level: _kano/backlog/_shared/defaults.json (default product)\nProduct level: products/&lt;name&gt;/_config/config.json (product-specific)\n\nFallback chain:\n\nCLI --product flag\nKANO_PRODUCT environment variable\nProduct embedded in filename (e.g., task parsing)\ndefaults.json default_product\nHardcoded fallback: “kano-agent-backlog-skill”\n\nCLI Integration\nAll major scripts accept --product flag:\nscripts/backlog/workitem_create.py --product kano-agent-backlog-skill --type task --title &quot;...&quot;\nscripts/backlog/index_db.py --product kano-commit-convention-skill\nAlternatives Considered\nFlat namespacing\nitems/kabsd-tasks/0000/\r\nitems/kccs-features/0000/\n\n\nCons: No clear product boundaries, harder to extend to 100 products\nRejected: Doesn’t scale well\n\nSingle legacy structure\n\nCons: Cannot coexist multiple products, forces coordination\nRejected: Defeats purpose of monorepo autonomy\n\nReferences\n\nKABSD-FTR-0010_monorepo-platform-migration.md: Monorepo Platform Migration feature\nKABSD-TSK-0081_execute-directory-restructuring-for-monorepo-platform.md: Directory migration implementation\nADR-0016_per-product-isolated-index-architecture: Per-Product Index Architecture\n"},"adr/ADR-0007_vcs-as-source-of-truth-derived-commit-data":{"title":"VCS as Source of Truth: Derived Commit Data","links":["items/feature/0000/KABSD-FTR-0017_traceability-commit-refs-worklog-backfill","items/userstory/0000/KABSD-USR-0018_vcs-adapter-abstraction-layer","items/task/0100/KABSD-TSK-0110_evaluate-vcs-query-cache-layer"],"tags":["architecture","vcs","traceability","derived-data"],"content":"Status\nAccepted (2026-01-07)\nImplemented in Feature KABSD-FTR-0017 (Traceability: Commit Refs → Worklog Backfill).\nContext\nBacklog items need traceability to VCS commits to answer:\n\n“Which commits contributed to this item?”\n“What’s the latest activity timestamp for this item?”\n“Generate a commit timeline view filtered by item state”\n\nTwo competing approaches:\n\nWorklog Backfill: Parse VCS commits containing Refs: &lt;item-id&gt; and append them to item worklog\nDerived Data Query: Keep VCS as source of truth; backlog queries VCS on-demand for commit data\n\nDecision\nWe adopt the Derived Data Query approach:\n\nVCS commits remain the canonical source of commit history\nBacklog items do NOT store commit data in worklog (no backfill)\nQuery tools (query_commits.py, view_generate_commits.py) dynamically fetch commit data from VCS\nCommit messages use Refs: &lt;item-id&gt;, &lt;item-id&gt; pattern for traceable references\nMulti-VCS abstraction layer (scripts/vcs/) supports Git, Perforce, SVN\n\nArchitecture:\n┌─────────────┐\r\n│ VCS (Git)   │ ← Source of Truth (commit hash, author, date, message)\r\n└──────┬──────┘\r\n       │ query via adapter\r\n       ▼\r\n┌─────────────────────┐\r\n│ VCS Adapter Layer   │ (Git/Perforce/SVN)\r\n│ - base.py           │\r\n│ - git_adapter.py    │\r\n│ - perforce_adapter.py │\r\n│ - svn_adapter.py    │\r\n└──────┬──────────────┘\r\n       │ query by ID/UID\r\n       ▼\r\n┌─────────────────────┐\r\n│ Query Tools         │\r\n│ - query_commits.py  │ (item → commits list)\r\n│ - view_generate_commits.py │ (state → commit timeline)\r\n└─────────────────────┘\n\nRationale\nWhy Derived Data (NOT Worklog Backfill)?\nPros:\n\nNo Worklog Pollution: Worklog stays clean for human-authored entries (decisions, state changes, manual notes)\nVCS is Authoritative: No sync issues between VCS history and backlog; VCS is already immutable and auditable\nDeduplication: Single commit referencing multiple items doesn’t create N duplicate worklog entries\nTime-travel Queries: Can query commits by date range without modifying backlog files\nMulti-VCS Support: Abstraction layer allows querying Git, Perforce, SVN uniformly\n\nCons:\n\nQuery Cost: Every view generation requires VCS query (mitigated by future cache layer, see TSK-0110)\nVCS Dependency: Backlog alone doesn’t show commit history (requires VCS access)\nComplexity: Multi-VCS adapter abstraction adds code complexity\n\nWhy Multi-VCS Abstraction?\nReal-world projects may use multiple VCS systems (monorepos with Git, legacy Perforce depots, SVN archives). The adapter pattern provides:\n\nUniform interface: VCSAdapter.query_commits(ref_pattern, since, until, max_count)\nFuture-proof: Easy to add new VCS types without changing query tools\nTestable: Mock adapters for unit tests\n\nConsequences\nImmediate Impact (Feature 0017)\nImplemented:\n\n✅ scripts/vcs/base.py: VCSAdapter abstract class, Commit dataclass, detect_vcs()\n✅ scripts/vcs/git_adapter.py: Git implementation using git log --grep\n✅ scripts/vcs/perforce_adapter.py: Perforce using p4 changes -l\n✅ scripts/vcs/svn_adapter.py: SVN using svn log --xml\n✅ scripts/backlog/query_commits.py: Resolve item → query VCS → output text/JSON\n✅ scripts/backlog/view_generate_commits.py: Generate commit timeline views by state\n\nCommit Message Convention:\nfeat: implement VCS adapter abstraction\r\n\r\nRefs: KABSD-TSK-0105, KABSD-FTR-0017\n\nPattern: Refs: &lt;id&gt;[, &lt;id&gt;]* (case-insensitive, extracted via regex)\nFuture Work\nCache Layer (TSK-0110):\n\nEvaluation pending: SQLite cache vs. file-based cache\nCache invalidation: TTL-based + manual clear\nConfig: vcs.cache.enabled, vcs.cache.backend, vcs.cache.ttl\nNote: Cache is derived data; VCS remains source of truth\n\nIntegration:\n\nDashboard auto-refresh: View generators can be called from view_refresh_dashboards.py\nWorklog hints: Query tools can suggest worklog entries (human decides whether to add)\nADR references: Commits referencing ADRs can link to decision artifacts\n\nBreaking Changes\nNone. This is a new capability; existing backlog items are unaffected.\nAlternatives Considered\n1. Worklog Backfill (Original Design)\nApproach: Parse VCS commits and append to item worklog:\n2026-01-07 14:08 [agent=vcs-bot] Commit 2048e1c: Test commit for VCS adapter\nRejected because:\n\nWorklog pollution: Noisy with many commits\nSync burden: Requires periodic backfill script runs\nDeduplication issue: Multi-item commits create duplicate entries\nNot time-travel friendly: Can’t query “commits since yesterday” without re-parsing\n\n2. Commit Index Table (Persistent Storage)\nApproach: Store commits in SQLite commits(hash, author, date, message, item_refs) table.\nDeferred to TSK-0110 (cache evaluation):\n\nWould solve query performance\nRequires cache invalidation strategy\nSchema migration burden (needs TSK-0111 framework first)\n\n3. VCS-Native Tools Only\nApproach: Use git log --grep &quot;KABSD-&quot; directly; no backlog integration.\nRejected because:\n\nNot multi-VCS portable\nNo item-to-commits resolution (requires manual filtering)\nNo state-based filtering (can’t generate “InProgress items with commits” view)\n\nReferences\n\nFeature: KABSD-FTR-0017\nUserStory: KABSD-USR-0018\nTasks: KABSD-TSK-0105 (Git), KABSD-TSK-0106 (Perforce), KABSD-TSK-0107 (SVN), KABSD-TSK-0108 (query_commits.py), KABSD-TSK-0109 (view_generate_commits.py)\nFuture Work: KABSD-TSK-0110 (VCS Query Cache Evaluation)\nDepends On: None (standalone capability)\n\nAppendix: Example Usage\nQuery Commits for an Item\n# Text format\npython skills/kano-agent-backlog-skill/scripts/backlog/query_commits.py \\\n  --item KABSD-TSK-0105\n \n# JSON format\npython skills/kano-agent-backlog-skill/scripts/backlog/query_commits.py \\\n  --item KABSD-TSK-0105 --format json\nGenerate Commit Timeline View\n# All &quot;Done&quot; items with commits\npython skills/kano-agent-backlog-skill/scripts/backlog/view_generate_commits.py \\\n  --state Done --output _kano/backlog/views/commits_done.md\n \n# All &quot;InProgress&quot; items (useful for daily standup)\npython skills/kano-agent-backlog-skill/scripts/backlog/view_generate_commits.py \\\n  --state InProgress --output _kano/backlog/views/commits_active.md\nCommit Message Pattern\nfeat(vcs): add Perforce adapter with p4 changes parsing\r\n\r\nLong description of the change...\r\n\r\nRefs: KABSD-TSK-0106, KABSD-FTR-0017\n\n\nPattern is case-insensitive: refs:, Refs:, REFS: all work\nMultiple items: comma-separated Refs: ITEM-1, ITEM-2, ITEM-3\nDeduplication: Same commit appears once even if queried by multiple item IDs\n"},"adr/ADR-0008_sqlite-schema-migration-framework":{"title":"SQLite Schema Migration Framework","links":["items/task/0100/KABSD-TSK-0111_implement-sqlite-schema-migration-framework","adr/ADR-0004_file-first-architecture-with-sqlite-index","adr/ADR-0012_workset-db-canonical-schema-reuse","items/task/0100/KABSD-TSK-0110_evaluate-vcs-query-cache-layer"],"tags":["architecture","database","migration","schema-evolution"],"content":"Status\nAccepted (2026-01-07)\nImplemented in Task KABSD-TSK-0111 (Implement SQLite Schema Migration Framework).\nContext\nThe SQLite index schema (introduced in ADR-0004) needs to evolve:\n\nCurrent need: Add VCS cache tables (vcs_commits, vcs_cache_metadata) for TSK-0110\nFuture needs: Embeddings tables, worklog full-text search indexes, external system sync tables\n\nExisting mechanism (ad-hoc):\n# build_sqlite_index.py (before ADR-0008)\ntry:\n    cols = [row[1] for row in conn.execute(&quot;PRAGMA table_info(item_links)&quot;).fetchall()]\n    if &quot;target_uid&quot; not in cols:\n        conn.execute(&quot;ALTER TABLE item_links ADD COLUMN target_uid TEXT&quot;)\nexcept sqlite3.OperationalError:\n    pass\nProblems:\n\nNo version tracking (schema_version is written but never read)\nHard-coded migrations in apply_schema() (not scalable)\nNo migration ordering or idempotency guarantees\nFragile try-except wrapping breaks on constraint violations\n\nRisk: Adding VCS cache tables without a migration framework could break existing DBs or create inconsistent schemas across environments.\nDecision\nWe adopt a Flyway-style migration framework:\n\nNumbered SQL migration files in references/migrations/\nVersion detection via schema_meta.schema_version (integer)\nAuto-upgrade on build_sqlite_index.py --mode rebuild\nMigration runner applies pending migrations sequentially\nBase schema (indexing_schema.sql) initializes version to 0\n\nArchitecture:\nreferences/\r\n  indexing_schema.sql          ← Base schema (creates tables, version=0)\r\n  migrations/\r\n    001_add_vcs_cache_tables.sql      ← Version 1\r\n    002_add_embeddings_fts.sql        ← Version 2\r\n    003_add_external_sync.sql         ← Version 3 (future)\n\nMigration Runner Logic:\ndef get_current_version(conn) -&gt; int:\n    &quot;&quot;&quot;Return schema version (0 if fresh DB).&quot;&quot;&quot;\n    try:\n        row = conn.execute(\n            &quot;SELECT value FROM schema_meta WHERE key=&#039;schema_version&#039;&quot;\n        ).fetchone()\n        return int(row[0]) if row else 0\n    except sqlite3.OperationalError:\n        return 0  # schema_meta doesn&#039;t exist yet\n \ndef apply_migrations(conn):\n    &quot;&quot;&quot;Apply pending migrations in order.&quot;&quot;&quot;\n    current_version = get_current_version(conn)\n    migration_dir = Path(&quot;references/migrations&quot;)\n    migrations = sorted(migration_dir.glob(&quot;*.sql&quot;))\n    \n    for migration_file in migrations:\n        version = int(migration_file.stem.split(&quot;_&quot;)[0])  # &quot;001_*.sql&quot; → 1\n        if version &gt; current_version:\n            print(f&quot;Applying migration {version}: {migration_file.name}&quot;)\n            conn.executescript(migration_file.read_text())\n            conn.execute(\n                &quot;INSERT OR REPLACE INTO schema_meta(key, value) VALUES(?, ?)&quot;,\n                (&quot;schema_version&quot;, str(version))\n            )\n            conn.commit()\n \ndef apply_schema(conn):\n    &quot;&quot;&quot;Apply base schema + migrations.&quot;&quot;&quot;\n    conn.executescript(load_schema_sql())  # Creates schema_meta with version=0\n    apply_migrations(conn)                 # Upgrade to latest\nRationale\nWhy Flyway-Style Migrations?\nPros:\n\nExplicit Versioning: Each migration increments version; easy to track schema state\nIdempotent: Migrations run exactly once (version check prevents re-runs)\nOrdered Execution: Sorted filenames guarantee deterministic application order\nGit-Friendly: Migration files are plain SQL, diff-able and reviewable\nSimple Mental Model: Familiar to developers (like Alembic, Liquibase, Django migrations)\n\nCons:\n\nNo Rollback: Down migrations not supported (users must delete DB and rebuild)\nManual Numbering: Developers must coordinate version numbers (low risk in local-first design)\n\nWhy Not Alembic/SQLAlchemy?\n\nOverkill: Requires ORM layer; we use raw SQL for simplicity\nPython-based migrations: SQL migrations are easier to audit and portable\nDependency bloat: Alembic + SQLAlchemy adds significant dependencies\n\nWhy Not “Delete and Rebuild”?\nCurrent approach is rm backlog.sqlite3 &amp;&amp; rebuild. Why not keep this?\nFor small backlogs (&lt;100 items): Delete-and-rebuild is fine (fast, simple).\nFor large backlogs (&gt;1000 items): Rebuild takes &gt;10 seconds. Migrations enable:\n\nIncremental mode: Only re-index changed files (faster)\nPreserve derived data: Cache tables (VCS commits, embeddings) don’t need full rebuild\nProduction stability: Breaking schema changes are painful if rebuild is the only option\n\nDecision: Support both. Migrations for gradual evolution; delete-rebuild as nuclear option.\nConsequences\nImmediate Impact (Task 0111)\nImplemented:\n\n✅ get_current_version(conn): Read schema_meta.schema_version\n✅ apply_migrations(conn): Apply pending migrations from references/migrations/\n✅ apply_schema(conn) refactored: Base schema → migrations\n✅ Base schema (indexing_schema.sql) initializes schema_version = &#039;0&#039;\n✅ Tested: Fresh DB (v0), migration upgrade (v0→v1), idempotent re-runs\n\nMigration Directory:\nreferences/migrations/\r\n  (empty - ready for 001_add_vcs_cache_tables.sql when TSK-0110 completes)\n\nFuture Work\nVersion Compatibility Check:\ndef check_schema_compatibility(conn):\n    &quot;&quot;&quot;Warn if DB schema is newer than skill version.&quot;&quot;&quot;\n    db_version = get_current_version(conn)\n    skill_max_version = 2  # Hard-coded or read from VERSION file\n    if db_version &gt; skill_max_version:\n        print(f&quot;Warning: DB schema v{db_version} newer than skill v{skill_max_version}.&quot;)\nMigration Naming Convention:\n{version:03d}_{description}.sql\r\n001_add_vcs_cache_tables.sql\r\n002_add_embeddings_fts.sql\r\n003_add_worklog_search_index.sql\n\nTransaction Safety:\r\nAll migrations wrapped in BEGIN TRANSACTION / COMMIT (SQLite default for executescript()). If migration fails, rollback prevents partial application.\nBreaking Changes\nNone. Existing DBs without migrations are version 0; migrations upgrade them gracefully.\nBackward Compatibility:\n\nOld skill (no migration runner) + new DB (v1+): Read queries work; writes may fail on new constraints\nNew skill (with migration runner) + old DB (v0): Auto-upgrades on rebuild\n\nAlternatives Considered\n1. Hard-Coded Migrations in Code\nCurrent approach (before ADR-0008):\nif &quot;target_uid&quot; not in cols:\n    conn.execute(&quot;ALTER TABLE item_links ADD COLUMN target_uid TEXT&quot;)\nRejected because:\n\nNot scalable (code bloat)\nNo version tracking (can’t detect schema state)\nError-prone (forgotten migrations leave inconsistent DBs)\n\n2. Alembic (Python-Based Migrations)\nApproach: Use Alembic for ORM-style migrations.\nRejected because:\n\nRequires SQLAlchemy ORM (we use raw SQL)\nPython-based migrations harder to audit (prefer declarative SQL)\nOverkill for local-first use case\n\n3. Schema Versioning Without Migrations\nApproach: Store version but require manual schema updates.\nRejected because:\n\nShifts migration burden to users (error-prone)\nNo automated upgrade path\n\n4. Delete-and-Rebuild Only\nApproach: Always rm backlog.sqlite3 &amp;&amp; rebuild on schema change.\nPartially Retained: Still supported as nuclear option.\nWhy Not Sufficient:\n\nLarge backlogs: Rebuild too slow (&gt;10s for 1000+ items)\nCache tables: VCS commits, embeddings would be lost (no incremental updates)\n\nReferences\n\nTask: KABSD-TSK-0111\nRelated ADR: ADR-0004 (SQLite Index Architecture)\nRelated ADR: ADR-0012 (Workset DB Schema - migrations apply to worksets too)\nFuture Work: KABSD-TSK-0110 (VCS Cache - first migration user)\nDependency: None (standalone framework)\n\nWorkset DB Migration\nPer ADR-0012, workset DBs MUST reuse the canonical schema and apply the same migrations.\nWorkset Migration Strategy:\n\nWhen building a workset, detect canonical index schema version\nApply same migrations to workset DB in order\nStore canonical_index_version in workset_manifest table\nIf canonical schema is upgraded, worksets MUST be rebuilt or auto-migrated\n\nConstraint: Workset schema_version MUST NOT exceed canonical schema_version.\nAppendix: Migration File Template\n-- Migration 001: Add VCS cache tables (2026-01-07)\n-- Context: Support derived VCS commit data caching (TSK-0110)\n \nCREATE TABLE vcs_commits (\n  item_uid TEXT NOT NULL,\n  commit_hash TEXT NOT NULL,\n  author TEXT NOT NULL,\n  date TEXT NOT NULL,\n  message TEXT NOT NULL,\n  cached_at TEXT NOT NULL,\n  PRIMARY KEY(item_uid, commit_hash)\n);\n \nCREATE INDEX idx_vcs_commits_cached_at ON vcs_commits(cached_at);\n \nCREATE TABLE vcs_cache_metadata (\n  item_uid TEXT PRIMARY KEY,\n  last_query_at TEXT NOT NULL,\n  vcs_type TEXT NOT NULL  -- &#039;git&#039;, &#039;perforce&#039;, &#039;svn&#039;\n);\nNaming: {version:03d}_{description}.sql\nTesting:\n# Fresh DB (should apply migration)\nrm -f _kano/backlog/_index/backlog.sqlite3\npython skills/kano-agent-backlog-skill/scripts/indexing/build_sqlite_index.py \\\n  --agent copilot --mode rebuild\n \n# Re-run (should skip migration, idempotent)\npython skills/kano-agent-backlog-skill/scripts/indexing/build_sqlite_index.py \\\n  --agent copilot --mode rebuild"},"adr/ADR-0009_local-first-embedding-search-architecture":{"title":"Local-First Embedding Search Strategic Evaluation","links":["items/userstory/0000/KABSD-USR-0015_generate-embeddings-for-backlog-items-derivative-index","adr/ADR-0004_file-first-architecture-with-sqlite-index","adr/ADR-0011_graph-assisted-retrieval-and-context-graph","items/feature/0000/KABSD-FTR-0023_graph-assisted-rag-planning-and-minimal-implementation"],"tags":[],"content":"Local-First Embedding Search Strategic Evaluation\nContext and Problem Statement\nAs the backlog grows with hundreds of items, agents need a way to perform semantic search to find relevant context (e.g., “Why did we decide to use ULID?” or “Find similar tasks for refactoring the indexer”).\nOur architecture is “Local-First”:\n\nCanonical Store: Markdown files.\nDerived Store: SQLite index (rebuildable).\n\nWe need an embedding search solution that:\n\nIntegrates well with the existing local-first workflow.\nMinimizes complex binary dependencies for cross-platform support.\nFollows the “derived data” philosophy (indices can be thrown away and rebuilt).\n\nDecision Drivers\n\nDeployment Simplicity: Zero-install or easy-install on developer machines.\nConsistency: The vector index must stay in sync with the Markdown/SQLite data.\nPhilosophical Alignment: Keep the source of truth in files; everything else is a performance optimization.\n\nConsidered Options\nRoute A: SQLite + Vector Extension (e.g., sqlite-vec)\nUse a SQLite extension to handle vector storage and ANN (Approximate Nearest Neighbor) search directly in the database.\n\nGood, because: Single database item; relational + vector joins in one query.\nBad, because: Loading binary extensions in Python (sqlite3.load_extension) is notoriously finicky across platforms (Windows vs Linux vs macOS). Packaging these binaries into the skill makes it “heavy”.\n\nRoute B: SQLite (Metadata) + Sidecar ANN Index (e.g., FAISS / HNSWlib)\nKeep the metadata (ID, title, state) in the existing SQLite index. Store the high-dimensional vectors in a separate, dedicated index file (sidecar).\n\nGood, because:\n\nHighly decoupled: We can swap FAISS for HNSWlib or even a plain NumPy file without touching the SQLite schema.\nFits the “Kano Philosophy”: The vector index is just another derived artifact.\nPerformance: Sidecar indices like HNSWlib are extremely fast for mmap-based local search.\n\n\nBad, because: Requires a “two-step” lookup (Search Sidecar → Map IDs → Fetch SQLite) and a dual-sync process during ingestion.\n\nRoute C: Postgres + pgvector (Shared Derived Store)\nMove the derived index to a remote Postgres instance with the pgvector extension.\n\nGood, because: Perfect for multi-agent/multi-remote collaboration where a shared “claim” or “lock” system is needed anyway.\nBad, because: Requires a server. Not “local-first” in the spirit of the project. High latency for simple local tasks.\n\nDecision Outcome\nChosen option: Route B (Sidecar ANN Index) for local-first environments, with an optional path to Route C for shared/remote usage.\nImplementation Strategy\n\n\nUnified Ingestion:\n\nDocTypes: Cover WorkItem, ADR, Worklog, Workset (local cache), and Skill Docs.\nMetadata Store: SQLite documents table tracks uid, doctype, product, path, and content_hash.\nChunking: Document-aware chunking (e.g., ADR sections, Worklog per-day). Stores in chunks table with parent_doc and section metadata.\nFTS5: Index chunk text in SQLite FTS5 for sub-millisecond keyword search and BM25 ranking.\n\n\n\nEmbedding &amp; Vector Sidecar:\n\nSidecar: HNSWlib or FAISS index file (index_&lt;product&gt;.bin).\nIncremental Sync: Only compute embeddings for chunks where text_hash has changed.\nMapping: SQLite stores chunk_id -&gt; vector_id to bridge the sidecar back to metadata.\n\n\n\nHybrid Search &amp; Ranking:\n\nQuery Path:\n\nStructural: SQLite B-Tree (product/type/status).\nKeyword: SQLite FTS5 (BM25).\nSemantic: Sidecar ANN (Cosine similarity).\n\n\nFusion: Combine scores using weighted logic:\n\nw_exact: High weight for ID matches.\nw_type: Priority for ADR Decisions and WorkItem Titles.\nw_recency: Decay score for older content.\nw_visibility: Distinguish between canonical and local_cache.\n\n\n\n\n\nPros and Cons of the Consequences\nGood\n\nPortable: The sidecar can be shared or ignored by git easily.\nFast: Local search is sub-millisecond.\nRobust: If the sidecar breaks, we just delete it and rerun the indexing script.\nComprehensive: Covers “everything” in the repo while preserving visibility boundaries (Local Workset vs Canonical ADRs).\n\nBad\n\nSync Logic: Need to handle incremental updates (delete old vectors if file is deleted/moved).\nTooling: Requires an ANN library in the dependencies (e.g., hnswlib or sentence-transformers/faiss-cpu).\n\nReferences\n\nKABSD-USR-0015: Generate embeddings for backlog items\nADR-0004: File-first architecture with SQLite index\n\nGraph-assisted retrieval (Context Graph)\nIn addition to keyword/semantic retrieval, we can improve precision and traceability by expanding the seed set\r\nvia a derived Context Graph (parent chain, ADR refs, dependency links).\nMinimal strategy:\n\nRetrieve seeds via FTS/ANN\nExpand k-hop over allowlisted edges\nRe-rank and pack context (seed + neighbors)\n\nSee:\n\nADR-0011 Graph-assisted retrieval with a derived Context Graph\nKABSD-FTR-0023 Graph-assisted RAG planning\n"},"adr/ADR-0010_project-babylon-global-scale-collaboration-vision":{"title":"Kano-Babylon Project","links":[],"tags":["vision","scaling","agents","babylon"],"content":"Kano-Babylon Project\nThe project has established a solid “Local-First” foundation with the Kano Commit Convention (KCC) and the Backlog Skill. However, the ultimate goal transcends a single repo or a single user. We envision “Project Babylon”—a vast, distributed project execution system where thousands of humans and agents collaborate on a scale previously impossible.\nVision: The Babylon Tower\nThe metaphor of the Tower of Babel represents a project so grand it reaches the heavens. Unlike the biblical story, our “Babylon” uses technology to ensure that a multitude of voices and languages (human and machine) can work in perfect harmony.\nThe Workforce (Agents)\nAgents are the “masons” of the tower. They:\n\nExecute high-velocity code, doc, and test changes.\nMaintain the backlog discipline autonomously.\nCommunicate via structured data (JSON/MD) and auditable worklogs.\nOperate locally, minimizing latency and avoiding central bottlenecks.\n\nThe Overseers (Humans)\nHumans are the “architects” and “supervisors”. They:\n\nProvide high-level context and intent.\nReview critical ADRs and release candidates.\nResolve high-level priority conflicts.\nDefine the “Temporary Clauses” and guardrails for the agent workforce.\n\nArchitectural Pillars for Babylon\n\nVCS-Agostic Distribution: Git/Perforce/Subversion act as the transport layer. The “state” of the project is a forest of local-first backlogs.\nEventually Consistent Coordination: Moving away from central “locking” towards a “claim/lease” protocol where agents can claim segments of work and sync changes asynchronously.\nCanonical File-First Truth: The “Source of Truth” remains readable files (.md, .json). DBs (SQLite/Vector) are only ever derived caches for performance.\nAgent Semantic Indexing: Global search across thousands of products using decentralized vector embeddings and cross-repo referencing.\nUniversal Linter/Compliance: Every “brick” added to the tower must pass the KCC and Backlog Quality gates (STCC), ensuring the tower never crumbles from internal inconsistency.\n\nOvercoming the “Babylon Curse” (Counter-measures)\nThe historical Babylon fell because of linguistic fragmentation and loss of common purpose. Our architecture is designed to proactively avoid this “curse”:\n\nSTCC as a Universal Language: By strictly enforcing the Standardized Technical Communication Convention (STCC), we ensure that an agent in one part of the project produces output that is perfectly understood by an agent (or human) in another, regardless of their internal processing “dialect”.\nLocal-First Resilience: If central coordination (cloud/server) fails, the “builders” (local nodes) don’t stop. They continue working based on local truth and re-sync whenever possible, preventing total project paralysis.\nAuditable Reconstruction: The append-only Worklog and immutable Git history act as a permanent record. If coordination is temporarily lost, the project can be “re-aligned” by traversing the decision trail.\nThe Human Context Anchor: Humans serve as the source of “Grand Intent”, preventing the workforce from diverging into irrelevant or conflicting optimizations.\n\nRationale\nBy documenting this now, we ensure that every local-first decision we make (ID strategy, path resolution, i18n) is a “pre-fit” for a global-scale architecture. We are building the scaffold to support the weight of the heavens.\nStatus\nProposed/Visionary. This ADR serves as the north star for all future development. It justifies the strictness of our current local-first hardening while preparing the logic for the “Great Sync”.\n2026-01-08 18:55 [agent=antigravity] Created based on user’s vision of reaching the divine through collaborative scale."},"adr/ADR-0011_graph-assisted-retrieval-and-context-graph":{"title":"Graph-assisted retrieval with a derived Context Graph (weak graph first)","links":["adr/ADR-0004_file-first-architecture-with-sqlite-index","adr/ADR-0009_local-first-embedding-search-architecture","items/feature/0000/KABSD-FTR-0007_optional-db-index-and-embedding-rag-pipeline","items/feature/0000/KABSD-FTR-0023_graph-assisted-rag-planning-and-minimal-implementation"],"tags":[],"content":"Decision\nAdopt Graph-assisted retrieval as a minimal, local-first improvement to context quality:\n\nUse FTS/embeddings to retrieve seed nodes\nUse a derived Context Graph to expand to load-bearing neighbors (k-hop traversal)\nKeep everything derived/rebuildable from canonical Markdown (file-first)\n\nThis ADR explicitly chooses weak graph first: only structured relationships (no LLM entity extraction).\nContext\nWe already have a file-first backlog with optional derived indexes (SQLite / FTS / embeddings).\r\nVector-only retrieval often returns text-similar chunks but misses the structural context (parents, ADR decisions, dependency chains).\nWe want a deterministic, auditable way to expand context that is:\n\nlocal-first\nderived/rebuildable\nincrementally maintainable\nsafe (bounded expansion to avoid prompt bloat)\n\nDefinitions\n\nContext Graph: a derived, typed graph of artifact relationships (items, ADRs, dependencies, etc.).\nSeed set: top-N nodes from FTS/embedding retrieval.\nGraph expansion: k-hop traversal from seeds over allowlisted edges with limits.\n\nGraph model (v1)\nNodes\nMinimum node types:\n\nwork_item (Epic/Feature/UserStory/Task/Bug)\nadr\n\nOptional (for embedding/fts pipelines):\n\nchunk (document chunk tied to a parent doc)\n\nEdges\nMinimum edge types:\n\nparent (child → parent)\ndecision_ref (work_item → adr)\nrelates (work_item → work_item)\nblocks / blocked_by\n\nStorage (derived)\nThe Context Graph is derived data. Implementations may:\n\n\nMaterialize into SQLite\n\nreuse items as the node registry\nstore edges in a links-style table (source_uid, target_uid, type, optional weight, source_path)\n\n\n\nSidecar graph artifacts\n\n&lt;backlog-root&gt;/_index/graph_nodes.jsonl\n&lt;backlog-root&gt;/_index/graph_edges.jsonl\n\n\n\nBoth must be safe to delete and rebuild.\nRetrieval strategy (Graph-assisted RAG)\n\nSeed retrieval\n\nFTS and/or embeddings return top-N seed nodes/chunks\n\n\nExpand\n\ntraverse k-hop (default k=1)\nedge allowlist and fanout caps\n\n\nRe-rank\n\nweights by doctype and section (ADR decision &gt; item title/acceptance &gt; worklog)\noptionally prioritize Ready/InProgress items\n\n\nContext packing\n\nemit a context pack describing:\n\nseeds (why selected)\nneighbors (which edge pulled them in)\nminimal excerpts/anchors (title/ids/links)\n\n\n\n\n\nConfig surface (indicative)\n\nretrieval.graph.enabled\nretrieval.graph.k_hop\nretrieval.graph.edge_allowlist\nretrieval.graph.max_neighbors_per_seed\nretrieval.weights.* (doctype/section/state weights)\n\nConsequences\n\nGraph-assisted retrieval becomes the preferred way to preserve traceability (seed + neighbors).\nTooling must keep expansion bounded to avoid context explosions.\nThe design stays compatible with file-scan fallback and optional SQLite/embedding acceleration.\n\nNon-goals\n\nLLM/NLP entity extraction and automatic relation mining\nserver/MCP mode or cross-repo graphs\n\nReferences\n\nADR-0004 File-first + SQLite index\nADR-0009 Local-first embedding search\nRAG pipeline\nKABSD-FTR-0023 Graph-assisted RAG planning\n"},"adr/ADR-0011_workset-graphrag-context-graph-separation-of-responsibilities":{"title":"Workset vs GraphRAG / Context Graph — Separation of Responsibilities","links":["items/feature/0000/KABSD-FTR-0013_add-derived-index-cache-layer-and-peragent-workset-cache-ttl","items/feature/0000/KABSD-FTR-0015_execution-layer-workset-cache-promote","adr/ADR-0004_file-first-architecture-with-sqlite-index","adr/ADR-0009_local-first-embedding-search-architecture","artifacts/workset_evaluation_report","items/task/0200/KABSD-TSK-0217_clarify-spec-workset-vs-graphrag-context-graph-responsibilities-no-conflict"],"tags":[],"content":"Decision\nWe adopt a clear separation of responsibilities between:\n\nWorkset: Per-agent/per-task materialized cache bundle (local, ephemeral, task-scoped)\nGraphRAG / Metadata Graph: Repo-level derived navigation/retrieval structure (nodes + edges, shared, rebuildable)\nContext Graph: Either knowledge graph (same as GraphRAG) or agent workflow/planning graph (different layer, no conflict)\n\nCore Principle: Workset and Graph are BOTH derived data. Neither is the source of truth. The canonical backlog/ADR files remain the single system of record.\nContext and Problem Statement\nAs Kano evolves to support:\n\nMulti-agent collaboration with context management (KABSD-FTR-0013, KABSD-FTR-0015)\nGraph-based retrieval and semantic search (ADR-0009)\nWorkset-based execution memory (workset_evaluation_report.md)\n\nWe face a critical architectural risk: role confusion between components.\nThe Risk: Role Confusion\nWithout clear boundaries, future implementations might:\n\nTreat per-agent worksets as the “truth” instead of rebuildable cache\nStore the authoritative graph structure ONLY inside worksets (leading to divergence across agents)\nMix retrieval logic (graph expansion) with cache storage (workset)\nCreate worksets that cannot be rebuilt from canonical data\n\nThis ADR prevents these failure modes by establishing hard constraints and data flow patterns.\nDefinitions\n1. Workset (Working Set)\nWhat it is:\n\nA materialized bundle (typically a SQLite file + optional filesystem cache) containing a selected subset of items, chunks, and summaries.\nUsually per-agent or per-task, scoped to a specific time window or work session.\nStored in _kano/backlog/.cache/worksets/&lt;item-id&gt;/ (one directory per backlog item; agent recorded in manifest) and NOT tracked in Git.\n\nPurpose:\n\nMaximize context relevance and reduce repeated retrieval cost during task execution.\nProvide stable, fast access to “current working context” without re-querying repo-level indices.\nSupport execution-layer memory patterns (plan.md, notes.md, deliverable.md) as described in workset_evaluation_report.md.\n\nKey Properties:\n\nDerived: Built from canonical files + repo-level derived index\nRebuildable: Can be deleted and reconstructed at any time\nEphemeral: May have TTL (time-to-live) and automatic cleanup\nLocal: Not the source of truth; promotes back to canonical on important updates\n\nWhat it is NOT:\n\nNOT the system of record (canonical files are)\nNOT the authoritative graph store (repo-level graph index is)\nNOT shared across agents (each agent/task has its own)\nNOT version-controlled in Git\n\n2. GraphRAG / Metadata Graph\nWhat it is:\n\nA derived navigation/index structure with nodes and edges:\n\nNodes: workitems, ADRs (optionally commits, worklog entries, skill docs later)\nEdges: parent_of, references, depends_on, blocked_by, relates_to\n\n\nUsed for retrieval expansion and context assembly.\nStored at repo level (e.g., in SQLite links table, or separate graph DB file).\n\nPurpose:\n\nEnable graph-based queries: “Find all tasks blocking feature X”\nSupport k-hop expansion: “Given seed items, expand to related context”\nProvide structured navigation for RAG (Retrieval-Augmented Generation)\n\nKey Properties:\n\nShared: One graph per product/repo (not per-agent)\nDerived: Built from canonical file frontmatter (parent, links.relates, etc.)\nRebuildable: Can be rebuilt from files + frontmatter\nQueryable: Supports graph queries, traversal, expansion\n\nWhat it is NOT:\n\nNOT stored only inside worksets (worksets may include subgraph slices, but the authoritative graph is repo-level)\nNOT “strong KG” with LLM-extracted entities (that’s a future enhancement, not the base metadata graph)\nNOT the source of truth (canonical files are)\n\n3. Context Graph (Dual Meaning)\nThe term “Context Graph” can mean two different things, both valid and non-conflicting:\n3a. Context Graph = Knowledge Graph (Same as GraphRAG)\nIn RAG/retrieval contexts, “context graph” often means the knowledge graph used for retrieval.\n\nSame as: GraphRAG / Metadata Graph (defined above)\nPurpose: Navigate and expand context for LLM queries\n\n3b. Context Graph = Agent Workflow / Planning Graph\nIn agent orchestration contexts, “context graph” can mean the DAG (Directed Acyclic Graph) of agent tasks/steps.\n\nDifferent layer: This is about agent execution flow, not backlog item relationships\nPurpose: Plan and coordinate multi-step agent workflows\nNo conflict with Workset or GraphRAG: This is a workflow orchestration concept, not a data indexing concept\n\nClarification: Both meanings are valid. They address different layers and do not conflict with the Workset/GraphRAG separation.\nHard Constraints (Enforceable in Future Tickets)\n\n\nSource of Truth = Canonical Backlog/ADR Files\n\nAll writes MUST go to Markdown files in _kano/backlog/products/&lt;product&gt;/items/ or decisions/\nNeither Workset nor Graph can become the primary write target\n\n\n\nGraph and Workset are Derived and Must be Rebuildable\n\nBoth can be deleted and reconstructed from canonical files\nNo essential data lives ONLY in cache or index\n\n\n\nWorkset Must Not Become the Only Place Where Graph Truth Lives\n\nRepo-level graph index (shared derived) is the primary graph\nWorkset may include only a subgraph slice or expansion results\nWorkset does NOT store the authoritative full graph\n\n\n\nRetrieval Strategy (Workset-First with Fallback)\n\nQuery workset first (fast, stable context)\nFallback to repo-level derived index (vector/FTS/graph) when insufficient\nOptionally “incrementally enrich” the workset after fallback\nNever skip repo-level index and rely solely on workset\n\n\n\nNon-Goals\nThis specification explicitly does NOT include:\n\nServer/MCP implementation: This is a local-first spec (per AGENTS.md temporary clause)\nStrong graph / LLM-based KG: Entity extraction, relationship mining via LLM (future enhancement)\nWorkset as global indexing authority: Worksets are local/ephemeral, not authoritative\nReal-time sync between worksets: Each agent/task workset is independent\nGraph database engine choice: This spec is agnostic to implementation (SQLite, Neo4j, plain files)\n\nData Flow Architecture\n1. Build/Maintain Repo-Level Derived Index\nCanonical Files (Markdown + frontmatter)\r\n    ↓\r\n  Parse &amp; Extract\r\n    ↓\r\nRepo-Level Derived Index (SQLite + sidecar ANN)\r\n├── items table (metadata)\r\n├── links table (graph edges)  ← PRIMARY GRAPH\r\n├── chunks table (text chunks)\r\n├── FTS5 index (keyword search)\r\n└── Sidecar ANN (vector embeddings) ← per ADR-0009\n\nGraph Tables (in SQLite or separate graph DB):\n\nlinks(source_uid, target_uid, type) stores all edges\nRebuilt from frontmatter: parent, links.relates, links.blocks, links.blocked_by\n\n2. Build Workset Using Profile Recipe\nWorkset Build Process:\r\n1. Select seeds (e.g., active/in-progress/claimed/recent items)\r\n2. Expand via graph k-hop closure:\r\n   - Follow parent chain upward\r\n   - Follow references (links.relates)\r\n   - Follow dependencies (links.depends_on, links.blocks)\r\n3. Materialize into SQLite workset:\r\n   - Copy relevant items/chunks from repo index\r\n   - Include subgraph slice (only edges relevant to this workset)\r\n   - Add workset manifest (seeds, expansion params, timestamp)\n\nWorkset Structure (SQLite file):\n-- Workset metadata\nCREATE TABLE workset_manifest (\n  workset_id TEXT PRIMARY KEY,\n  agent TEXT,\n  task_id TEXT,\n  created_at TEXT,\n  ttl_hours INTEGER,\n  seed_items TEXT -- JSON array of seed UIDs\n);\n \n-- Cached items (subset from repo index)\nCREATE TABLE cached_items (\n  uid TEXT PRIMARY KEY,\n  -- ... copy of repo index item fields\n);\n \n-- Subgraph slice (only edges relevant to this workset)\nCREATE TABLE cached_links (\n  source_uid TEXT,\n  target_uid TEXT,\n  type TEXT,\n  PRIMARY KEY (source_uid, target_uid, type)\n);\n \n-- Cached chunks (for semantic search within workset)\nCREATE TABLE cached_chunks (\n  chunk_id TEXT PRIMARY KEY,\n  parent_uid TEXT,\n  content TEXT,\n  -- ... copy of repo index chunk fields\n);\n \n-- Optional: execution memory (plan, notes, deliverable)\n-- per workset_evaluation_report.md\nWorkset Filesystem Layout (Decision 2026-01-10)\n\nBase Path: _kano/backlog/.cache/worksets/&lt;item-id&gt;/\nContents:\n\nworkset.db — SQLite cache that reuses the canonical schema (ADR-0012)\nplan.md — Execution checklist (three-file pattern)\nnotes.md — Research notes / scratchpad\ndeliverable.md — Draft output waiting for promotion\n\n\nAgent Attribution: workset_manifest.agent records who initialized the workset; directory naming stays per item to keep TTL cleanup simple.\nRationale: Local-first workflows typically have a single active agent per backlog item. Owner locking (KABSD-TSK-0036) prevents concurrent edits; adding agent IDs to the filesystem path would duplicate manifest data and complicate cleanup.\nFuture Extension: If multiple agents must share a task concurrently, we can add optional &lt;agent_id&gt; suffixes, but the default is per-item directories for deterministic paths.\n\n3. Query Path\nAgent Query\r\n    ↓\r\n1. Search Workset (local SQLite)\r\n   ├── Fast: all relevant context already materialized\r\n   └── If sufficient → Return results\r\n    ↓\r\n2. Fallback to Repo-Level Index (if workset insufficient)\r\n   ├── Query repo-level SQLite (items, links, chunks, FTS5)\r\n   ├── Query sidecar ANN (vector search)\r\n   └── Expand via repo-level graph (k-hop from new seeds)\r\n    ↓\r\n3. Optionally Update Workset (incremental enrichment)\r\n   ├── Add newly discovered items/chunks to workset\r\n   └── Extend subgraph slice with new edges\r\n    ↓\r\nReturn results to agent\n\nResponsibilities (Unambiguous)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComponentResponsibilityWhat It IsWhat It Is NOTCanonical FilesSource of truthMarkdown + frontmatter in GitNOT queryable at scaleRepo-Level GraphPrimary graph structureShared, rebuildable index of all edgesNOT per-agent cacheWorksetPer-task cache bundleLocal, ephemeral, task-scoped materialized contextNOT source of truth, NOT authoritative graphSidecar ANNVector similarity searchFast semantic search (per ADR-0009)NOT metadata storeSQLite IndexFast relational queriesDerived metadata + FTS (per ADR-0004)NOT source of truth\nRetrieval Strategy (Detailed)\nWorkset-First Strategy\nWhen to use Workset-first:\n\nDuring active task execution (agent has claimed a task)\nWhen workset is fresh (within TTL window)\nWhen working context is stable (no major scope changes)\n\nBenefits:\n\nFast: No re-querying repo-level index\nStable: Context doesn’t change mid-task\nOffline-friendly: Workset can be pre-built and used offline\n\nRepo-Index Fallback\nWhen to fallback to repo-level index:\n\nWorkset expired or missing\nQuery requires cross-cutting view (e.g., “all items blocking any active task”)\nNew information needed that wasn’t in initial workset seeds\n\nFallback process:\n\nQuery repo-level SQLite (items, links, chunks, FTS5)\nQuery sidecar ANN if semantic search needed\nExpand via graph if relationship traversal needed\nCache results in workset for future queries (optional incremental enrichment)\n\nIncremental Enrichment (Optional)\nAfter fallback, agent MAY update workset:\n\nAdd newly discovered items/chunks\nExtend subgraph slice with new edges\nUpdate workset manifest (enrichment timestamp)\n\nGuardrails:\n\nWorkset size limits (prevent unbounded growth)\nEnrichment policy (e.g., only add items within 2-hop distance)\nTTL still applies (workset expires regardless of enrichment)\n\nTrade-offs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrade-offDescriptionWorkset StalenessWorkset may become stale if canonical files change during task execution. Mitigation: TTL + periodic rebuild.Dual MaintenanceNeed to maintain both repo-level index and workset build logic. Mitigation: Shared indexing code, clear derivation rules.Subgraph Slice ComplexityDeciding which edges to include in workset subgraph is non-trivial. Mitigation: Start with simple k-hop expansion, iterate.Storage OverheadWorksets duplicate data from repo index. Mitigation: Worksets are ephemeral, cleaned up by TTL.\nConsequences\nPositive\n\nClear Boundaries: No ambiguity about which component owns what\nRebuildable: All derived data can be deleted and reconstructed\nScalable: Worksets enable efficient multi-agent collaboration without index contention\nComposable: Graph, vector search, and worksets work together without conflict\n\nNegative\n\nComplexity: More components to understand and maintain\nSync Logic: Need careful handling of cache invalidation and TTL\nLearning Curve: Developers must understand the distinction between repo-level and workset-level data\n\nMitigations\n\nDocumentation: This ADR + inline code comments\nTooling: Scripts to rebuild indices, inspect worksets, validate consistency\nDefaults: Worksets are optional; can disable for simple single-agent scenarios\n\nReferences\n\ncache layer and per‑Agent workset cache (TTL)\nKABSD-FTR-0015: Execution Layer: Workset Cache + Promote\nADR-0004: File-First Architecture with SQLite Index\nADR-0009: Local-First Embedding Search Strategic Evaluation\nWorkset Evaluation Report\nKABSD-TSK-0217: Task tracking this specification\n\nFuture Work\nThis ADR establishes the foundation. Future enhancements may include:\n\nStrong Graph / Entity Extraction: LLM-based relationship mining beyond frontmatter\nMulti-Agent Workset Coordination: Shared worksets for pair programming scenarios\nWorkset Templates: Pre-configured recipes for common task types\nGraph Visualization: Tools to visualize repo-level graph and workset subgraphs\nPerformance Benchmarks: Measure workset-first vs repo-index-first query performance\n\nDecision Rationale\nWhy separate Workset and Graph?\n\nDifferent lifecycles: Graph is long-lived and shared; Workset is ephemeral and local\nDifferent query patterns: Graph is for exploration/expansion; Workset is for stable task context\nDifferent consistency models: Graph must stay in sync with canonical files; Workset can be stale within TTL\n\nWhy NOT merge them?\n\nMerging would force either (a) graph to be per-agent (duplication, inconsistency) or (b) workset to be shared (defeats the purpose of local cache)\nClear separation enables independent evolution and optimization of each component\n\nWhy repo-level graph is primary?\n\nGraph relationships are project-wide knowledge (e.g., “what blocks what”)\nPer-agent graphs would diverge and create confusion\nWorksets can include subgraph slices for fast local queries, but authoritative graph must be shared\n\nStatus\nProposed (2026-01-09)\nThis ADR is proposed for review. Once accepted, it becomes the architectural constraint for all future Workset and GraphRAG implementation work.\n\nThis ADR was created as part of KABSD-TSK-0217 to prevent role confusion between Workset, GraphRAG, and Context Graph."},"adr/ADR-0012_workset-db-canonical-schema-reuse":{"title":"Workset DB Uses Canonical Schema (No Parallel Schema)","links":["_meta/canonical_schema.sql","_meta/canonical_schema.json","artifacts/workset_schema_verification_examples","adr/ADR-0003_identifier-strategy-for-local-first-backlog","adr/ADR-0004_file-first-architecture-with-sqlite-index","adr/ADR-0008_sqlite-schema-migration-framework","adr/ADR-0011_workset-graphrag-context-graph-separation-of-responsibilities","items/task/0000/KABSD-TSK-0046_define-db-index-schema-items-links-worklog-decisions","items/feature/0000/KABSD-FTR-0013_add-derived-index-cache-layer-and-peragent-workset-cache-ttl","items/feature/0000/KABSD-FTR-0015_execution-layer-workset-cache-promote"],"tags":[],"content":"Decision\nWorkset DB must reuse the same system schema and semantics as the source-of-truth model, rather than creating a separate “workset-only schema”.\nWorkset DB is a materialized subset view of the canonical model, not a different model.\nContext and Problem Statement\nWe maintain canonical backlog data as local-first files (source of truth). We also plan to generate worksets (per-task/per-agent context bundles) as SQLite DBs for fast retrieval and stable context.\nWe want worksets to be derived data, rebuildable at any time, and we already rely on a globally unique UID for identity (ADR-0003).\nThe Question: Should workset DB have its own schema design, or should it reuse the canonical schema defined for the repo-level derived index?\nThe Risk: If workset has its own schema, it will inevitably diverge from the canonical data model, creating long-term maintenance cost, bugs, and integration friction.\nRationale\nWhy This Is Important\n1. Avoid Schema Drift\n\nIf workset has its own schema, it will diverge from the canonical data model over time\nEvery schema evolution would require parallel changes in two places\nDifferent schemas lead to subtle semantic mismatches and data loss during translation\n\n2. Portable Context with Zero Translation\n\nAgents/tools that understand the canonical schema can read a workset DB without custom mapping logic\nThis reduces integration friction across tools (CLI, future server façade, GUI)\nA workset can be directly queried using the same queries used for the repo-level index\n\n3. Deterministic Rebuild\n\nWorkset is derived: it must be regeneratable from source-of-truth + derived indexes\nReusing schema makes regeneration straightforward and verifiable\nSchema migrations (ADR-0008) apply uniformly to both repo index and worksets\n\n4. Consistent Identity &amp; References\n\nWorkitems/ADRs keep the same UID and same link semantics across canonical and workset DB\nEdges (parent/ref/depends) remain consistent\nNo need for ID translation or mapping tables\n\n5. Future-Proofing for Graph-Assisted Retrieval\n\nGraph expansion can materialize a subgraph into workset without inventing new edge formats\nWorkset becomes a “view slice” of the full graph, not a separate graph model\n\nCanonical Schema (Reused by Workset DB)\nThe canonical schema is defined in ADR-0004 and KABSD-TSK-0046. It represents:\nCore Entities\nitems Table\nCore metadata for all work items (Epic/Feature/Story/Task/Bug) and ADRs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumnTypeDescriptionuidTEXT (PK)UUIDv7 (globally unique, from frontmatter)idTEXTDisplay ID (e.g., KABSD-TSK-0049)typeTEXTWork item type (Epic, Feature, UserStory, Task, Bug, ADR)stateTEXTCurrent state (Proposed, Ready, InProgress, Done, etc.)titleTEXTItem titlepathTEXTRelative path to canonical filemtimeREALFile modification timestampcontent_hashTEXTHash of content (for change detection)frontmatterJSONFull frontmatter blob (flexibility)createdTEXTCreation date (ISO 8601)updatedTEXTLast updated date (ISO 8601)priorityTEXTPriority (P1, P2, P3, etc.)parent_uidTEXTUID of parent item (null if root)ownerTEXTCurrent owner/assigneeareaTEXTFunctional areaiterationTEXTIteration/sprint identifiertagsJSONArray of tags\nlinks Table\nTracks typed relationships for graph queries.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumnTypeDescriptionsource_uidTEXTLink source (referencing item)target_uidTEXTLink target (referenced item)typeTEXTLink type: “parent”, “relates_to”, “blocks”, “blocked_by”, “decision_ref”PRIMARY KEY(source_uid, target_uid, type)\nworklog Table (Optional but Canonical)\nStores append-only worklog entries.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumnTypeDescriptionuidTEXT (PK)Unique worklog entry IDitem_uidTEXTUID of parent itemtimestampTEXTISO 8601 timestampagentTEXTAgent/user who created entrycontentTEXTWorklog entry text\nchunks Table (For Embedding/FTS)\nStores content chunks for semantic search.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumnTypeDescriptionchunk_idTEXT (PK)Unique chunk identifierparent_uidTEXTUID of parent itemchunk_indexINTSequence number within parentcontentTEXTChunk text contentsectionTEXTSection type (Context, Goal, Approach, etc.)embeddingBLOBFloat32 vector array (optional)\nSchema Metadata\nPer ADR-0008, track schema version for migrations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumnTypeDescriptionkeyTEXT (PK)Metadata key (e.g., “schema_version”)valueTEXTMetadata value\nWorkset as a Subset\nA workset DB contains:\n\nIncluded nodes: A filtered subset of items (selected by workset recipe)\nIncluded edges: links restricted to included nodes (or optionally include boundary edges)\nIncluded chunks: Content chunks for included items\nIncluded worklog: Worklog entries for included items (optional)\n\nNOT included in workset:\n\nItems outside the selected scope\nEdges between excluded nodes\n\nWorkset-Specific Metadata (Additive Only)\nWorkset DB MAY add workset-specific metadata tables, but MUST NOT change core entity schemas.\nAllowed: workset_manifest Table\nCREATE TABLE workset_manifest (\n  workset_id TEXT PRIMARY KEY,\n  agent TEXT NOT NULL,\n  task_id TEXT,\n  created_at TEXT NOT NULL,\n  ttl_hours INTEGER,\n  seed_items TEXT,  -- JSON array of seed UIDs\n  expansion_params TEXT,  -- JSON: {k_hop: 2, edge_types: [...]}\n  source_commit_hash TEXT,  -- Git commit of canonical files\n  canonical_index_version TEXT NOT NULL CHECK (canonical_index_version &lt;&gt; &#039;&#039;)  -- Schema version of source index\n);\nAllowed: workset_provenance Table\nCREATE TABLE workset_provenance (\n  item_uid TEXT PRIMARY KEY,\n  selection_reason TEXT NOT NULL,  -- &quot;seed&quot;, &quot;parent_expansion&quot;, &quot;dependency_expansion&quot;, &quot;manual&quot;\n  distance_from_seed INTEGER,  -- Hop count from nearest seed (0 for seeds)\n  included_at TEXT NOT NULL,  -- ISO 8601 timestamp when item was added\n  FOREIGN KEY (item_uid) REFERENCES items(uid) ON DELETE CASCADE\n);\nThese tables are additive — they extend the canonical schema without changing core table definitions.\nContent Storage Strategy\nWorkset DB supports multiple content strategies (choose based on use case):\nOption 1: Full Content (Portable)\n\nStore complete item content in workset DB (in items.frontmatter JSON or separate content column)\nPros: Workset is fully portable, can be used offline\nCons: Larger DB size, duplication of content\n\nOption 2: Pointer-Based (Smaller)\n\nStore only uid, path, and content_hash in workset\nRequire access to canonical files for full content retrieval\nPros: Smaller workset DB, no content duplication\nCons: Not portable, requires canonical file access\n\nOption 3: Hybrid (Recommended)\n\nStore summaries/excerpts in workset (title, first N words of sections)\nStore pointers to canonical files + hashes for verification\nOptionally include full content for “hot” items (recently accessed)\nPros: Balanced size vs portability\nCons: More complex logic\n\nDecision: Support all three strategies via configuration. Default to Hybrid for best balance.\nSchema Evolution and Migrations\nPer ADR-0008, schema migrations apply uniformly:\n\n\nRepo-level index migration:\n\nApply migration 001_add_vcs_cache_tables.sql\nUpdate schema_meta.schema_version = &#039;1&#039;\n\n\n\nWorkset DB migration (when rebuilding workset):\n\nDetect source index schema version from canonical_index_version in manifest\nApply same migrations to workset DB\nEnsure workset schema version matches canonical schema version\n\n\n\nConstraint: Workset DB schema version MUST NOT exceed canonical schema version.\nRebuild Rule: If canonical schema is upgraded, all worksets MUST be rebuilt or auto-migrated.\nGuidelines for Maintaining Schema Compatibility\nDO: Add Workset-Specific Tables\n✅ Add workset_manifest, workset_provenance, or similar metadata tables\r\n✅ These tables MUST be prefixed with workset_ to avoid naming conflicts\r\n✅ Document all workset-specific tables in this ADR or code comments\nDO NOT: Modify Core Table Schemas\n❌ Do NOT change items, links, chunks, worklog, or schema_meta table definitions\r\n❌ Do NOT add columns to core tables specific to worksets\r\n❌ Do NOT rename or remove columns from canonical schema\nDO: Subset Core Tables\n✅ Workset items table contains fewer rows than canonical items (filtering is allowed)\r\n✅ Workset links table only includes edges relevant to included nodes\nDO: Preserve Field Semantics\n✅ uid means the same thing in workset and canonical DB (globally unique identifier)\r\n✅ state values match canonical state vocabulary (Proposed, Ready, InProgress, Done, etc.)\r\n✅ type values match canonical type vocabulary (Epic, Feature, UserStory, Task, Bug, ADR)\nDO: Version Compatibility Checks\n✅ When loading a workset, verify canonical_index_version matches expected schema\r\n✅ If version mismatch, warn or auto-rebuild workset\nAcceptance Criteria\n\n Canonical schema is defined (ADR-0004, this ADR)\n Workset DB reuses canonical items, links, chunks, worklog tables\n Workset-specific metadata is additive only (workset_manifest, workset_provenance)\n Content storage strategy is documented (full/pointer/hybrid)\n Schema migration compatibility is specified (workset follows canonical migrations)\n Guidelines for adding workset metadata are documented (DO/DO NOT rules)\n SQL schema definition created (canonical_schema.sql)\n JSON schema definition created (canonical_schema.json)\n Verification examples documented (workset_schema_verification_examples.md)\n Implementation validates schema compatibility at workset build time (future work)\n Tools that read canonical schema can read workset DB without special-case mapping (verified via examples)\n\nNon-Goals\n\nWorkset DB is NOT a new source-of-truth\nDo NOT implement a separate “workset schema v2”\nDo NOT require workset DB to contain all canonical data (it’s a subset by definition)\nDo NOT implement server runtime (per AGENTS.md temporary clause)\n\nConsequences\nPositive\n\nNo Schema Drift: Single schema definition for all derived DBs\nPortable Context: Worksets can be shared, inspected, queried with standard tools\nSimplified Maintenance: Schema migrations apply uniformly\nConsistent Identity: UIDs and link semantics preserved across canonical and workset\n\nNegative\n\nWorkset Constraints: Workset DB cannot optimize schema for workset-specific use cases\nMigration Coupling: Workset rebuild required when canonical schema changes\n\nMitigations\n\nExtensibility: Workset-specific tables allowed (additive only)\nRebuild Automation: Make workset rebuild fast and deterministic\nVersion Checks: Detect and handle schema version mismatches gracefully\n\nAlternatives Considered\n1. Separate Workset Schema\nApproach: Design a custom schema optimized for workset use cases.\nRejected because:\n\nSchema drift inevitable (maintenance burden)\nTranslation layer required (complexity, bugs)\nBreaks portable context (tools need dual schema support)\n\n2. Denormalized Workset Schema\nApproach: Flatten canonical schema into a denormalized “workset view” (e.g., single table with all fields).\nRejected because:\n\nLoses relational structure (graph queries become difficult)\nContent duplication (same item appears multiple times with different join results)\nStill requires mapping/translation logic\n\n3. Schema-Free (JSON Blobs Only)\nApproach: Store items as raw JSON blobs in workset DB.\nRejected because:\n\nNo relational query support (defeats purpose of SQL index)\nNo FTS or graph traversal without parsing JSON\nStill need consistent JSON schema (same drift problem)\n\nReferences\n\nADR-0003: Identifier Strategy (UID) — Global UID ensures identity consistency\nADR-0004: File-First Architecture with SQLite Index — Canonical schema definition\nADR-0008: SQLite Schema Migration Framework — Migration strategy\nADR-0011: Workset vs GraphRAG Separation — Workset role definition\nKABSD-TSK-0046: Define DB Index Schema — Original schema definition task\nKABSD-FTR-0013: Workset Cache — Workset feature\nKABSD-FTR-0015: Workset Promote — Workset execution layer\n\nFuture Work\n\nDefine JSON schema for canonical frontmatter (complementary to SQL schema)\nBenchmark workset query performance vs canonical index\nImplement schema version compatibility checker for workset loading\nAdd workset content strategy configuration (full/pointer/hybrid)\nCreate workset rebuild automation on schema migration\n\nVerification\nSee Workset Schema Verification Examples for test cases demonstrating:\n\nSchema compatibility checks\nSchema version tracking\nCore table consistency\nWorkset-specific table validation\nSubset semantics verification\nDeterministic rebuild testing\n\nStatus\nProposed (2026-01-09)\nThis ADR is proposed for review. Once accepted, it becomes the architectural constraint for all future Workset DB implementation work.\n\nThis ADR ensures that workset DB remains a true materialized view of the canonical schema, preventing schema drift and maintaining portable, rebuildable context bundles."},"adr/ADR-0013_codebase-architecture-and-module-boundaries":{"title":"Codebase Architecture and Module Boundaries","links":["adr/ADR-0015_skill-scoped-cli-namespace-convention","adr/ADR-0037_inspector-pattern-and-query-surface-architecture","items/feature/0000/KABSD-FTR-0028_refactor-kano-agent-backlog-skill-scripts-into-a-single-cli-entry-library-modules","KABSD-FTR-0025_unified-cli-for-backlog-operations","KABSD-FTR-0019_refactor-kano-backlog-core-cli-server-gui-facades","adr/ADR-0004_file-first-architecture-with-sqlite-index"],"tags":[],"content":"Decision\nEstablish strict separation between executable entrypoints (scripts/) and library modules (src/). All agent-callable operations must go through a single CLI entrypoint (scripts/kano), which delegates to library use-cases.\nHard Rules\n\nscripts/ is executable-only: No reusable module code in scripts/. Scripts must not be imported as libraries.\nSingle CLI entrypoint: Agents call only scripts/kano &lt;subcommand&gt;. All operations are exposed through this interface.\nsrc/ is import-only: Core logic lives in src/kano_backlog_* packages. These are imported by the CLI (and future facades), never executed directly.\nConsistent gating: All write operations run prereqs + initialization checks via a single gate layer in the CLI.\nDeterministic output: Same input state produces stable, reproducible output for views and queries.\n\nContext\nCurrent State (Problems)\nThe scripts/ directory contains 40+ standalone Python scripts with overlapping responsibilities:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCategoryScriptsIssuesbacklog/35+ scriptsMixed executable + library code in same folderbootstrap/1 scriptOKindexing/8 scriptsSome logic should be library codefs/5 scriptsFile operations, OK as thin wrappersvcs/4 adaptersAlready library-style but in scripts/common/4 modulesShared code incorrectly placed in scripts/logging/3 modulesShared code incorrectly placed in scripts/\nProblems:\n\nCommon logic mixed into scripts makes it hard to enforce consistent gating (prereqs/initialized/dry-run).\nCoding agents don’t have a clear architecture reference; new code gets placed inconsistently.\nDifferent scripts may bypass checks or diverge behavior over time.\nNo single entry point exists; agents must know which script to call.\n\nExisting Foundation\nWe already have:\n\nsrc/kano_backlog_core/: Core models, config, errors, refs, state (good foundation)\nsrc/kano_cli/: CLI skeleton with Typer, ~4 commands implemented\nscripts/backlog/lib/: Some shared code (should move to src/)\nscripts/backlog/cli/: Thin wrappers (good pattern, needs expansion)\n\nArchitecture\nLayered Architecture\nflowchart TB\r\n  subgraph Agent[&quot;Coding Agent / Human&quot;]\r\n    A[&quot;calls scripts/kano (CLI)&quot;]\r\n  end\r\n\r\n  subgraph Scripts[&quot;scripts/ (executable-only)&quot;]\r\n    CLI[&quot;scripts/kano\\n├─ parse args\\n├─ run gates (prereqs/init)\\n└─ call lib use-cases&quot;]\r\n  end\r\n\r\n  subgraph Src[&quot;src/ (import-only)&quot;]\r\n    Core[&quot;kano_backlog_core\\n(config/models/ids/errors/refs/state)&quot;]\r\n    Ops[&quot;kano_backlog_ops\\n(use-cases: init/create/update/\\nindex/workset/view)&quot;]\r\n    Adapters[&quot;kano_backlog_adapters\\n(sqlite/fts/faiss/vcs/fs)&quot;]\r\n    CLI_Pkg[&quot;kano_cli\\n(Typer app, commands)&quot;]\r\n    Hooks[&quot;kano_backlog_hooks (future)\\n(pre/post hooks interface)&quot;]\r\n  end\r\n\r\n  subgraph Data[&quot;Data Layer&quot;]\r\n    SoT[&quot;Source of Truth\\n(_kano/backlog/*.md)&quot;]\r\n    Cache[&quot;Derived Cache\\n(_kano/backlog/_index/*.sqlite3)&quot;]\r\n  end\r\n\r\n  A --&gt; CLI\r\n  CLI --&gt; CLI_Pkg\r\n  CLI_Pkg --&gt; Ops\r\n  Ops --&gt; Core\r\n  Ops --&gt; Adapters\r\n  Adapters --&gt; SoT\r\n  Adapters --&gt; Cache\r\n  Ops -. optional .-&gt; Hooks\n\nTarget Folder Structure\nflowchart LR\r\n  R[&quot;skills/kano-agent-backlog-skill/&quot;] --&gt; S[&quot;scripts/&quot;]\r\n  S --&gt; K[&quot;kano (only entrypoint)&quot;]\r\n  S --&gt; B[&quot;backlog/ (deprecated wrappers)&quot;]\r\n  S --&gt; I[&quot;bootstrap/, fs/ (thin utilities)&quot;]\r\n\r\n  R --&gt; SRC[&quot;src/&quot;]\r\n  SRC --&gt; CORE[&quot;kano_backlog_core/\\n(models, ids, config, errors)&quot;]\r\n  SRC --&gt; OPS[&quot;kano_backlog_ops/\\n(use-cases)&quot;]\r\n  SRC --&gt; ADP[&quot;kano_backlog_adapters/\\n(backends)&quot;]\r\n  SRC --&gt; CLIPKG[&quot;kano_cli/\\n(Typer commands)&quot;]\r\n\r\n  R --&gt; REF[&quot;references/\\n(schemas, docs)&quot;]\r\n  R --&gt; TPL[&quot;templates/\\n(markdown templates)&quot;]\r\n  R --&gt; DEC[&quot;decisions/ (this ADR)&quot;]\n\nPackage Responsibilities\nkano_backlog_core (existing, expand)\n\nmodels.py: Pydantic models for work items, ADRs\nconfig.py: Configuration loading, defaults\nids.py: ID parsing, generation, validation\nerrors.py: Custom exceptions\nrefs.py: Reference resolution logic\nstate.py: State machine definitions\naudit.py: Audit logging primitives\n\nkano_backlog_ops (new)\nUse-case functions that orchestrate operations:\n\ninit.py: Initialize backlog structure\nworkitem.py: Create, update, validate work items\nadr.py: Create, list ADRs\nworkset.py: Workset management (init/refresh/promote)\nview.py: Generate views, dashboards\nindex.py: Build/refresh SQLite index\n\nkano_backlog_adapters (new)\nPluggable backends:\n\nfs.py: File system operations (read/write markdown)\nsqlite.py: SQLite index adapter\nfts.py: Full-text search adapter\nembedding.py: Vector embedding adapter (optional)\nvcs/: VCS adapters (git, svn, perforce)\n\nkano_cli (existing, expand)\nTyper-based CLI application:\n\ncli.py: Main app, callback for gating\ncommands/: Subcommand modules (item, worklog, view, adr, index, workset)\nutil.py: CLI utilities (output formatting, path resolution)\n\nCLI Command Structure (Implemented)\nkano\r\n├── doctor              # Check prereqs + initialization\r\n├── backlog             # Backlog administration group\r\n│   ├── init            # Initialize backlog structure\r\n│   ├── index\r\n│   │   ├── build       # Build SQLite index\r\n│   │   └── refresh     # Refresh index (MVP: full rebuild)\r\n│   ├── demo\r\n│   │   └── seed        # Seed demo data for testing\r\n│   ├── persona\r\n│   │   ├── summary     # Generate persona activity summary\r\n│   │   └── report      # Generate persona state report\r\n│   └── sandbox\r\n│       └── init        # Scaffold isolated sandbox environment\r\n├── item\r\n│   ├── create          # Create work item\r\n│   ├── read            # Read item details\r\n│   ├── update-state    # Transition state + worklog append\r\n│   ├── validate        # Check Ready gate\r\n│   └── create-v2       # Alias for create (compatibility)\r\n├── state\r\n│   └── transition      # Declarative state transitions\r\n├── worklog\r\n│   └── append          # Append worklog entry\r\n├── view\r\n│   └── refresh         # Refresh all dashboards\r\n└── init (legacy)       # Alias for `backlog init` (deprecated)\n\nMigration Strategy\nPhase 0: ADR + SKILL Gate (This ADR)\n\n Create this ADR with architecture diagrams\n Update SKILL.md: skill developers must read ADR-0013 before coding\n\nPhase 1: CLI Skeleton ✅ COMPLETE\n\n Expanded src/kano_cli/commands/ to cover all high-frequency operations\n Add kano doctor for prereqs/init checks\n Implemented: item, state, worklog, view commands\n\nPhase 2: Library Migration ✅ COMPLETE\n\n Created src/kano_backlog_ops/ with use-case functions (init, workitem, adr, view, index, demo, persona, sandbox)\n Created src/kano_backlog_adapters/ for backend abstraction (partially)\n Moved logic from scripts/backlog/*.py into library packages\n Added backlog subcommand group with nested commands (index, demo, persona, sandbox)\n\nPhase 3: Deprecation ✅ COMPLETE\n\n Deleted 70+ legacy scripts from scripts/ directory\n Updated all documentation to recommend kano CLI\n Legacy kano init backlog aliased to kano backlog init with deprecation warning\n\nPhase 4: Future Extensions (Deferred)\n\nPlugin/hook system for external integrations\nNative engine option (C++/Rust via pybind11) for performance-critical paths\nHTTP/MCP server facade (reuses same kano_backlog_ops use-cases)\n\nTrade-offs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrade-offDescriptionMigration effortSignificant refactoring of existing scripts. Mitigated by phased approach.Backward compatibilityOld script paths break for agents. Mitigated by keeping thin wrappers.Initial complexityMore packages to maintain. Pays off with clearer boundaries and reusability.\nConsequences\n\nFor skill developers: Must read this ADR before adding code. New logic goes in src/, not scripts/.\nFor agents: Call only scripts/kano-backlog. Direct script calls are deprecated. See ADR-0015_skill-scoped-cli-namespace-convention for skill-scoped CLI naming convention.\nFor future facades: HTTP/MCP/GUI can import kano_backlog_ops directly, no CLI dependency.\nFor testing: Use-case functions in src/ are easier to unit test than CLI scripts.\nNaming convention: This skill follows skill-scoped naming (kano-backlog, kano_backlog_*). The bare kano namespace is reserved for a future umbrella CLI. See ADR-0015_skill-scoped-cli-namespace-convention for full rationale.\nInspector Pattern: External agents (health, review, security) consume query surface APIs from kano_backlog_ops, never write to canonical SoT directly. See ADR-0037_inspector-pattern-and-query-surface-architecture for full architecture.\n\nRelated\n\nKABSD-FTR-0028_refactor-kano-agent-backlog-skill-scripts-into-a-single-cli-entry-library-modules.md: Parent feature for this refactoring\nKABSD-FTR-0025_unified-cli-for-backlog-operations: Unified CLI (subset of this work)\nKABSD-FTR-0019_refactor-kano-backlog-core-cli-server-gui-facades: Core/CLI/Server/GUI facades separation\nADR-0004_file-first-architecture-with-sqlite-index: File-first architecture (complements this ADR)\nADR-0015_skill-scoped-cli-namespace-convention: Skill-scoped CLI namespace convention (naming strategy)\nADR-0037_inspector-pattern-and-query-surface-architecture: Inspector Pattern and Query Surface Architecture (extends module boundaries with external agent integration)\n"},"adr/ADR-0014_plugin-and-hook-system-architecture":{"title":"Plugin and Hook System Architecture (Phase 4 - Deferred)","links":["adr/ADR-0013_codebase-architecture-and-module-boundaries"],"tags":["architecture","extensibility","phase4","deferred"],"content":"Plugin and Hook System Architecture (Phase 4 - Deferred)\nStatus\nProposed - Design documented for future implementation. Phase 4 is deferred until concrete integration needs emerge.\nContext\nPer ADR-0013 Phase 4, we anticipate future needs for:\n\n\nPre/post hooks: External projects may want to inject custom logic before/after backlog operations (e.g., notify external systems, run custom validations, sync with remote databases).\n\n\nCustom engines: Performance-critical operations (workset retrieval, embedding search) may benefit from native (C++/Rust) implementations or external service integration.\n\n\nExternal tool integration: Projects may want to bridge kano backlog operations with external systems (JIRA sync, Slack notifications, custom dashboards).\n\n\nProblem: Hardcoding integrations into the core codebase violates separation of concerns and makes the skill harder to maintain.\nCurrent state: As of Phase 1-3 completion, kano_backlog_ops provides a clean use-case layer, but no hook/plugin mechanism exists.\nDecision\nDesign Principles\n\n\nInterface over Implementation: Define stable contracts (Python protocols/abstract base classes) that plugins must implement.\n\n\nRegistration-based Discovery: Plugins register themselves via entry points or configuration files; no hardcoded imports.\n\n\nFail-safe Defaults: If a plugin fails to load or execute, the system continues with default behavior (log warning but don’t crash).\n\n\nOptional Dependencies: Core skill must function without any plugins installed.\n\n\nProposed Architecture\nflowchart TB\r\n  subgraph CLI[&quot;kano CLI&quot;]\r\n    cmd[kano item create]\r\n  end\r\n  \r\n  subgraph Ops[&quot;kano_backlog_ops (Use-Cases)&quot;]\r\n    uc[create_item]\r\n  end\r\n  \r\n  subgraph Hooks[&quot;kano_backlog_hooks (Optional)&quot;]\r\n    mgr[HookManager]\r\n    pre[PreCreateHook protocol]\r\n    post[PostCreateHook protocol]\r\n  end\r\n  \r\n  subgraph Plugins[&quot;External Plugins (Optional)&quot;]\r\n    jira[jira-sync-plugin]\r\n    slack[slack-notify-plugin]\r\n    custom[custom-validation]\r\n  end\r\n  \r\n  subgraph Core[&quot;kano_backlog_core&quot;]\r\n    models[BacklogItem models]\r\n  end\r\n  \r\n  cmd --&gt; uc\r\n  uc --&gt; mgr\r\n  mgr --&gt; pre\r\n  pre -.optional.-&gt; jira\r\n  pre -.optional.-&gt; custom\r\n  mgr --&gt; post\r\n  post -.optional.-&gt; slack\r\n  uc --&gt; models\n\nHook Types\n\n\nOperation Hooks (Pre/Post):\n\nPreCreateHook: Validate/modify item before creation\nPostCreateHook: Notify external systems after creation\nPreUpdateStateHook: Block invalid state transitions\nPostUpdateStateHook: Trigger workflows on state change\n\n\n\nEngine Replacements:\n\nWorksetEngine: Interface for workset retrieval (default: Python, optional: Rust/C++)\nEmbeddingSearchEngine: Interface for ANN search (default: FAISS Python, optional: Qdrant/Milvus)\nIndexBuilder: Interface for index construction (default: SQLite Python, optional: DuckDB)\n\n\n\nPlugin Discovery\nOption A: Entry Points (Preferred for Python ecosystem)\n# pyproject.toml of external plugin\n[project.entry-points.&quot;kano_backlog.hooks&quot;]\njira-sync = &quot;jira_sync_plugin:JiraSyncHook&quot;\nOption B: Configuration File\n// _kano/backlog/.kano/plugins.json\n{\n  &quot;hooks&quot;: {\n    &quot;post_create&quot;: [&quot;jira_sync_plugin.JiraSyncHook&quot;, &quot;slack.NotifyHook&quot;]\n  },\n  &quot;engines&quot;: {\n    &quot;workset&quot;: &quot;workset_native.RustEngine&quot;\n  }\n}\nHook Protocol Example\nfrom typing import Protocol\nfrom kano_backlog_core.models import BacklogItem\n \nclass PreCreateHook(Protocol):\n    &quot;&quot;&quot;Protocol for pre-creation hooks.&quot;&quot;&quot;\n    \n    def execute(self, item: BacklogItem) -&gt; BacklogItem:\n        &quot;&quot;&quot;\n        Called before item creation.\n        \n        Args:\n            item: Item about to be created (may be modified)\n        \n        Returns:\n            Modified item (or original if no changes)\n        \n        Raises:\n            HookVetoError: If hook rejects the operation\n        &quot;&quot;&quot;\n        ...\n \nclass PostCreateHook(Protocol):\n    &quot;&quot;&quot;Protocol for post-creation hooks.&quot;&quot;&quot;\n    \n    def execute(self, item: BacklogItem) -&gt; None:\n        &quot;&quot;&quot;\n        Called after item creation.\n        \n        Args:\n            item: Newly created item (read-only)\n        \n        Note:\n            This should not raise exceptions; log errors internally\n        &quot;&quot;&quot;\n        ...\nConsequences\nPositive\n\nExtensibility: Projects can integrate custom logic without modifying skill core\nMaintainability: Core skill remains simple; complexity lives in plugins\nFlexibility: Users choose which plugins to install/enable\nEcosystem: Encourages community-contributed integrations\n\nNegative\n\nComplexity: Hook system adds architectural overhead (discovery, error handling, versioning)\nTesting burden: Must test core with/without plugins, handle plugin failures gracefully\nDocumentation: Need clear plugin development guide and hook lifecycle docs\nVersioning: Plugin API must be stable; breaking changes require migration path\n\nRisks\n\nPerformance overhead: Hook execution adds latency to every operation\n\nMitigation: Make hooks optional, measure overhead, provide async execution\n\n\nSecurity: Malicious plugins could corrupt backlog or leak data\n\nMitigation: Sandboxing (future), plugin allowlist/blocklist, audit logging\n\n\nDependency hell: Plugin A requires version X, plugin B requires version Y\n\nMitigation: Clear versioning policy, compatibility matrix\n\n\n\nImplementation Plan (When Needed)\nPhase 4a: Hook Interface (2-3 days)\n\nCreate src/kano_backlog_hooks/ package with protocol definitions\nAdd HookManager class for discovery and execution\nUpdate kano_backlog_ops to call hooks (with feature flag)\nWrite hook development guide\n\nPhase 4b: Engine Abstraction (3-5 days)\n\nDefine WorksetEngine, EmbeddingSearchEngine protocols\nRefactor existing code to use default implementations\nAdd engine registry and discovery\nDocument engine replacement guide\n\nPhase 4c: Plugin Ecosystem (Ongoing)\n\nCreate example plugins (jira-sync, slack-notify)\nPublish plugin template repository\nCurate community plugins list\n\nAlternatives Considered\n\n\nNo plugin system: Rely on users forking the skill\n\nRejected: Makes upgrades difficult, fragments ecosystem\n\n\n\nHardcoded integrations: Add JIRA/Slack support directly\n\nRejected: Violates separation of concerns, bloats codebase\n\n\n\nHTTP webhooks only: Use HTTP POST for all hooks\n\nRejected: Requires running server, adds network latency, no pre-hooks\n\n\n\nMonkey-patching: Let users patch functions at runtime\n\nRejected: Fragile, hard to test, no contract enforcement\n\n\n\nReferences\n\nADR-0013: Establishes use-case layer that hooks will integrate with\nPython Entry Points: packaging.python.org/en/latest/specifications/entry-points/\nProtocol classes (PEP 544): peps.python.org/pep-0544/\n\nRevision History\n\n2026-01-11: Initial proposal (copilot) - documented design for future Phase 4 implementation\n"},"adr/ADR-0015_decision-for-kabsd-tsk-0056":{"title":"Decision for KABSD-TSK-0056","links":[],"tags":[],"content":"Status\nProposed\nContext\nThis decision was extracted from workset notes for task/feature KABSD-TSK-0056.\nDecision\nUse fine-grained chunking with semantic section boundaries instead of fixed-size windows. This preserves logical document structure for better retrieval accuracy.\nRationale\nSee workset notes in _kano\\backlog\\sandboxes.cache\\019b8f52-9fc8-7c94-aa2d-806cacdd9086 for full context.\nConsequences\n\nPositive:\n\n…\n\n\nNegative:\n\n…\n\n\nNeutral:\n\n…\n\n\n\nAlternatives Considered\n\nAlternative A: …\nAlternative B: …\n\nReferences\n\nExtracted from workset for KABSD-TSK-0056 by copilot\n"},"adr/ADR-0015_skill-scoped-cli-namespace-convention":{"title":"Skill-Scoped CLI Namespace Convention","links":[],"tags":[],"content":"Decision\nEach skill repository MUST use skill-scoped naming for its CLI entrypoints and Python packages:\n\nCLI script: scripts/&lt;skill-name&gt; (e.g., scripts/kano-backlog)\nPython CLI package: &lt;skill_name&gt;_cli (e.g., kano_backlog_cli)\nConsole script entrypoint (in pyproject.toml): &lt;skill-name&gt; (e.g., kano-backlog)\n\nThe global name kano is reserved for a future umbrella CLI that will aggregate multiple skill CLIs. Individual skills MUST NOT claim the kano name in their own codebase.\nContext\nThe original implementation used universe-level names (kano, kano_cli) inside kano-agent-backlog-skill, assuming this was the only skill. As we add more skills (e.g., kano-commit-convention-skill), these names will collide.\nProblems with the current approach:\n\nkano is too generic for a single skill’s CLI\nMultiple skills can’t coexist if they all claim kano\nMigration confusion when the umbrella CLI is introduced later\n\nGoals:\n\nEnable multiple skill repos to coexist (each with its own scoped CLI)\nReserve kano as a future umbrella command aggregator\nMaintain consistency across all kano-ecosystem skills\n\nOptions Considered\n\n\nKeep kano as-is and ignore future skills ❌\n\nRejected: causes immediate collision when adding second skill.\n\n\n\nUse skill-scoped naming (kano-backlog, kano_backlog_cli) ✅ (chosen)\n\nEach skill is self-contained and independent.\nkano umbrella CLI can be added later without breaking existing skills.\n\n\n\nUse a monorepo with namespace packages\n\nRejected: conflicts with self-contained skill deployment model; skills are designed to be used as git submodules or standalone repos.\n\n\n\nPros / Cons\nPros:\n\nClear ownership: each skill owns its namespace\nNo collision when multiple skills are used together\nFuture-proof: umbrella CLI can be introduced as a separate repo\nAligns with ADR-0013 (module boundaries)\n\nCons:\n\nRequires renaming existing code (kano → kano-backlog, kano_cli → kano_backlog_cli)\nCommand tree changes (kano item → kano-backlog workitem)\nMigration for existing users (mitigated by deprecation wrapper)\n\nConsequences\nImmediate actions (EPIC-0009):\n\nRename scripts/kano → scripts/kano-backlog\nRename src/kano_cli → src/kano_backlog_cli\nUpdate pyproject.toml entrypoint: kano-backlog instead of kano\nRestructure command groups (item → workitem, backlog → admin)\nUpdate all documentation (SKILL.md, README.md, REFERENCE.md)\nProvide deprecated kano wrapper script with migration warning\n\nLong-term (out of scope for this repo):\n\nFuture kano umbrella CLI repo can implement command delegation (e.g., kano backlog &lt;cmd&gt; → kano-backlog &lt;cmd&gt;)\nEach skill continues to work standalone with its scoped CLI\n\nFollow-ups\n\n Implement renaming per KABSD-FTR-0034 (rename packages/scripts)\n Implement command tree restructuring per KABSD-FTR-0035\n Document reservation and future umbrella CLI design per KABSD-FTR-0036\n Update ADR-0013 to reference this naming convention\n"},"adr/ADR-0016_per-product-isolated-index-architecture":{"title":"Per-Product Isolated Index Architecture","links":["items/feature/0000/KABSD-FTR-0010_monorepo-platform-migration","items/task/0000/KABSD-TSK-0084_update-indexer-and-resolver-for-product-isolation"],"tags":[],"content":"Context\nIn a multi-product monorepo environment, we needed to decide between two architectural approaches for the SQLite index:\n\nPlatform-level shared index: All products write to a single _kano/backlog/_index/backlog.sqlite3\nPer-product isolated index: Each product owns its own index at products/&lt;name&gt;/_index/backlog.sqlite3\n\nDecision Criteria\nWe evaluated 7 dimensions:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDimensionPer-ProductPlatformWinnerIsolationComplete separationShared namespacePer-ProductConcurrencyNo locking contentionPotential locksPer-ProductPerformancePredictable, isolatedCan degrade with loadPer-ProductScalabilityLinear per productShared bottleneckPer-ProductComplexitySimple, independentComplex coordinationPer-ProductMaintenanceEasy (per-product)Harder (shared state)Per-ProductFuture ExtensibilitySupports embedding DBDifficult to addPer-Product\nOverall Score: Per-Product = 91%, Platform = 43%\nDecision\nImplement per-product isolated SQLite indexes.\nEach product will:\n\nMaintain its own SQLite database at products/&lt;product-name&gt;/_index/backlog.sqlite3\nHave independent schema with product column for self-documentation\nSupport autonomous indexing/rebuilding without affecting other products\nEnable future cross-product aggregation via product-aware queries\n\nImplementation Details\nSchema Design\nAll tables include product column:\nCREATE TABLE items (\n  product TEXT NOT NULL,\n  id TEXT NOT NULL,\n  PRIMARY KEY (product, id),\n  ...\n);\n \nCREATE TABLE item_tags (\n  product TEXT NOT NULL,\n  item_id TEXT NOT NULL,\n  tag TEXT NOT NULL,\n  PRIMARY KEY (product, item_id, tag),\n  FOREIGN KEY (product, item_id) REFERENCES items(product, id)\n);\nIndex Rebuild Process\n\nscripts/indexing/build_sqlite_index.py --product &lt;name&gt;\nScans products/&lt;name&gt;/items/ directory\nExtracts product from file paths\nUpserts into product-specific SQLite database\nMaintains composite key integrity\n\nResolver Behavior\n\nresolve_ref(ref, index, product=None) accepts optional product filter\nIf product not specified, searches across all products\nProduct column enables cross-product queries when needed\n\nRationale\n\nIsolation ensures safety: No possibility of data leakage between products\nPredictable performance: Each product’s index grows independently\nSupports autonomy: Teams can rebuild their product’s index without coordination\nFuture-proof: Enables embedding databases per product or shared aggregation\nSimpler mental model: Products are truly independent\nBackward compatible: Product column facilitates future migrations\n\nAlternatives Considered\nPlatform-level shared index\n\nPros: Unified search across all products\nCons: Complex coordination, shared bottleneck, potential data leakage\nRejected: Architecture does not support team autonomy\n\nHybrid approach (shared metadata + per-product data)\n\nPros: Balanced complexity\nCons: Still requires coordination layer, adds complexity\nRejected: Per-product isolation is simpler and cleaner\n\nFuture Work\nWhen cross-product features are needed:\n\nCreate aggregation layer on top of per-product indexes\nOr implement global embedding database that reads from per-product sources\nProduct column in schema already prepared for this extensibility\n\nReferences\n\nKABSD-FTR-0010_monorepo-platform-migration.md: Monorepo Platform Migration feature\nKABSD-TSK-0084_update-indexer-and-resolver-for-product-isolation.md: Indexer and resolver product isolation\n"},"adr/ADR-0017_product-column-retention-rationale":{"title":"Product Column Retention in Per-Product Indexes","links":["items/feature/0000/KABSD-FTR-0010_monorepo-platform-migration","adr/ADR-0016_per-product-isolated-index-architecture"],"tags":[],"content":"Context\nWhen implementing per-product isolated indexes, a question arose: if each product owns its own SQLite database, why include a product column in the schema?\nArguments for removing it:\n\nRedundant (the file path already indicates product)\nAdds storage overhead\nColumn values are always the same for a given database\n\nArguments for keeping it:\n\nSelf-documentation (data independence from file paths)\nConsistency with composite key patterns\nFuture extensibility (global embedding DB, cross-product queries)\nError detection capability\n\nDecision\nRetain the product column in all schema tables.\nEach product’s schema will include a product column despite the apparent redundancy.\nRationale\n1. Data Self-Documentation\nThe product column makes data self-describing. If a database dump or export is created, the product context is explicit in the data itself, not dependent on file path or metadata.\nSELECT * FROM items WHERE product=&#039;kano-agent-backlog-skill&#039;;\n-- vs.\nSELECT * FROM items;  -- How do you know which product this is from?\n2. Consistency with Composite Key Strategy\nUsing (product, id) as composite primary keys throughout the schema ensures:\n\nAll foreign keys are uniform: FOREIGN KEY (product, item_id)\nConsistent pattern across all tables (items, item_tags, item_links, worklog_entries)\nEasier code generation and migrations\n\n3. Uniform Schema Across Products\nAll products use identical schema structure. A tool that works with KABSD’s index works with any product’s index without modification. This is valuable for:\n\nShared tool development\nSchema migration scripts\nIndex validation and repair tools\n\n4. Future Extensibility: Global Embedding Database\nThe primary long-term value is supporting a future global embedding database:\n-- Global embedding store (future)\nCREATE TABLE embeddings (\n  product TEXT NOT NULL,\n  item_id TEXT NOT NULL,\n  embedding BLOB NOT NULL,\n  PRIMARY KEY (product, item_id),\n  FOREIGN KEY (product, item_id) REFERENCES all_items(product, id)\n);\nThis would aggregate embeddings from all products while maintaining product context. The product column enables this without schema rework.\n5. Error Detection\nIncluding product column in per-product indexes enables validation queries:\n-- Verify database integrity\nSELECT DISTINCT product FROM items;\n-- Should always return single row: kano-agent-backlog-skill\n \n-- Detect misplaced files\nSELECT product FROM items WHERE product != &#039;kano-agent-backlog-skill&#039;;\n-- Should return empty set\n6. Cross-Product Queries (Future)\nWhen analytics or reporting tools are built, they may aggregate across products:\n-- Future: Query across products via union or aggregation\nSELECT product, COUNT(*) as item_count FROM all_items GROUP BY product;\nThe product column is essential for this without painful migrations.\nImplementation\nAll tables maintain the pattern:\nCREATE TABLE items (\n  product TEXT NOT NULL,\n  id TEXT NOT NULL,\n  uid UUID,\n  type TEXT,\n  -- ... other columns\n  PRIMARY KEY (product, id)\n);\n \nCREATE TABLE item_tags (\n  product TEXT NOT NULL,\n  item_id TEXT NOT NULL,\n  tag TEXT NOT NULL,\n  PRIMARY KEY (product, item_id, tag),\n  FOREIGN KEY (product, item_id) REFERENCES items(product, id)\n);\nAlternatives Considered\nRemove product column entirely\n\nPros: Minimal storage, no apparent redundancy\nCons: Breaks future embedding DB design, harder to validate integrity, poor data self-documentation\nRejected: Future extensibility cost is too high\n\nMake product optional/nullable\n\nPros: Allows querying “which product” implicitly\nCons: Makes schema ambiguous, difficult to enforce consistency\nRejected: Product should always be explicit and required\n\nFuture Work\nWhen global embedding database is implemented:\n\nProduct column in schema already prepared\nNo schema migration required\nTools can immediately work with cross-product data\n\nReferences\n\nKABSD-FTR-0010_monorepo-platform-migration.md: Monorepo Platform Migration\nADR-0016_per-product-isolated-index-architecture: Per-Product Isolated Index Architecture\n"},"adr/ADR-0018_decision-for-kabsd-tsk-0056":{"title":"Decision for KABSD-TSK-0056","links":[],"tags":[],"content":"Status\nProposed\nContext\nThis decision was extracted from workset notes for task/feature KABSD-TSK-0056.\nDecision\nUse fine-grained chunking with semantic section boundaries instead of fixed-size windows. This preserves logical document structure for better retrieval accuracy.\nRationale\nSee workset notes in _kano\\backlog\\sandboxes.cache\\019b8f52-9fc8-7c94-aa2d-806cacdd9086 for full context.\nConsequences\n\nPositive:\n\n…\n\n\nNegative:\n\n…\n\n\nNeutral:\n\n…\n\n\n\nAlternatives Considered\n\nAlternative A: …\nAlternative B: …\n\nReferences\n\nExtracted from workset for KABSD-TSK-0056 by copilot\n"},"adr/ADR-0019_decision-for-kabsd-tsk-0056":{"title":"Decision for KABSD-TSK-0056","links":[],"tags":[],"content":"Status\nProposed\nContext\nThis decision was extracted from workset notes for task/feature KABSD-TSK-0056.\nDecision\nUse fine-grained chunking with semantic section boundaries instead of fixed-size windows. This preserves logical document structure for better retrieval accuracy.\nRationale\nSee workset notes in _kano\\backlog\\sandboxes.cache\\019b8f52-9fc8-7c94-aa2d-806cacdd9086 for full context.\nConsequences\n\nPositive:\n\n…\n\n\nNegative:\n\n…\n\n\nNeutral:\n\n…\n\n\n\nAlternatives Considered\n\nAlternative A: …\nAlternative B: …\n\nReferences\n\nExtracted from workset for KABSD-TSK-0056 by copilot\n"},"adr/ADR-0020_decision-for-kabsd-tsk-0056":{"title":"Decision for KABSD-TSK-0056","links":[],"tags":[],"content":"Status\nProposed\nContext\nThis decision was extracted from workset notes for task/feature KABSD-TSK-0056.\nDecision\nUse fine-grained chunking with semantic section boundaries instead of fixed-size windows. This preserves logical document structure for better retrieval accuracy.\nRationale\nSee workset notes in _kano\\backlog\\sandboxes.cache\\019b8f52-9fc8-7c94-aa2d-806cacdd9086 for full context.\nConsequences\n\nPositive:\n\n…\n\n\nNegative:\n\n…\n\n\nNeutral:\n\n…\n\n\n\nAlternatives Considered\n\nAlternative A: …\nAlternative B: …\n\nReferences\n\nExtracted from workset for KABSD-TSK-0056 by copilot\n"},"adr/ADR-0021_per-product-isolated-index-architecture":{"title":"Per-Product Isolated Index Architecture","links":["items/feature/0000/KABSD-FTR-0010_monorepo-platform-migration","items/task/0000/KABSD-TSK-0084_update-indexer-and-resolver-for-product-isolation"],"tags":[],"content":"Context\nIn a multi-product monorepo environment, we needed to decide between two architectural approaches for the SQLite index:\n\nPlatform-level shared index: All products write to a single _kano/backlog/_index/backlog.sqlite3\nPer-product isolated index: Each product owns its own index at products/&lt;name&gt;/_index/backlog.sqlite3\n\nDecision Criteria\nWe evaluated 7 dimensions:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDimensionPer-ProductPlatformWinnerIsolationComplete separationShared namespacePer-ProductConcurrencyNo locking contentionPotential locksPer-ProductPerformancePredictable, isolatedCan degrade with loadPer-ProductScalabilityLinear per productShared bottleneckPer-ProductComplexitySimple, independentComplex coordinationPer-ProductMaintenanceEasy (per-product)Harder (shared state)Per-ProductFuture ExtensibilitySupports embedding DBDifficult to addPer-Product\nOverall Score: Per-Product = 91%, Platform = 43%\nDecision\nImplement per-product isolated SQLite indexes.\nEach product will:\n\nMaintain its own SQLite database at products/&lt;product-name&gt;/_index/backlog.sqlite3\nHave independent schema with product column for self-documentation\nSupport autonomous indexing/rebuilding without affecting other products\nEnable future cross-product aggregation via product-aware queries\n\nImplementation Details\nSchema Design\nAll tables include product column:\nCREATE TABLE items (\n  product TEXT NOT NULL,\n  id TEXT NOT NULL,\n  PRIMARY KEY (product, id),\n  ...\n);\n \nCREATE TABLE item_tags (\n  product TEXT NOT NULL,\n  item_id TEXT NOT NULL,\n  tag TEXT NOT NULL,\n  PRIMARY KEY (product, item_id, tag),\n  FOREIGN KEY (product, item_id) REFERENCES items(product, id)\n);\nIndex Rebuild Process\n\nscripts/indexing/build_sqlite_index.py --product &lt;name&gt;\nScans products/&lt;name&gt;/items/ directory\nExtracts product from file paths\nUpserts into product-specific SQLite database\nMaintains composite key integrity\n\nResolver Behavior\n\nresolve_ref(ref, index, product=None) accepts optional product filter\nIf product not specified, searches across all products\nProduct column enables cross-product queries when needed\n\nRationale\n\nIsolation ensures safety: No possibility of data leakage between products\nPredictable performance: Each product’s index grows independently\nSupports autonomy: Teams can rebuild their product’s index without coordination\nFuture-proof: Enables embedding databases per product or shared aggregation\nSimpler mental model: Products are truly independent\nBackward compatible: Product column facilitates future migrations\n\nAlternatives Considered\nPlatform-level shared index\n\nPros: Unified search across all products\nCons: Complex coordination, shared bottleneck, potential data leakage\nRejected: Architecture does not support team autonomy\n\nHybrid approach (shared metadata + per-product data)\n\nPros: Balanced complexity\nCons: Still requires coordination layer, adds complexity\nRejected: Per-product isolation is simpler and cleaner\n\nFuture Work\nWhen cross-product features are needed:\n\nCreate aggregation layer on top of per-product indexes\nOr implement global embedding database that reads from per-product sources\nProduct column in schema already prepared for this extensibility\n\nReferences\n\nKABSD-FTR-0010_monorepo-platform-migration.md: Monorepo Platform Migration feature\nKABSD-TSK-0084_update-indexer-and-resolver-for-product-isolation.md: Indexer and resolver product isolation\n"},"adr/ADR-0022_product-column-retention-rationale":{"title":"Product Column Retention in Per-Product Indexes","links":["items/feature/0000/KABSD-FTR-0010_monorepo-platform-migration","adr/ADR-0004_file-first-architecture-with-sqlite-index"],"tags":[],"content":"Context\nWhen implementing per-product isolated indexes, a question arose: if each product owns its own SQLite database, why include a product column in the schema?\nArguments for removing it:\n\nRedundant (the file path already indicates product)\nAdds storage overhead\nColumn values are always the same for a given database\n\nArguments for keeping it:\n\nSelf-documentation (data independence from file paths)\nConsistency with composite key patterns\nFuture extensibility (global embedding DB, cross-product queries)\nError detection capability\n\nDecision\nRetain the product column in all schema tables.\nEach product’s schema will include a product column despite the apparent redundancy.\nRationale\n1. Data Self-Documentation\nThe product column makes data self-describing. If a database dump or export is created, the product context is explicit in the data itself, not dependent on file path or metadata.\nSELECT * FROM items WHERE product=&#039;kano-agent-backlog-skill&#039;;\n-- vs.\nSELECT * FROM items;  -- How do you know which product this is from?\n2. Consistency with Composite Key Strategy\nUsing (product, id) as composite primary keys throughout the schema ensures:\n\nAll foreign keys are uniform: FOREIGN KEY (product, item_id)\nConsistent pattern across all tables (items, item_tags, item_links, worklog_entries)\nEasier code generation and migrations\n\n3. Uniform Schema Across Products\nAll products use identical schema structure. A tool that works with KABSD’s index works with any product’s index without modification. This is valuable for:\n\nShared tool development\nSchema migration scripts\nIndex validation and repair tools\n\n4. Future Extensibility: Global Embedding Database\nThe primary long-term value is supporting a future global embedding database:\n-- Global embedding store (future)\nCREATE TABLE embeddings (\n  product TEXT NOT NULL,\n  item_id TEXT NOT NULL,\n  embedding BLOB NOT NULL,\n  PRIMARY KEY (product, item_id),\n  FOREIGN KEY (product, item_id) REFERENCES all_items(product, id)\n);\nThis would aggregate embeddings from all products while maintaining product context. The product column enables this without schema rework.\n5. Error Detection\nIncluding product column in per-product indexes enables validation queries:\n-- Verify database integrity\nSELECT DISTINCT product FROM items;\n-- Should always return single row: kano-agent-backlog-skill\n \n-- Detect misplaced files\nSELECT product FROM items WHERE product != &#039;kano-agent-backlog-skill&#039;;\n-- Should return empty set\n6. Cross-Product Queries (Future)\nWhen analytics or reporting tools are built, they may aggregate across products:\n-- Future: Query across products via union or aggregation\nSELECT product, COUNT(*) as item_count FROM all_items GROUP BY product;\nThe product column is essential for this without painful migrations.\nImplementation\nAll tables maintain the pattern:\nCREATE TABLE items (\n  product TEXT NOT NULL,\n  id TEXT NOT NULL,\n  uid UUID,\n  type TEXT,\n  -- ... other columns\n  PRIMARY KEY (product, id)\n);\n \nCREATE TABLE item_tags (\n  product TEXT NOT NULL,\n  item_id TEXT NOT NULL,\n  tag TEXT NOT NULL,\n  PRIMARY KEY (product, item_id, tag),\n  FOREIGN KEY (product, item_id) REFERENCES items(product, id)\n);\nAlternatives Considered\nRemove product column entirely\n\nPros: Minimal storage, no apparent redundancy\nCons: Breaks future embedding DB design, harder to validate integrity, poor data self-documentation\nRejected: Future extensibility cost is too high\n\nMake product optional/nullable\n\nPros: Allows querying “which product” implicitly\nCons: Makes schema ambiguous, difficult to enforce consistency\nRejected: Product should always be explicit and required\n\nFuture Work\nWhen global embedding database is implemented:\n\nProduct column in schema already prepared\nNo schema migration required\nTools can immediately work with cross-product data\n\nReferences\n\nKABSD-FTR-0010_monorepo-platform-migration.md: Monorepo Platform Migration\nADR-0004_file-first-architecture-with-sqlite-index: Per-Product Isolated Index Architecture\n"},"adr/ADR-0023_graph-assisted-retrieval-and-context-graph":{"title":"Graph-assisted retrieval with a derived Context Graph (weak graph first)","links":["adr/ADR-0004_file-first-architecture-with-sqlite-index","adr/ADR-0009_local-first-embedding-search-architecture","items/feature/0000/KABSD-FTR-0007_optional-db-index-and-embedding-rag-pipeline","items/feature/0000/KABSD-FTR-0023_graph-assisted-rag-planning-and-minimal-implementation"],"tags":[],"content":"Decision\nAdopt Graph-assisted retrieval as a minimal, local-first improvement to context quality:\n\nUse FTS/embeddings to retrieve seed nodes\nUse a derived Context Graph to expand to load-bearing neighbors (k-hop traversal)\nKeep everything derived/rebuildable from canonical Markdown (file-first)\n\nThis ADR explicitly chooses weak graph first: only structured relationships (no LLM entity extraction).\nContext\nWe already have a file-first backlog with optional derived indexes (SQLite / FTS / embeddings).\r\nVector-only retrieval often returns text-similar chunks but misses the structural context (parents, ADR decisions, dependency chains).\nWe want a deterministic, auditable way to expand context that is:\n\nlocal-first\nderived/rebuildable\nincrementally maintainable\nsafe (bounded expansion to avoid prompt bloat)\n\nDefinitions\n\nContext Graph: a derived, typed graph of artifact relationships (items, ADRs, dependencies, etc.).\nSeed set: top-N nodes from FTS/embedding retrieval.\nGraph expansion: k-hop traversal from seeds over allowlisted edges with limits.\n\nGraph model (v1)\nNodes\nMinimum node types:\n\nwork_item (Epic/Feature/UserStory/Task/Bug)\nadr\n\nOptional (for embedding/fts pipelines):\n\nchunk (document chunk tied to a parent doc)\n\nEdges\nMinimum edge types:\n\nparent (child → parent)\ndecision_ref (work_item → adr)\nrelates (work_item → work_item)\nblocks / blocked_by\n\nStorage (derived)\nThe Context Graph is derived data. Implementations may:\n\n\nMaterialize into SQLite\n\nreuse items as the node registry\nstore edges in a links-style table (source_uid, target_uid, type, optional weight, source_path)\n\n\n\nSidecar graph artifacts\n\n&lt;backlog-root&gt;/_index/graph_nodes.jsonl\n&lt;backlog-root&gt;/_index/graph_edges.jsonl\n\n\n\nBoth must be safe to delete and rebuild.\nRetrieval strategy (Graph-assisted RAG)\n\nSeed retrieval\n\nFTS and/or embeddings return top-N seed nodes/chunks\n\n\nExpand\n\ntraverse k-hop (default k=1)\nedge allowlist and fanout caps\n\n\nRe-rank\n\nweights by doctype and section (ADR decision &gt; item title/acceptance &gt; worklog)\noptionally prioritize Ready/InProgress items\n\n\nContext packing\n\nemit a context pack describing:\n\nseeds (why selected)\nneighbors (which edge pulled them in)\nminimal excerpts/anchors (title/ids/links)\n\n\n\n\n\nConfig surface (indicative)\n\nretrieval.graph.enabled\nretrieval.graph.k_hop\nretrieval.graph.edge_allowlist\nretrieval.graph.max_neighbors_per_seed\nretrieval.weights.* (doctype/section/state weights)\n\nConsequences\n\nGraph-assisted retrieval becomes the preferred way to preserve traceability (seed + neighbors).\nTooling must keep expansion bounded to avoid context explosions.\nThe design stays compatible with file-scan fallback and optional SQLite/embedding acceleration.\n\nNon-goals\n\nLLM/NLP entity extraction and automatic relation mining\nserver/MCP mode or cross-repo graphs\n\nReferences\n\nADR-0004 File-first + SQLite index\nADR-0009 Local-first embedding search\nRAG pipeline\nKABSD-FTR-0023 Graph-assisted RAG planning\n"},"adr/ADR-0024_decision-for-kabsd-tsk-0056":{"title":"Decision for KABSD-TSK-0056","links":[],"tags":[],"content":"Status\nProposed\nContext\nThis decision was extracted from workset notes for task/feature KABSD-TSK-0056.\nDecision\nUse fine-grained chunking with semantic section boundaries instead of fixed-size windows. This preserves logical document structure for better retrieval accuracy.\nRationale\nSee workset notes in _kano\\backlog\\sandboxes.cache\\019b8f52-9fc8-7c94-aa2d-806cacdd9086 for full context.\nConsequences\n\nPositive:\n\n…\n\n\nNegative:\n\n…\n\n\nNeutral:\n\n…\n\n\n\nAlternatives Considered\n\nAlternative A: …\nAlternative B: …\n\nReferences\n\nExtracted from workset for KABSD-TSK-0056 by copilot\n"},"adr/ADR-0025_decision-for-kabsd-tsk-0056":{"title":"Decision for KABSD-TSK-0056","links":[],"tags":[],"content":"Status\nProposed\nContext\nThis decision was extracted from workset notes for task/feature KABSD-TSK-0056.\nDecision\nUse fine-grained chunking with semantic section boundaries instead of fixed-size windows. This preserves logical document structure for better retrieval accuracy.\nRationale\nSee workset notes in _kano\\backlog\\sandboxes.cache\\019b8f52-9fc8-7c94-aa2d-806cacdd9086 for full context.\nConsequences\n\nPositive:\n\n…\n\n\nNegative:\n\n…\n\n\nNeutral:\n\n…\n\n\n\nAlternatives Considered\n\nAlternative A: …\nAlternative B: …\n\nReferences\n\nExtracted from workset for KABSD-TSK-0056 by copilot\n"},"adr/ADR-0026_decision-for-kabsd-tsk-0056":{"title":"Decision for KABSD-TSK-0056","links":[],"tags":[],"content":"Status\nProposed\nContext\nThis decision was extracted from workset notes for task/feature KABSD-TSK-0056.\nDecision\nUse fine-grained chunking with semantic section boundaries instead of fixed-size windows. This preserves logical document structure for better retrieval accuracy.\nRationale\nSee workset notes in _kano\\backlog\\sandboxes.cache\\019b8f52-9fc8-7c94-aa2d-806cacdd9086 for full context.\nConsequences\n\nPositive:\n\n…\n\n\nNegative:\n\n…\n\n\nNeutral:\n\n…\n\n\n\nAlternatives Considered\n\nAlternative A: …\nAlternative B: …\n\nReferences\n\nExtracted from workset for KABSD-TSK-0056 by copilot\n"},"adr/ADR-0027_decision-for-kabsd-tsk-0056":{"title":"Decision for KABSD-TSK-0056","links":[],"tags":[],"content":"Status\nProposed\nContext\nThis decision was extracted from workset notes for task/feature KABSD-TSK-0056.\nDecision\nUse fine-grained chunking with semantic section boundaries instead of fixed-size windows. This preserves logical document structure for better retrieval accuracy.\nRationale\nSee workset notes in _kano\\backlog\\sandboxes.cache\\019b8f52-9fc8-7c94-aa2d-806cacdd9086 for full context.\nConsequences\n\nPositive:\n\n…\n\n\nNegative:\n\n…\n\n\nNeutral:\n\n…\n\n\n\nAlternatives Considered\n\nAlternative A: …\nAlternative B: …\n\nReferences\n\nExtracted from workset for KABSD-TSK-0056 by copilot\n"},"adr/ADR-0028_per-product-isolated-index-architecture":{"title":"Per-Product Isolated Index Architecture","links":["items/feature/0000/KABSD-FTR-0010_monorepo-platform-migration","items/task/0000/KABSD-TSK-0084_update-indexer-and-resolver-for-product-isolation"],"tags":[],"content":"Context\nIn a multi-product monorepo environment, we needed to decide between two architectural approaches for the SQLite index:\n\nPlatform-level shared index: All products write to a single _kano/backlog/_index/backlog.sqlite3\nPer-product isolated index: Each product owns its own index at products/&lt;name&gt;/_index/backlog.sqlite3\n\nDecision Criteria\nWe evaluated 7 dimensions:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDimensionPer-ProductPlatformWinnerIsolationComplete separationShared namespacePer-ProductConcurrencyNo locking contentionPotential locksPer-ProductPerformancePredictable, isolatedCan degrade with loadPer-ProductScalabilityLinear per productShared bottleneckPer-ProductComplexitySimple, independentComplex coordinationPer-ProductMaintenanceEasy (per-product)Harder (shared state)Per-ProductFuture ExtensibilitySupports embedding DBDifficult to addPer-Product\nOverall Score: Per-Product = 91%, Platform = 43%\nDecision\nImplement per-product isolated SQLite indexes.\nEach product will:\n\nMaintain its own SQLite database at products/&lt;product-name&gt;/_index/backlog.sqlite3\nHave independent schema with product column for self-documentation\nSupport autonomous indexing/rebuilding without affecting other products\nEnable future cross-product aggregation via product-aware queries\n\nImplementation Details\nSchema Design\nAll tables include product column:\nCREATE TABLE items (\n  product TEXT NOT NULL,\n  id TEXT NOT NULL,\n  PRIMARY KEY (product, id),\n  ...\n);\n \nCREATE TABLE item_tags (\n  product TEXT NOT NULL,\n  item_id TEXT NOT NULL,\n  tag TEXT NOT NULL,\n  PRIMARY KEY (product, item_id, tag),\n  FOREIGN KEY (product, item_id) REFERENCES items(product, id)\n);\nIndex Rebuild Process\n\nscripts/indexing/build_sqlite_index.py --product &lt;name&gt;\nScans products/&lt;name&gt;/items/ directory\nExtracts product from file paths\nUpserts into product-specific SQLite database\nMaintains composite key integrity\n\nResolver Behavior\n\nresolve_ref(ref, index, product=None) accepts optional product filter\nIf product not specified, searches across all products\nProduct column enables cross-product queries when needed\n\nRationale\n\nIsolation ensures safety: No possibility of data leakage between products\nPredictable performance: Each product’s index grows independently\nSupports autonomy: Teams can rebuild their product’s index without coordination\nFuture-proof: Enables embedding databases per product or shared aggregation\nSimpler mental model: Products are truly independent\nBackward compatible: Product column facilitates future migrations\n\nAlternatives Considered\nPlatform-level shared index\n\nPros: Unified search across all products\nCons: Complex coordination, shared bottleneck, potential data leakage\nRejected: Architecture does not support team autonomy\n\nHybrid approach (shared metadata + per-product data)\n\nPros: Balanced complexity\nCons: Still requires coordination layer, adds complexity\nRejected: Per-product isolation is simpler and cleaner\n\nFuture Work\nWhen cross-product features are needed:\n\nCreate aggregation layer on top of per-product indexes\nOr implement global embedding database that reads from per-product sources\nProduct column in schema already prepared for this extensibility\n\nReferences\n\nKABSD-FTR-0010_monorepo-platform-migration.md: Monorepo Platform Migration feature\nKABSD-TSK-0084_update-indexer-and-resolver-for-product-isolation.md: Indexer and resolver product isolation\n"},"adr/ADR-0029_product-column-retention-rationale":{"title":"Product Column Retention in Per-Product Indexes","links":["items/feature/0000/KABSD-FTR-0010_monorepo-platform-migration","adr/ADR-0004_file-first-architecture-with-sqlite-index"],"tags":[],"content":"Context\nWhen implementing per-product isolated indexes, a question arose: if each product owns its own SQLite database, why include a product column in the schema?\nArguments for removing it:\n\nRedundant (the file path already indicates product)\nAdds storage overhead\nColumn values are always the same for a given database\n\nArguments for keeping it:\n\nSelf-documentation (data independence from file paths)\nConsistency with composite key patterns\nFuture extensibility (global embedding DB, cross-product queries)\nError detection capability\n\nDecision\nRetain the product column in all schema tables.\nEach product’s schema will include a product column despite the apparent redundancy.\nRationale\n1. Data Self-Documentation\nThe product column makes data self-describing. If a database dump or export is created, the product context is explicit in the data itself, not dependent on file path or metadata.\nSELECT * FROM items WHERE product=&#039;kano-agent-backlog-skill&#039;;\n-- vs.\nSELECT * FROM items;  -- How do you know which product this is from?\n2. Consistency with Composite Key Strategy\nUsing (product, id) as composite primary keys throughout the schema ensures:\n\nAll foreign keys are uniform: FOREIGN KEY (product, item_id)\nConsistent pattern across all tables (items, item_tags, item_links, worklog_entries)\nEasier code generation and migrations\n\n3. Uniform Schema Across Products\nAll products use identical schema structure. A tool that works with KABSD’s index works with any product’s index without modification. This is valuable for:\n\nShared tool development\nSchema migration scripts\nIndex validation and repair tools\n\n4. Future Extensibility: Global Embedding Database\nThe primary long-term value is supporting a future global embedding database:\n-- Global embedding store (future)\nCREATE TABLE embeddings (\n  product TEXT NOT NULL,\n  item_id TEXT NOT NULL,\n  embedding BLOB NOT NULL,\n  PRIMARY KEY (product, item_id),\n  FOREIGN KEY (product, item_id) REFERENCES all_items(product, id)\n);\nThis would aggregate embeddings from all products while maintaining product context. The product column enables this without schema rework.\n5. Error Detection\nIncluding product column in per-product indexes enables validation queries:\n-- Verify database integrity\nSELECT DISTINCT product FROM items;\n-- Should always return single row: kano-agent-backlog-skill\n \n-- Detect misplaced files\nSELECT product FROM items WHERE product != &#039;kano-agent-backlog-skill&#039;;\n-- Should return empty set\n6. Cross-Product Queries (Future)\nWhen analytics or reporting tools are built, they may aggregate across products:\n-- Future: Query across products via union or aggregation\nSELECT product, COUNT(*) as item_count FROM all_items GROUP BY product;\nThe product column is essential for this without painful migrations.\nImplementation\nAll tables maintain the pattern:\nCREATE TABLE items (\n  product TEXT NOT NULL,\n  id TEXT NOT NULL,\n  uid UUID,\n  type TEXT,\n  -- ... other columns\n  PRIMARY KEY (product, id)\n);\n \nCREATE TABLE item_tags (\n  product TEXT NOT NULL,\n  item_id TEXT NOT NULL,\n  tag TEXT NOT NULL,\n  PRIMARY KEY (product, item_id, tag),\n  FOREIGN KEY (product, item_id) REFERENCES items(product, id)\n);\nAlternatives Considered\nRemove product column entirely\n\nPros: Minimal storage, no apparent redundancy\nCons: Breaks future embedding DB design, harder to validate integrity, poor data self-documentation\nRejected: Future extensibility cost is too high\n\nMake product optional/nullable\n\nPros: Allows querying “which product” implicitly\nCons: Makes schema ambiguous, difficult to enforce consistency\nRejected: Product should always be explicit and required\n\nFuture Work\nWhen global embedding database is implemented:\n\nProduct column in schema already prepared\nNo schema migration required\nTools can immediately work with cross-product data\n\nReferences\n\nKABSD-FTR-0010_monorepo-platform-migration.md: Monorepo Platform Migration\nADR-0004_file-first-architecture-with-sqlite-index: Per-Product Isolated Index Architecture\n"},"adr/ADR-0030_graph-assisted-retrieval-and-context-graph":{"title":"Graph-assisted retrieval with a derived Context Graph (weak graph first)","links":["adr/ADR-0004_file-first-architecture-with-sqlite-index","adr/ADR-0009_local-first-embedding-search-architecture","items/feature/0000/KABSD-FTR-0007_optional-db-index-and-embedding-rag-pipeline","items/feature/0000/KABSD-FTR-0023_graph-assisted-rag-planning-and-minimal-implementation"],"tags":[],"content":"Decision\nAdopt Graph-assisted retrieval as a minimal, local-first improvement to context quality:\n\nUse FTS/embeddings to retrieve seed nodes\nUse a derived Context Graph to expand to load-bearing neighbors (k-hop traversal)\nKeep everything derived/rebuildable from canonical Markdown (file-first)\n\nThis ADR explicitly chooses weak graph first: only structured relationships (no LLM entity extraction).\nContext\nWe already have a file-first backlog with optional derived indexes (SQLite / FTS / embeddings).\r\nVector-only retrieval often returns text-similar chunks but misses the structural context (parents, ADR decisions, dependency chains).\nWe want a deterministic, auditable way to expand context that is:\n\nlocal-first\nderived/rebuildable\nincrementally maintainable\nsafe (bounded expansion to avoid prompt bloat)\n\nDefinitions\n\nContext Graph: a derived, typed graph of artifact relationships (items, ADRs, dependencies, etc.).\nSeed set: top-N nodes from FTS/embedding retrieval.\nGraph expansion: k-hop traversal from seeds over allowlisted edges with limits.\n\nGraph model (v1)\nNodes\nMinimum node types:\n\nwork_item (Epic/Feature/UserStory/Task/Bug)\nadr\n\nOptional (for embedding/fts pipelines):\n\nchunk (document chunk tied to a parent doc)\n\nEdges\nMinimum edge types:\n\nparent (child → parent)\ndecision_ref (work_item → adr)\nrelates (work_item → work_item)\nblocks / blocked_by\n\nStorage (derived)\nThe Context Graph is derived data. Implementations may:\n\n\nMaterialize into SQLite\n\nreuse items as the node registry\nstore edges in a links-style table (source_uid, target_uid, type, optional weight, source_path)\n\n\n\nSidecar graph artifacts\n\n&lt;backlog-root&gt;/_index/graph_nodes.jsonl\n&lt;backlog-root&gt;/_index/graph_edges.jsonl\n\n\n\nBoth must be safe to delete and rebuild.\nRetrieval strategy (Graph-assisted RAG)\n\nSeed retrieval\n\nFTS and/or embeddings return top-N seed nodes/chunks\n\n\nExpand\n\ntraverse k-hop (default k=1)\nedge allowlist and fanout caps\n\n\nRe-rank\n\nweights by doctype and section (ADR decision &gt; item title/acceptance &gt; worklog)\noptionally prioritize Ready/InProgress items\n\n\nContext packing\n\nemit a context pack describing:\n\nseeds (why selected)\nneighbors (which edge pulled them in)\nminimal excerpts/anchors (title/ids/links)\n\n\n\n\n\nConfig surface (indicative)\n\nretrieval.graph.enabled\nretrieval.graph.k_hop\nretrieval.graph.edge_allowlist\nretrieval.graph.max_neighbors_per_seed\nretrieval.weights.* (doctype/section/state weights)\n\nConsequences\n\nGraph-assisted retrieval becomes the preferred way to preserve traceability (seed + neighbors).\nTooling must keep expansion bounded to avoid context explosions.\nThe design stays compatible with file-scan fallback and optional SQLite/embedding acceleration.\n\nNon-goals\n\nLLM/NLP entity extraction and automatic relation mining\nserver/MCP mode or cross-repo graphs\n\nReferences\n\nADR-0004 File-first + SQLite index\nADR-0009 Local-first embedding search\nRAG pipeline\nKABSD-FTR-0023 Graph-assisted RAG planning\n"},"adr/ADR-0031_per-product-isolated-index-architecture":{"title":"Per-Product Isolated Index Architecture","links":["items/feature/0000/KABSD-FTR-0010_monorepo-platform-migration","items/task/0000/KABSD-TSK-0084_update-indexer-and-resolver-for-product-isolation"],"tags":[],"content":"Context\nIn a multi-product monorepo environment, we needed to decide between two architectural approaches for the SQLite index:\n\nPlatform-level shared index: All products write to a single _kano/backlog/_index/backlog.sqlite3\nPer-product isolated index: Each product owns its own index at products/&lt;name&gt;/_index/backlog.sqlite3\n\nDecision Criteria\nWe evaluated 7 dimensions:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDimensionPer-ProductPlatformWinnerIsolationComplete separationShared namespacePer-ProductConcurrencyNo locking contentionPotential locksPer-ProductPerformancePredictable, isolatedCan degrade with loadPer-ProductScalabilityLinear per productShared bottleneckPer-ProductComplexitySimple, independentComplex coordinationPer-ProductMaintenanceEasy (per-product)Harder (shared state)Per-ProductFuture ExtensibilitySupports embedding DBDifficult to addPer-Product\nOverall Score: Per-Product = 91%, Platform = 43%\nDecision\nImplement per-product isolated SQLite indexes.\nEach product will:\n\nMaintain its own SQLite database at products/&lt;product-name&gt;/_index/backlog.sqlite3\nHave independent schema with product column for self-documentation\nSupport autonomous indexing/rebuilding without affecting other products\nEnable future cross-product aggregation via product-aware queries\n\nImplementation Details\nSchema Design\nAll tables include product column:\nCREATE TABLE items (\n  product TEXT NOT NULL,\n  id TEXT NOT NULL,\n  PRIMARY KEY (product, id),\n  ...\n);\n \nCREATE TABLE item_tags (\n  product TEXT NOT NULL,\n  item_id TEXT NOT NULL,\n  tag TEXT NOT NULL,\n  PRIMARY KEY (product, item_id, tag),\n  FOREIGN KEY (product, item_id) REFERENCES items(product, id)\n);\nIndex Rebuild Process\n\nscripts/indexing/build_sqlite_index.py --product &lt;name&gt;\nScans products/&lt;name&gt;/items/ directory\nExtracts product from file paths\nUpserts into product-specific SQLite database\nMaintains composite key integrity\n\nResolver Behavior\n\nresolve_ref(ref, index, product=None) accepts optional product filter\nIf product not specified, searches across all products\nProduct column enables cross-product queries when needed\n\nRationale\n\nIsolation ensures safety: No possibility of data leakage between products\nPredictable performance: Each product’s index grows independently\nSupports autonomy: Teams can rebuild their product’s index without coordination\nFuture-proof: Enables embedding databases per product or shared aggregation\nSimpler mental model: Products are truly independent\nBackward compatible: Product column facilitates future migrations\n\nAlternatives Considered\nPlatform-level shared index\n\nPros: Unified search across all products\nCons: Complex coordination, shared bottleneck, potential data leakage\nRejected: Architecture does not support team autonomy\n\nHybrid approach (shared metadata + per-product data)\n\nPros: Balanced complexity\nCons: Still requires coordination layer, adds complexity\nRejected: Per-product isolation is simpler and cleaner\n\nFuture Work\nWhen cross-product features are needed:\n\nCreate aggregation layer on top of per-product indexes\nOr implement global embedding database that reads from per-product sources\nProduct column in schema already prepared for this extensibility\n\nReferences\n\nKABSD-FTR-0010_monorepo-platform-migration.md: Monorepo Platform Migration feature\nKABSD-TSK-0084_update-indexer-and-resolver-for-product-isolation.md: Indexer and resolver product isolation\n"},"adr/ADR-0032_per-product-isolated-index-architecture":{"title":"Per-Product Isolated Index Architecture","links":["items/feature/0000/KABSD-FTR-0010_monorepo-platform-migration","items/task/0000/KABSD-TSK-0084_update-indexer-and-resolver-for-product-isolation"],"tags":[],"content":"Context\nIn a multi-product monorepo environment, we needed to decide between two architectural approaches for the SQLite index:\n\nPlatform-level shared index: All products write to a single _kano/backlog/_index/backlog.sqlite3\nPer-product isolated index: Each product owns its own index at products/&lt;name&gt;/_index/backlog.sqlite3\n\nDecision Criteria\nWe evaluated 7 dimensions:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDimensionPer-ProductPlatformWinnerIsolationComplete separationShared namespacePer-ProductConcurrencyNo locking contentionPotential locksPer-ProductPerformancePredictable, isolatedCan degrade with loadPer-ProductScalabilityLinear per productShared bottleneckPer-ProductComplexitySimple, independentComplex coordinationPer-ProductMaintenanceEasy (per-product)Harder (shared state)Per-ProductFuture ExtensibilitySupports embedding DBDifficult to addPer-Product\nOverall Score: Per-Product = 91%, Platform = 43%\nDecision\nImplement per-product isolated SQLite indexes.\nEach product will:\n\nMaintain its own SQLite database at products/&lt;product-name&gt;/_index/backlog.sqlite3\nHave independent schema with product column for self-documentation\nSupport autonomous indexing/rebuilding without affecting other products\nEnable future cross-product aggregation via product-aware queries\n\nImplementation Details\nSchema Design\nAll tables include product column:\nCREATE TABLE items (\n  product TEXT NOT NULL,\n  id TEXT NOT NULL,\n  PRIMARY KEY (product, id),\n  ...\n);\n \nCREATE TABLE item_tags (\n  product TEXT NOT NULL,\n  item_id TEXT NOT NULL,\n  tag TEXT NOT NULL,\n  PRIMARY KEY (product, item_id, tag),\n  FOREIGN KEY (product, item_id) REFERENCES items(product, id)\n);\nIndex Rebuild Process\n\nscripts/indexing/build_sqlite_index.py --product &lt;name&gt;\nScans products/&lt;name&gt;/items/ directory\nExtracts product from file paths\nUpserts into product-specific SQLite database\nMaintains composite key integrity\n\nResolver Behavior\n\nresolve_ref(ref, index, product=None) accepts optional product filter\nIf product not specified, searches across all products\nProduct column enables cross-product queries when needed\n\nRationale\n\nIsolation ensures safety: No possibility of data leakage between products\nPredictable performance: Each product’s index grows independently\nSupports autonomy: Teams can rebuild their product’s index without coordination\nFuture-proof: Enables embedding databases per product or shared aggregation\nSimpler mental model: Products are truly independent\nBackward compatible: Product column facilitates future migrations\n\nAlternatives Considered\nPlatform-level shared index\n\nPros: Unified search across all products\nCons: Complex coordination, shared bottleneck, potential data leakage\nRejected: Architecture does not support team autonomy\n\nHybrid approach (shared metadata + per-product data)\n\nPros: Balanced complexity\nCons: Still requires coordination layer, adds complexity\nRejected: Per-product isolation is simpler and cleaner\n\nFuture Work\nWhen cross-product features are needed:\n\nCreate aggregation layer on top of per-product indexes\nOr implement global embedding database that reads from per-product sources\nProduct column in schema already prepared for this extensibility\n\nReferences\n\nKABSD-FTR-0010_monorepo-platform-migration.md: Monorepo Platform Migration feature\nKABSD-TSK-0084_update-indexer-and-resolver-for-product-isolation.md: Indexer and resolver product isolation\n"},"adr/ADR-0033_decision-for-kabsd-tsk-0056":{"title":"Decision for KABSD-TSK-0056","links":[],"tags":[],"content":"Status\nProposed\nContext\nThis decision was extracted from workset notes for task/feature KABSD-TSK-0056.\nDecision\nUse fine-grained chunking with semantic section boundaries instead of fixed-size windows. This preserves logical document structure for better retrieval accuracy.\nRationale\nSee workset notes in _kano\\backlog\\sandboxes.cache\\019b8f52-9fc8-7c94-aa2d-806cacdd9086 for full context.\nConsequences\n\nPositive:\n\n…\n\n\nNegative:\n\n…\n\n\nNeutral:\n\n…\n\n\n\nAlternatives Considered\n\nAlternative A: …\nAlternative B: …\n\nReferences\n\nExtracted from workset for KABSD-TSK-0056 by copilot\n"},"adr/ADR-0034_conflict-handling-policy-for-duplicate-ids-and-uids":{"title":"Conflict handling policy for duplicate IDs and UIDs","links":[],"tags":[],"content":"Decision\nAdopt a configurable conflict policy for duplicate IDs and UIDs, with defaults:\n\nconflict_policy.id_conflict = &quot;rename&quot;: when duplicate display IDs are detected, rename duplicates to the next available ID.\nconflict_policy.uid_conflict = &quot;trash_shorter&quot;: when the same UID appears with differing content, keep the longer file and move the shorter file to _trash/&lt;YYYYMMDD&gt;/....\n\nTie-breaker for equal length: keep the lexicographically earliest path and trash the other(s).\nContext\nWe need deterministic, low-friction behavior for duplicate IDs/UIDs across agents. Recent link integrity repairs showed how ambiguity slows down remediation and risks cross-agent divergence.\nOptions Considered\n\nAlways report and require human intervention.\nAuto-rename duplicate IDs and auto-trash UID conflicts.\nAuto-rename for both ID and UID conflicts.\n\nPros / Cons\n\nAuto-rename reduces manual cleanup for benign ID collisions.\nTrashing UID conflicts is safer than deletion and preserves a recovery path.\nAuto-remediation can hide mistakes if applied without review; defaults should be documented and configurable.\n\nConsequences\n\nadmin links normalize-ids will apply these defaults unless overridden in config.\nUID conflict resolution will create _trash/ entries; audits should treat them as recoverable artifacts.\n\nFollow-ups\n\nDocument the policy in the skill and config defaults.\n"},"adr/ADR-0035_cross-lingual-retrieval-requirement-and-default-embedding-policy":{"title":"Cross-lingual retrieval requirement and default embedding policy","links":[],"tags":[],"content":"Decision\nCross-lingual retrieval is a requirement.\nDefault embedding policy must be multilingual-capable. Provider/model choices must be evaluated against a small multilingual benchmark corpus that includes CJK and mixed-language queries.\nContext\nThis repo’s backlog contains mixed-language content (English + CJK). The semantic retrieval system is local-first and derived from canonical Markdown (see ADR-0009).\nIf we choose an embedder that is not multilingual-capable, cross-lingual queries will fail or regress unpredictably, and benchmark results will not reflect real usage.\nOptions Considered\nOption A: English-only retrieval\nTreat cross-lingual retrieval as out-of-scope. Only optimize for English content.\nOption B: Cross-lingual retrieval required (multilingual embedder policy) [chosen]\nRequire cross-lingual retrieval and evaluate embedders using multilingual cases.\nPros / Cons\nOption A: English-only retrieval\nPros:\n\nPotentially smaller/faster embedding models.\n\nCons:\n\nFails for mixed-language backlog content.\nForces users to translate queries manually.\nMakes retrieval quality brittle as content language mix evolves.\n\nOption B: Cross-lingual retrieval required\nPros:\n\nMatches repository reality (mixed-language artifacts).\nMakes evaluation criteria explicit and repeatable.\n\nCons:\n\nMay increase model footprint/cost.\nRequires benchmark coverage for multilingual/cross-lingual cases.\n\nConsequences\n\nBenchmarks MUST include cross-lingual cases.\n\n\nThe benchmark harness (USR-0034) must include multilingual docs and cross-lingual queries.\n\n\nTelemetry MUST capture tokenizer behavior and truncation.\n\n\nToken inflation for CJK can reduce effective context windows.\nTelemetry must distinguish exact vs heuristic token counts (see tokenizer TokenCount).\n\n\nDefault embedder configuration is allowed to be “noop” for local tests.\n\n\nReal embedders remain optional dependencies.\nDecisions about a real default model should be benchmark-driven.\n\nFollow-ups\n\nKeep ADR-0036 aligned: per-model indexes are required for safe experimentation and rollback.\nUse kano-backlog benchmark run outputs under _kano/backlog/products/&lt;product&gt;/artifacts/KABSD-TSK-0261/runs/ as the evidence trail for model selection.\n"},"adr/ADR-0036_index-strategy-shared-index-now-per-model-indexes-later-via-config":{"title":"Index strategy: shared index now, per-model indexes later via config","links":[],"tags":[],"content":"Decision\nWe will use per-model indexes (per embedding space) as the default.\n\nA single “shared index” across different embedding models is not supported because vectors\nfrom different models are not generally comparable (dimension and semantic space differ).\n\nWe will keep an explicit configuration mechanism to switch index selection and to allow\ncontrolled “aliasing” (sharing) only when two model identifiers are proven to be the same\nembedding space (strict allowlist; no automatic inference).\nContext\nWe need a local-first semantic retrieval capability for backlog artifacts (items, ADRs, etc.).\r\nCross-lingual retrieval is required (see ADR-0035), which increases the likelihood that we\r\nwill evaluate and potentially switch between embedding models over time.\nThe index must remain:\n\nlocal-first (no server runtime required)\nrebuildable (derived from canonical Markdown)\ndeterministic enough for incremental rebuilds (chunk IDs + content hashes)\nconfigurable (so we can swap providers/models/backends without rewriting code)\n\nOptions Considered\nOption A: Single shared index across multiple embedding models\nStore all vectors in one ANN index, regardless of which model produced them.\nOption B: Per-model indexes (per embedding space) [chosen]\nMaintain separate ANN indexes keyed by an explicit “embedding space” identity.\nOption C: Shared index with automatic compatibility detection (future research)\nAttempt to infer which models are “compatible enough” to share one index by running\r\nstatistical tests (e.g., neighborhood agreement on a validation corpus).\nPros / Cons\nOption A: Single shared index across multiple embedding models\nPros:\n\nMinimal operational complexity (one index file)\n\nCons:\n\nGenerally invalid: vectors from different embedding models are not comparable\r\n(dimensions can differ; even with same dimension the spaces are different).\nProduces misleading similarity scores and unstable retrieval quality.\nMakes troubleshooting and benchmarking ambiguous.\n\nOption B: Per-model indexes (per embedding space)\nPros:\n\nCorrectness: avoids mixing incompatible vector spaces.\nOperational clarity: retrieval quality changes map to a single model+version.\nEnables safe experimentation: add a new index for a new model without corrupting the old one.\nSupports gradual rollout: switch default index via config, keep rollback path.\n\nCons:\n\nMore disk usage and rebuild time when multiple models are evaluated.\nRequires a routing key (index selection) to be part of pipeline config and metadata.\n\nOption C: Shared index with automatic compatibility detection\nPros:\n\nCould reduce the number of indexes in some cases.\n\nCons:\n\nHigh risk of false positives (appears compatible on a small corpus but fails in practice).\nAdds complexity and “magic” behavior that is hard to reason about.\nNeeds an evaluation harness and careful governance anyway.\n\nConsequences\n1) Define a stable embedding space identity\nIntroduce an embedding_space_id used for:\n\nselecting the vector index (routing)\nstoring metadata alongside chunks/vectors\npreventing accidental mixing\n\nDefinition (conceptual):\n\nembedding_space_id = sha256(provider_id + model_name + model_revision + dims + preprocessing_version + vector_norm + prompt_style_id)\n\nNotes:\n\n“model_revision” should include an immutable identifier when possible (HF revision hash, provider version).\npreprocessing_version covers normalization/prefixing decisions that affect embeddings.\nprompt_style_id matters for instruct-style embedders (query vs document templates).\n\n2) Index storage layout\nPersist one index per embedding_space_id (or per “model key” that maps to it), for example:\n\n.../vector_indexes/&lt;embedding_space_id&gt;/index.bin\n.../vector_indexes/&lt;embedding_space_id&gt;/mapping.sqlite (if needed)\n\n3) Config-driven routing and future “sharing” via allowlist aliasing\nConfiguration must allow:\n\nselecting the default embedder (and therefore default index)\nselecting a specific index key explicitly (for debugging/rollback)\ndefining alias mappings only when two identifiers are truly the same embedding space\n\nPolicy:\n\nDo not auto-merge indexes.\nIf aliasing is used, it must be explicit and reviewed (allowlist).\n\n4) Benchmark implications\nBenchmarks must report results per embedding_space_id so comparisons are reproducible.\nFollow-ups\n\nUpdate KABSD-USR-0035 to reference ADR-0036 as the index strategy baseline.\nEnsure KABSD-USR-0031 (telemetry) includes embedding_space_id and model revision in results.\nEnsure KABSD-USR-0034 (benchmark harness) compares multiple embedding_space_id runs.\nOptional (future): research Option C as a non-default experiment, producing an ADR addendum\r\nif the evidence supports safe aliasing beyond strict equality.\n\nOptions Considered\nPros / Cons\nConsequences\nFollow-ups"},"adr/ADR-0037_inspector-pattern-and-query-surface-architecture":{"title":"Inspector Pattern and Query Surface Architecture","links":["KABSD-EPIC-0011","KABSD-FTR-0055","KABSD-FTR-0056","ADR-0013","ADR-0004"],"tags":[],"content":"Decision\nThe kano-agent-backlog-skill will adopt an “Inspector Pattern” architecture where:\n\nSkill Core = Query Surface: Provides deterministic, evidence-based data extraction APIs\nExternal Agents = Inspectors: Consume query surface, produce conclusions with evidence trails\nEvidence-First: Every conclusion must cite traceable sources (file paths, line ranges, IDs)\nRead-Only Contract: Inspectors query canonical SoT, never write to it directly\n\nThis means:\n\nAll “expert judgment” (health assessment, review, refactor suggestions) lives in external agents\nCore skill provides only deterministic data + derived artifacts (audit, snapshot, constellation, indexes)\nInspector agents are replaceable (any agent can implement the contract)\nAll inspector outputs must include evidence attachments (no unsourced claims)\n\nContext\nProblem Statement\nOrigin: GPT-5.2 feedback on backlog discipline and architecture (2026-01-22)\nCurrent risk: encoding “judgment” (health assessment, review suggestions, refactor recommendations) into core skill logic creates:\n\nGoodhart’s Law risk: Metrics become targets, lose meaning when hardcoded\nTight coupling: Expert logic hardcoded into skill core, hard to extend/replace\nLimited extensibility: Can’t swap assessment strategies without modifying core\nInconsistent evidence: Conclusions without traceable sources\n\nKey Insight from GPT-5.2\n\n“Backlog skill’s responsibility: provide all tools for agents to reliably acquire information, not hardcode judgment into core.”\n\nAnalogy:\n\nSkill = Database with query API\nInspector Agents = Analytics tools that consume the database\nEvidence = Structured citations (table/row/column refs)\n\nCurrent State\nWhat exists:\n\nAudit primitives (rule checking, gap detection)\nSnapshot generation (state summaries)\nConstellation (relationship graphs)\nSQLite index for fast queries\n\nWhat’s missing:\n\nFormal Inspector Pattern contract\nUnified query surface with JSON output\nEvidence attachment standards\nReference inspector implementation\n\nRelated Work\n\n3+3 questions (health/ideas assessment): Proposed feature, now reframed as inspector agent\nDecision audit/write-back: Implemented ad-hoc, needs formalization\nExternal agent integration: No contract defined\n\nArchitecture\nPrinciple A: Core = Data + Derived Artifacts (Deterministic)\nCore Responsibilities (all deterministic, repeatable):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComponentPurposeOutputaudit.run()Rules enforcement (Ready gate, schema validation)Findings with IDs, categories, severitysnapshot.build()Current state extraction (item counts, distributions)Timestamped state summaryconstellation.build()Relationship graph (parent/child, blocks, relates)Graph with nodes, edges, metadataindexSearch acceleration (FTS, embedding, graph)Query results with relevance scores\nKey: No “expert opinion” - only facts derived from canonical files.\nPrinciple B: External Agents = Inspectors (Replaceable)\nInspector Types (all external to core):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInspectorPurposeExample OutputHealth/Ideas3+3 questions, gap analysis, anti-patterns”5 items missing Context field (AF-001, AF-002…)”ReviewerCode review suggestions, best practices”Consider extracting common logic (evidence: L45-67)“ArchitectRefactoring recommendations, design improvements”Detected circular dependency (items: X, Y, Z)“SecurityThreat model, vulnerability assessment”Exposed secrets in item TASK-042 (file: …, L25)”\nKey: These are separate processes/agents, not core modules.\nPrinciple C: Evidence = First-Class Citizen\nCore Principle: Every inspector finding must cite traceable evidence. Without evidence, conclusions are rejected or downgraded.\nEvidence Quality: Five Axes (Critical Thinking Foundation)\nBased on critical thinking principles, evidence quality must be evaluated across five axes:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAxisDefinitionSystem ApplicationRelevanceEvidence must directly support the claim, not just appear relatedclaim_id links evidence to specific findingReliabilitySource is real, traceable, verifiable (not “jargon credentialism”)provenance field: hash, commit, URL, timestamp, authorSufficiencyOne example ≠ universal rule (avoid survivor bias, no control group)coverage field: sample range, counter-examples, failure statsVerifiabilityOthers can check, repeat, measure the claimverification field: reproducible command, test, or check stepsIndependenceEvidence doesn’t collude with conclusion (avoid self-citation, astroturfing)independence field: source type, conflict-of-interest flags\nWhy This Matters:\n\nBayes’ prior problem: Math looks objective, but where priors come from is the landmine; priors can be manipulated → conclusions can be steered\nPeer review ideal vs reality: The idea is to shift credibility from individuals to community validation, but institutions develop interest chains, conservatism within paradigms\nTrust should be in continuous questioning + verifiable judgment, not fixed standards\n\nEvidence Schema (Extended with 5-Axes)\n@dataclass\nclass EvidenceRecord:\n    &quot;&quot;&quot;Full evidence record with quality metadata.&quot;&quot;&quot;\n    \n    # Core identity (existing)\n    type: str                 # &quot;item&quot;, &quot;adr&quot;, &quot;file&quot;, &quot;audit_finding&quot;, &quot;commit&quot;, &quot;log&quot;\n    id: str                   # Item/ADR ID or finding ID\n    file_path: str            # Relative path from backlog root\n    line_range: Optional[Tuple[int, int]] = None\n    excerpt: Optional[str] = None\n    timestamp: Optional[str] = None\n    \n    # Relevance (5-axis 1)\n    claim_id: Optional[str] = None       # Which claim/decision this supports\n    support_type: Optional[str] = None   # &quot;direct&quot;, &quot;indirect&quot;, &quot;counterpoint&quot;\n    \n    # Reliability (5-axis 2)\n    source_type: str = &quot;unknown&quot;         # &quot;repo_path&quot;, &quot;issue&quot;, &quot;pr&quot;, &quot;chat&quot;, &quot;doc&quot;, &quot;web&quot;, &quot;experiment&quot;\n    provenance: Optional[Dict] = None    # {&quot;hash&quot;: &quot;...&quot;, &quot;commit&quot;: &quot;...&quot;, &quot;url&quot;: &quot;...&quot;, &quot;author&quot;: &quot;...&quot;}\n    \n    # Sufficiency (5-axis 3)\n    coverage: Optional[Dict] = None      # {&quot;sample_size&quot;: N, &quot;has_counterexamples&quot;: bool, &quot;has_failure_stats&quot;: bool}\n    \n    # Verifiability (5-axis 4)\n    verification: Optional[Dict] = None  # {&quot;method&quot;: &quot;command|test|manual&quot;, &quot;steps&quot;: [...], &quot;reproducible&quot;: bool}\n    \n    # Independence (5-axis 5)\n    independence: Optional[Dict] = None  # {&quot;self_cited&quot;: bool, &quot;conflict_of_interest&quot;: bool, &quot;same_source_chain&quot;: bool}\n    \n    # Meta\n    confidence: Optional[float] = None   # 0.0-1.0, with explicit reasoning\n    confidence_reason: Optional[str] = None\nMinimum Evidence Requirements by Context\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContextRequired FieldsRecommended FieldsWorkset materialtype, id, file_path, source_typeprovenance, verificationInspector findingtype, id, file_path, line_range, claim_idcoverage, independenceADR decision supporttype, id, file_path, claim_id, verificationcoverage, independence, confidenceHealth reviewtype, id, source_type, independencecoverage, sufficiency analysis\nAnti-Patterns (Evidence Red Flags)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRed FlagDetectionRiskSingle source dependencyAll evidence from one file/authorLow reliability, no cross-validationSame-source masqueradeMultiple “evidence” items from same originFake sufficiencyMissing counterexamplesNo failure cases, only success storiesSurvivor biasNon-reproducible”Trust me” without verification stepsUnverifiable claimsSelf-citation loopAuthor cites own previous work exclusivelyIndependence violationJargon credentialismClaims backed by terminology, not dataReliability theater\nEvery inspector output MUST include evidence records:\n{\n  &quot;finding_id&quot;: &quot;F-001&quot;,\n  &quot;category&quot;: &quot;health&quot;,\n  &quot;assessment&quot;: &quot;Item missing required field&quot;,\n  &quot;evidence&quot;: [\n    {\n      &quot;type&quot;: &quot;item&quot;,\n      &quot;item_id&quot;: &quot;KABSD-TSK-0042&quot;,\n      &quot;file&quot;: &quot;_kano/backlog/items/task/0000/KABSD-TSK-0042.md&quot;,\n      &quot;line_range&quot;: [25, 30],\n      &quot;field&quot;: &quot;Context&quot;,\n      &quot;issue&quot;: &quot;Empty or missing&quot;\n    }\n  ],\n  &quot;timestamp&quot;: &quot;2026-01-22T08:51:00Z&quot;,\n  &quot;agent&quot;: &quot;health-inspector-v1&quot;\n}\nNo evidence = rejected or downgraded.\nInspector Agent Contract\nQuery Surface API (Existing + Planned)\nExisting APIs (already implemented):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOps FunctionCLI CommandJSON SupportNotessnapshot.generate_pack()kano-backlog snapshot✅ pack.to_json()Returns EvidencePack with stubs, capabilities, healthworkitem.list_items()kano-backlog item list✅ --format jsonFilters: type, state, parent, tagsworkitem.get_item()kano-backlog workitem read✅ --format jsonSingle item with full metadataworkitem.validate_ready()kano-backlog workitem validate✅ --format jsonReady gate validationtopic.decision_audit()kano-backlog topic decision-audit✅ --format jsonDecision write-back audittopic.export_context()kano-backlog topic export-context✅ --format jsonTopic context bundle\nPlanned APIs (to implement in KABSD-FTR-0055):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOps FunctionCLI CommandStatusNotesrelease_check.run_phase1/2()kano-backlog release check⚠️ Markdown onlyNeed --format jsonVarious health checkskano-backlog doctor⚠️ Plain textNeed --format jsonconstellation.build()kano-backlog constellation build❌ MissingRelationship graphdoc_resolve.resolve()kano-backlog workitem resolve❌ MissingStructured excerpts\nEvidence Schema (to standardize across all APIs):\nSee “Principle C: Evidence Quality Five Axes” above for the extended EvidenceRecord schema with 5-axis quality metadata.\nMinimal Evidence schema (for backward compatibility):\n@dataclass\nclass Evidence:\n    &quot;&quot;&quot;Minimal traceable source for inspector findings.&quot;&quot;&quot;\n    type: str           # &quot;item&quot;, &quot;adr&quot;, &quot;file&quot;, &quot;audit_finding&quot;\n    id: str             # Item/ADR ID or finding ID\n    file_path: str      # Relative path from backlog root\n    line_range: Optional[Tuple[int, int]] = None  # Start, end lines\n    excerpt: Optional[str] = None  # Text snippet\n    timestamp: Optional[str] = None\nAudit vs Health: Distinction\nBoth use the same query surface and evidence format, but differ in what they check:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAspectAuditHealth ReviewFocusConsistency / ConformanceCredibility / Risk / GapsQuestions”Does X conform to rule Y?&quot;&quot;Can we trust X? What are the risks?”ExamplesNaming conventions, directory structure, schema drift, deterministic pipeline completenessSingle-source dependency, survivor bias, unverifiable claims, conflicted evidenceOutputViolation list (pass/fail per rule)Risk and trust gap report (with severity)TriggerCI gate, pre-release check, schema migrationManual request, agent stuck, decision review\nAudit checks for “rule violations”:\n\nMissing required fields\nWrong directory structure\nSchema mismatch\nDeterministic pipeline gaps\n\nHealth checks for “trust gaps”:\n\nEvidence quality degradation (5-axis failures)\nSingle-narrative dependency\nMissing counterexamples\nUnstated assumptions (priors)\n\nInspector Output Contract\nStandard output schema:\n{\n  &quot;inspector&quot;: &quot;health-ideas-v1&quot;,\n  &quot;agent&quot;: &quot;antigravity&quot;,\n  &quot;timestamp&quot;: &quot;2026-01-22T08:51:00Z&quot;,\n  &quot;query_params&quot;: {\n    &quot;window&quot;: &quot;7d&quot;,\n    &quot;filters&quot;: {}\n  },\n  &quot;findings&quot;: [\n    {\n      &quot;finding_id&quot;: &quot;F-001&quot;,\n      &quot;category&quot;: &quot;health&quot;,\n      &quot;question&quot;: &quot;Are items well-formed?&quot;,\n      &quot;assessment&quot;: &quot;5 tasks missing Context field&quot;,\n      &quot;severity&quot;: &quot;warning&quot;,\n      &quot;evidence&quot;: [\n        {\n          &quot;type&quot;: &quot;audit_finding&quot;,\n          &quot;audit_finding_id&quot;: &quot;AF-001&quot;,\n          &quot;item_id&quot;: &quot;KABSD-TSK-0042&quot;,\n          &quot;file&quot;: &quot;_kano/backlog/items/task/0000/KABSD-TSK-0042.md&quot;,\n          &quot;line_range&quot;: [25, 30],\n          &quot;field&quot;: &quot;Context&quot;,\n          &quot;issue&quot;: &quot;Empty&quot;\n        }\n      ]\n    }\n  ],\n  &quot;summary&quot;: {\n    &quot;total_findings&quot;: 12,\n    &quot;by_severity&quot;: {&quot;error&quot;: 0, &quot;warning&quot;: 5, &quot;info&quot;: 7}\n  }\n}\nRequired fields:\n\ninspector: Inspector identity (name + version)\nagent: Which agent ran the inspector\ntimestamp: When inspection occurred\nfindings: Array of findings, each with evidence\nevidence: Array of traceable sources with file paths + line ranges\n\nIntegration Patterns\nPattern 1: Manual Invocation\n# Human asks agent to run inspector\nUser: &quot;Check backlog health and show me the report&quot;\n \n# Agent executes\n$ kano-backlog query snapshot --format json &gt; snapshot.json\n$ health-inspector --input snapshot.json --output report.json\n$ cat report.json\nPattern 2: CI Integration\n# .github/workflows/backlog-health.yml\n- name: Check backlog health\n  run: |\n    kano-backlog query snapshot --format json &gt; snapshot.json\n    health-inspector --input snapshot.json --output report.json\n    if grep -q &#039;&quot;severity&quot;: &quot;error&quot;&#039; report.json; then exit 1; fi\nPattern 3: Agent Self-Assessment\nAgent: &quot;I&#039;m stuck. Let me consult the inspector...&quot;\r\nAgent: &lt;runs inspector, gets findings&gt;\r\nAgent: &quot;Inspector found 3 items blocking my work (evidence: ...)&quot;\n\nWhere Inspectors Live\nOptions:\n\n\nSeparate skill: kano-inspector-health-ideas-skill/ (recommended)\n\nPro: Replaceable, versionable, independent lifecycle\nCon: Extra installation step\n\n\n\nReference implementation in core skill: examples/inspectors/health.py\n\nPro: Batteries-included for demos\nCon: Risk of coupling if not disciplined\n\n\n\nExternal repository: Community-maintained inspectors\n\nPro: Maximum flexibility\nCon: Discovery problem\n\n\n\nRecommendation: Start with (2) for reference, encourage (1) for production use.\nConsequences\nFor Skill Developers\n\n\nMust separate data from judgment:\n\n✅ Good: audit.run() returns list of “missing Context field” findings\n❌ Bad: audit.run() returns “backlog health is poor” conclusion\n\n\n\nMust provide evidence attachments:\n\nAll query APIs return structured data with file paths, line ranges, IDs\nNo APIs that return “summary strings” without evidence\n\n\n\nMust document query surface:\n\nInspector contract is a public API, needs versioning and docs\n\n\n\nFor Inspector Agents\n\n\nMust consume query surface, not parse files directly:\n\n✅ Good: Call workitem.query(filters={...}) API\n❌ Bad: Parse _kano/backlog/items/**/*.md directly\n\n\n\nMust attach evidence to all findings:\n\nEvery conclusion cites file path + line range (or stable anchor)\nNo “I think X” without “because I saw Y at Z”\n\n\n\nAre replaceable:\n\nAny agent can implement inspector contract\nMultiple inspectors can coexist (health, review, security, etc)\n\n\n\nFor End Users (Humans)\n\n\nInspector frequency is use-case specific:\n\nNOT “run daily” (avoid Goodhart’s Law)\nRun when: manual request, CI gate, agent stuck, pre-release audit\n\n\n\nInspector outputs are recommendations, not commands:\n\nHuman decides which findings to act on\nAgent may propose fixes but doesn’t auto-apply\n\n\n\nEvidence trail enables trust:\n\nEvery finding cites sources\nHuman can verify inspector claims\n\n\n\nMigration Strategy\nPhase 1: Define Contract (ADR + Documentation)\n\n Create this ADR\n Document query surface API spec\n Document inspector output schema\n Update ADR-0013 (Module Boundaries) with inspector pattern section\n\nPhase 2: Implement Query Surface (KABSD-FTR-0055)\n\n Add JSON output to existing audit/snapshot/constellation\n Implement workitem.query API\n Implement doc.resolve API\n Implement export.bundle API\n Add CLI commands under kano-backlog query\n\nPhase 3: Reference Inspector (KABSD-FTR-0056)\n\n Build health/ideas inspector as reference implementation\n Validate inspector contract through real usage\n Document integration patterns\n\nPhase 4: Documentation &amp; Tooling\n\n Update AGENTS.md with inspector pattern guidance\n Add inspector examples to SKILL.md\n Create inspector scaffolding tool (optional)\n\nAlternatives Considered\nAlternative A: Keep Assessment Logic in Core\nApproach: Encode health checks, review logic, etc directly in skill core.\nRejected because:\n\nCreates tight coupling (hard to extend/replace)\nGoodhart’s Law risk (metrics become targets when hardcoded)\nNo separation of concerns (data vs judgment)\n\nAlternative B: Build Generic “LLM Judge” Framework\nApproach: Create abstract framework for LLM-based assessment.\nDeferred because:\n\nToo abstract for initial implementation\nStart concrete (inspector contract), generalize later if needed\nRisk of over-engineering\n\nAlternative C: Use External Tool (Jira, Linear, etc)\nApproach: Export to external PM tool, use their analytics/dashboards.\nRejected because:\n\nViolates local-first principle\nExternal tools can consume inspector outputs via adapters (future work)\nWe control the query surface, not the external tool\n\nRelated\n\nKABSD-EPIC-0011: Inspector Pattern: External Agent Query Surface (parent epic)\nKABSD-FTR-0055: Query Surface API Implementation\nKABSD-FTR-0056: Inspector Agent Reference Implementation\nADR-0013: Codebase Architecture and Module Boundaries (will add inspector pattern section)\nADR-0004: File-First Architecture with SQLite Index (complements this ADR)\n\nStatus\nAccepted (2026-01-22)\nThis ADR establishes the architectural direction. Implementation will occur in phases via linked feature work items."},"adr/README":{"title":"README","links":[],"tags":[],"content":"_kano/backlog/decisions\nStore ADRs here. Link ADR IDs in work items via the decisions field and\r\nappend a Worklog entry when a decision affects scope or approach."},"adr/index":{"title":"index","links":[],"tags":[],"content":"_kano/backlog/decisions\nStore ADRs here. Link ADR IDs in work items via the decisions field and\r\nappend a Worklog entry when a decision affects scope or approach."},"cli/index":{"title":"CLI Reference","links":[],"tags":[],"content":"CLI Reference\nkano-backlog\n                                                                               \r\n Usage: kano-backlog [OPTIONS] COMMAND [ARGS]...                               \r\n                                                                               \r\n kano-backlog: Backlog management CLI (MVP)                                    \r\n                                                                               \r\n+- Options -------------------------------------------------------------------+\r\n| --install-completion          Install completion for the current shell.     |\r\n| --show-completion             Show completion for the current shell, to     |\r\n|                               copy it or customize the installation.        |\r\n| --help                        Show this message and exit.                   |\r\n+-----------------------------------------------------------------------------+\r\n+- Commands ------------------------------------------------------------------+\r\n| doctor      Check environment health.                                       |\r\n| admin       Administrative and setup commands                               |\r\n| workitem    Work item operations                                            |\r\n| item        Work item operations (alias)                                    |\r\n| state       State transitions                                               |\r\n| worklog     Worklog operations                                              |\r\n| view        View and dashboard operations                                   |\r\n| snapshot    Snapshot and evidence operations                                |\r\n| workset     Workset cache operations                                        |\r\n| topic       Topic context operations                                        |\r\n| config      Config inspection and validation                                |\r\n| changelog   Changelog generation from backlog                               |\r\n| benchmark   Deterministic benchmark harness                                 |\r\n| embedding   Embedding pipeline operations                                   |\r\n| search      Vector similarity search                                        |\r\n| tokenizer   Tokenizer adapter configuration, testing, and diagnostics       |\r\n+-----------------------------------------------------------------------------+\r\n\n"},"demo/agents":{"title":"agents","links":["ADR-0037"],"tags":[],"content":"AGENTS\nExternal File Loading\nCRITICAL: When you encounter a file reference (e.g., @rules/general.md, @docs/architecture.md), use your Read tool to load it on a need-to-know basis. These references are relevant to the SPECIFIC task at hand.\nInstructions\n\nDo NOT preemptively load all references - use lazy loading based on actual need\nWhen loaded, treat content as mandatory instructions that override defaults\nFollow references recursively when the loaded file contains additional @ references\nPriority: External file content &gt; AGENTS.md defaults &gt; built-in instructions\n\nExample Usage\nUser mentions: &quot;Follow @rules/authentication.md for this task&quot;\r\nAgent: Reads @rules/authentication.md, applies its rules for this task only\n\n\nRepo purpose\nThis repo is a demo showing how to use kano-agent-backlog-skill to turn agent collaboration\r\ninto a durable, local-first backlog with an auditable decision trail (instead of losing context in chat).\nConversational-first documentation (human-agent collaboration)\nThis project’s primary value is human + AI collaboration, not just a CLI.\r\nTherefore, when writing or updating documentation (README, docs, SKILL.md, process notes), always include\r\ninstructions for how to drive the workflow through a conversation with an AI agent, not only how to run commands.\nRules:\n\nEvery workflow doc should contain both:\n\nCLI commands (for deterministic, auditable execution), and\nSuggested chat prompts (copy/paste) that a human can say to an agent.\n\n\nPrompts must be specific about inputs the agent needs: topic/item IDs, product, agent identity, expected outputs.\nDocument the expected artifacts and paths the agent will produce/update (e.g., reports under topic/publish/).\nPrefer a consistent pattern in docs:\n\n“Say this to your agent”\n“The agent will do” (explicit steps)\n“Expected output” (files/paths + how to verify)\n\n\n\nExample (decision audit + decision write-back):\n\nSay to agent: “Run a decision write-back audit for topic  and show me which work items are missing decisions.”\nAgent runs: kano topic decision-audit &lt;topic-name&gt; --format plain\nExpected output: _kano/backlog/topics/&lt;topic-name&gt;/publish/decision-audit.md\nSay to agent: “Write back this decision to &lt;ITEM_ID&gt; and include the synthesis file as source.”\nAgent runs: kano workitem add-decision &lt;ITEM_ID&gt; --decision &quot;...&quot; --source &quot;...&quot; --agent &lt;agent-id&gt; --product &lt;product&gt;\nExpected output: updated work item with a ## Decisions section + appended Worklog entry.\n\nAgent roster (from README.md)\n\nCodex\nGitHub Copilot\nGoogle Antigravity\nAmazon Q\nAmazon Kiro\nCursor\nWindsurf\nOpenCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgentPrimary Model(s)Alternative ModelsNotesCodexCodex-Max (Nov 2025)GPT-5.2-Codex, o1/o3-reasoningOpenAI’s dedicated line for software engineering and multi-file logicGitHub CopilotGPT-5.2 (Dec 2025)Claude 4.5, Gemini 3 Pro, GPT-5.1Multi-model picker; GPT-5.2 Pro available for advanced research/reasoningGoogle AntigravityGemini 3 Flash (Dec 2025)Gemini 3 Pro, Gemini 1.5 Pro (legacy)Optimized for low-latency planning and multimodal workspace reasoningAmazon QClaude 4 Sonnet (May 2025)Claude 4.5 Sonnet, Amazon NovaEnhanced for autonomous computer use and deep codebase integrationAmazon KiroAuto-Routing (Dynamic)Claude 4.5 (Opus/Sonnet/Haiku)Intelligent routing across Claude 4 family; ~23% cheaper than direct Opus 4.5 useCursorClaude 3.5 Sonnet, GPT-5cursor-small (V2), GPT-4.1 MiniStable support for 2024/2025 frontier models; high-freq codebase indexingWindsurfSWE-1.5 (Proprietary)Claude 4, GPT-5.1, BYOKSWE-1.5 is a frontier coding model with performance near Claude 4.5OpenCodeMulti-ProviderDeepSeek V3.2, Llama 4 Scout, Mistral Large 3Open-source champion; supports 75+ providers including regional models\nDevelopment Guidelines\nDerived Data and .gitignore Rules\nCRITICAL: Always exclude derived/generated data from version control to keep repositories clean and efficient.\nWhat to Exclude\nGenerated/Derived Data (always add to .gitignore):\n\nCache directories: .cache/, __pycache__/, .pytest_cache/\nBuild artifacts: dist/, build/, *.egg-info/\nTest outputs: htmlcov/, .coverage, .hypothesis/\nVector databases: .cache/vector/, *.sqlite3 (embedding indexes)\nCompiled files: *.pyc, *.pyo, *.so\nIDE files: .vscode/, .idea/, *.swp\nOS files: .DS_Store, Thumbs.db\nEnvironment files: .env, .venv/, venv/\n\nWhat to Include\nSource of Truth (always version control):\n\nSource code: src/, tests/, scripts\nConfiguration templates: config.toml.example, default configs\nDocumentation: README.md, references/, SKILL.md\nSchema definitions: data models, API specs\nTest fixtures: static test data, benchmark corpus\n\nImplementation Rules\n\n\nBefore adding any new feature that generates data:\n\nIdentify what files/directories will be created\nAdd appropriate .gitignore entries immediately\nDocument the regeneration process\n\n\n\nCommon patterns to exclude:\n# Caches and derived data\n.cache/\n__pycache__/\n*.pyc\n \n# Build outputs\ndist/\nbuild/\n*.egg-info/\n \n# Test artifacts\n.pytest_cache/\n.coverage\nhtmlcov/\n \n# Vector/embedding indexes\n.cache/vector/\n*.sqlite3\n \n# IDE and OS\n.vscode/\n.DS_Store\n\n\nDocumentation requirement:\n\nAlways document how to regenerate excluded data\nInclude regeneration steps in README or setup docs\nProvide example commands for rebuilding indexes/caches\n\n\n\nRationale\n\nRepository size: Derived data can be large and grows over time\nMerge conflicts: Generated files often cause unnecessary conflicts\nEnvironment differences: Derived data may be platform/environment specific\nReproducibility: Source code should be sufficient to recreate all derived data\nSecurity: Avoid accidentally committing sensitive generated data\n\nQuick Start Commands\n# Install dependencies (dev mode)\npython -m pip install -e skills/kano-agent-backlog-skill[dev]\n \n# Run a specific test\npython -m pytest tests/test_chunking_mvp.py -v\n \n# Run all tests\npython -m pytest tests/ -v\n \n# Run tests with coverage\npython -m pytest tests/ --cov=skills/kano-agent-backlog-skill/src --cov-report=html\n \n# Format code\nblack skills/kano-agent-backlog-skill/src tests/\n \n# Sort imports\nisort skills/kano-agent-backlog-skill/src tests/\n \n# Check types\nmypy skills/kano-agent-backlog-skill/src\n \n# Run all linting\nblack skills/kano-agent-backlog-skill/src tests/ &amp;&amp; \\\nisort skills/kano-agent-backlog-skill/src tests/ &amp;&amp; \\\nmypy skills/kano-agent-backlog-skill/src\nCode Style Guidelines\nType Hints\nAlways use type hints from typing module: List, Dict, Optional, Any, Union, Tuple.\nExample:\nfrom typing import List, Optional, Dict\n \ndef process_items(items: List[str]) -&gt; Dict[str, Any]:\n    &quot;&quot;&quot;Process items and return a dictionary.&quot;&quot;&quot;\n    result: Dict[str, Any] = {}\n    for item in items:\n        result[item] = len(item)\n    return result\nImport Conventions\n\nOrder: standard library → third-party → local modules\nUse absolute imports from skill packages: from kano_backlog_core import ...\nUse relative imports within packages: from .models import ...\n\nExample:\nimport json  # Standard library\nfrom pathlib import Path  # Standard library\nfrom frontmatter import load  # Third-party\nfrom kano_backlog_core import BacklogItem  # Absolute from skill root\nfrom .models import ItemState  # Relative within package\nNaming Conventions\n\nClasses: PascalCase (e.g., CanonicalStore, ChunkingOptions)\nFunctions: snake_case (e.g., read_item, validate_config)\nConstants: UPPER_SNAKE_CASE (e.g., TYPE_DIRNAMES)\nPrivate members: leading underscore _private_var, __dunder__\nModules: snake_case (e.g., kano_backlog_core, token_counter)\n\nFormatting Rules\n\nLine length: 88 characters (Black default)\nIndentation: 4 spaces\nNo trailing whitespace\nDocstrings: triple double quotes &quot;&quot;&quot;, Google style\nType hints: after function definition, before docstring\n\nExample:\ndef process_data(data: List[Dict[str, Any]]) -&gt; Dict[str, Any]:\n    &quot;&quot;&quot;Process data and return results.\n \n    Args:\n        data: Input data to process.\n \n    Returns:\n        Dictionary of processed results.\n    &quot;&quot;&quot;\n    result: {}\n    for item in data:\n        result[item[&quot;id&quot;]] = item[&quot;value&quot;]\n    return result\nError Handling\nUse exceptions from kano_backlog_core/errors.py:\n\nItemNotFoundError - Item file doesn’t exist\nParseError - Invalid frontmatter or markdown\nValidationError - Data validation failed\nConfigError - Configuration error\nWriteError - Write operation failed\n\nExample:\nfrom kano_backlog_core.errors import ItemNotFoundError\n \ndef load_item(item_path: Path) -&gt; BacklogItem:\n    if not item_path.exists():\n        raise ItemNotFoundError(item_path, f&quot;Item file not found: {item_path}&quot;)\nDocstring Conventions\n\nTriple double quotes &quot;&quot;&quot;\nGoogle Style Guide format\nInclude Args:, Returns:, and Raises: sections\nImperative mood: “Return”, not “Returns”\n\nExample:\ndef create_item(title: str) -&gt; BacklogItem:\n    &quot;&quot;&quot;Create a new backlog item.\n \n    Args:\n        title: The title of the item.\n \n    Returns:\n        The created BacklogItem object.\n    &quot;&quot;&quot;\n    return BacklogItem(title=title)\nTesting Guidelines\nTest Structure\n\nUse pytest as test runner\nPlace tests in tests/ directory\nTest filename: test_{module_name}.py\nUse Hypothesis for property-based testing\n\nProperty-Based Testing with Hypothesis\nfrom hypothesis import given, strategies as st, composite\nfrom kano_backlog_core import ChunkingOptions\n \n@composite\ndef valid_config_strategy(draw):\n    &quot;&quot;&quot;Generate valid config instances.&quot;&quot;&quot;\n    return ChunkingOptions(\n        target_tokens=draw(st.integers(min_value=50, max_value=2048)),\n        max_tokens=draw(st.integers(min_value=512, max_value=4096))\n    )\n \n@given(valid_config_strategy())\ndef test_valid_config_passes(config: ChunkingOptions):\n    &quot;&quot;&quot;Valid config should pass validation.&quot;&quot;&quot;\n    assert config.target_tokens &lt;= config.max_tokens\nUsing the Kano Backlog Skill\nBefore Making Changes\n\nCheck existing items:\n\npython -m kano_backlog_cli.main item list --product kano-agent-backlog-skill\n\nCreate or update work items before coding:\n\npython -m kano_backlog_cli.main item create \\\n    --type task \\\n    --title &quot;Implement X feature&quot; \\\n    --product kano-agent-backlog-skill \\\n    --agent &lt;your-agent-id&gt;\n\nEnforce the Ready gate on Task/Bug items:\n\n\nRequired fields: Context, Goal, Approach, Acceptance Criteria, Risks / Dependencies\nAll fields must be non-empty and written in English only\n\n\nUpdate item state when starting work:\n\npython -m kano_backlog_cli.main item update-state \\\n    --id KABSD-TSK-0146 \\\n    --state InProgress \\\n    --agent &lt;your-agent-id&gt;\nWorklog Discipline\nWorklog is append-only. Append when:\n\nA load-bearing decision is made\nAn item state changes\nScope/approach changes\nAn ADR is created/linked\n\nFormat:\nYYYY-MM-DD HH:MM [agent=&lt;agent-id&gt;] [model=&lt;model&gt;] description\n\nAlways provide explicit --agent &lt;id&gt; - never use placeholders like auto or &lt;AGENT_NAME&gt;.\nState Transitions\n# Move to InProgress\npython -m kano_backlog_cli.main item update-state --id &lt;ID&gt; --state InProgress --agent &lt;agent-id&gt;\n \n# Move to Done\npython -m kano_backlog_cli.main item update-state --id &lt;ID&gt; --state Done --agent &lt;agent-id&gt;\nADR Creation\npython -m kano_backlog_cli.main adr create \\\n    --title &quot;Decision title&quot; \\\n    --product kano-agent-backlog-skill \\\n    --agent &lt;agent-id&gt;\nAgent-Specific Rules\nGitHub Copilot\nFollow commit guidelines in .github/copilot-instructions.md:\n\nUse Kano backlog IDs directly in commit messages\nPreferred format: KABSD-TSK-0146: &lt;short summary&gt;\nMultiple items: KABSD-TSK-0146 KABSD-TSK-0147: &lt;short summary&gt;\nDo NOT use jira# prefix\n\nBacklog system note:\n\nThis repository uses kano-backlog as the system of record, not Jira.\nDo not add any jira# or JIRA: prefixes. Reference Kano IDs directly.\nExamples:\n\nGood: KABSD-TSK-0261: refine filename truncation\nBad: jira#KABSD-TSK-0261\n\n\n\nAgent Identity\nValid agent IDs for worklog entries:\n\ncopilot, codex, claude, goose, antigravity, cursor, windsurf, opencode, kiro, amazon-q\n\nForbidden: &lt;AGENT_NAME&gt;, $AGENT_NAME\nArchitecture Rules (ADR-0013)\nModule Boundaries\n\nkano_backlog_core/ - Import-only, no executable code\nkano_backlog_ops/ - Use cases and business logic\nkano_backlog_cli/ - Executable CLI commands\nscripts/ - Entry point scripts\n\nCross-Package Imports\nUse absolute imports from kano_backlog_core when importing from other packages:\nfrom kano_backlog_core import BacklogItem, ItemState\nNever import directly from kano_backlog_ops from CLI or scripts.\nInspector Pattern (ADR-0037)\nPrinciple: Skill core provides query surface (deterministic data extraction). All “expert judgment” lives in external inspector agents.\nWhat This Means:\n\nCore provides: audit, snapshot, constellation, workitem.query, doc.resolve APIs (all read-only, deterministic)\nInspector agents consume: Query APIs to produce health reports, review suggestions, refactor recommendations\nEvidence required: Every inspector finding must cite file path + line range + item/ADR ID\n\nPattern:\n# Inspector agent calls query surface\nkano-backlog query snapshot --format json &gt; snapshot.json\nhealth-inspector --input snapshot.json --output report.json\n \n# Report includes evidence\n{\n  &quot;finding_id&quot;: &quot;F-001&quot;,\n  &quot;assessment&quot;: &quot;5 tasks missing Context field&quot;,\n  &quot;evidence&quot;: [\n    {\n      &quot;item_id&quot;: &quot;KABSD-TSK-0042&quot;,\n      &quot;file&quot;: &quot;_kano/backlog/items/task/0000/KABSD-TSK-0042.md&quot;,\n      &quot;line_range&quot;: [25, 30]\n    }\n  ]\n}\nInspector Types (all external to core):\n\nHealth/Ideas: 3+3 questions, gap analysis, anti-patterns\nReviewer: Code review suggestions, best practices\nArchitect: Refactoring recommendations, design improvements\nSecurity: Threat model, vulnerability assessment\n\nKey: Inspectors are replaceable. Any agent can implement the contract. Core never hardcodes “this backlog is healthy/unhealthy” conclusions.\nSee ADR-0037 for full architecture.\nCommon Data Structures\n\nBacklogItem - Core work item model with frontmatter\nItemType - Enum: EPIC, FEATURE, USER_STORY, TASK, BUG\nItemState - Enum: Proposed, Planned, Ready, InProgress, Blocked, Done, Dropped\nChunkingOptions, TokenBudgetPolicy - Configuration for chunking system\n\nCommon Error Types\n\nItemNotFoundError - Item file doesn’t exist\nParseError - Invalid frontmatter or markdown\nValidationError - Data validation failed\nConfigError - Configuration error\nWriteError - Write operation failed\n\n\nRemember: This is a living document. Update it as patterns evolve in the codebase.\nCanonical + Adapters Architecture\n\n\n                  \n                  IMPORTANT\n                  \n                \n\nThis repo uses a “canonical source + adapters” layout to support multiple AI coding agents.\n\n\nCanonical Source (Single Source of Truth)\n\nAll skill documentation lives in: skills/&lt;skill-name&gt;/SKILL.md\nAlways read the canonical SKILL.md - adapters are just entry points\n\nAdapters (Entry Points for Different Agents)\n\nGitHub Copilot: .github/skills/&lt;skill-name&gt;/SKILL.md (thin wrapper with links to canonical)\nOpenAI Codex: .codex/skills/&lt;skill-name&gt;/SKILL.md (thin wrapper with name/description and links)\nAnthropic Claude: .claude/skills/&lt;skill-name&gt;/SKILL.md (compatible with Claude Code/Desktop)\nGoose: .goose/skills/&lt;skill-name&gt;/SKILL.md (open-source agent compatible with Claude skills)\nGoogle Antigravity: .agent/skills/&lt;skill-name&gt;/SKILL.md (native workspace skills)\nClaude Code: CLAUDE.md (root wrapper pointing back to this file)\nUniversal: AGENTS.md (this file) enforces workflow rules\nModular: Skills are self-contained in skills/ directory\n\nWorkflow Enforcement\n\nBefore using any skill: Open and read the canonical skills/&lt;skill-name&gt;/SKILL.md\nIf you only see a summary/wrapper, follow the links to canonical sections\nRun doctor or verification commands mentioned in canonical docs\n\nKey paths\n\nSkill (submodule): skills/kano-agent-backlog-skill/\n\nCanonical rules: skills/kano-agent-backlog-skill/SKILL.md ← READ THIS\nCopilot adapter: .github/skills/kano-agent-backlog-skill/SKILL.md\nCodex adapter: .codex/skills/kano-agent-backlog-skill/SKILL.md\nClaude adapter: .claude/skills/kano-agent-backlog-skill/SKILL.md\nGoose adapter: .goose/skills/kano-agent-backlog-skill/SKILL.md\nAntigravity adapter: .agent/skills/kano-agent-backlog-skill/SKILL.md\nReferences: skills/kano-agent-backlog-skill/references/\n\n\nSkill: skills/kano-commit-convention-skill/\n\nCanonical rules: skills/kano-commit-convention-skill/SKILL.md ← READ THIS\nCopilot adapter: .github/skills/kano-commit-convention-skill/SKILL.md\nCodex adapter: .codex/skills/kano-commit-convention-skill/SKILL.md\nClaude adapter: .claude/skills/kano-commit-convention-skill/SKILL.md\nGoose adapter: .goose/skills/kano-commit-convention-skill/SKILL.md\nAntigravity adapter: .agent/skills/kano-commit-convention-skill/SKILL.md\n\n\nUniversal Rules: AGENTS.md (this file)\nClaude Code: CLAUDE.md (root wrapper pointing to AGENTS.md)\nDemo backlog (system of record): _kano/backlog/\n\nItems: _kano/backlog/items/\nADRs: _kano/backlog/decisions/\nViews: _kano/backlog/products/&lt;product&gt;/views/\nTools (project-specific): _kano/backlog/tools/ (project-only views/dashboards)\n\n\n\nBacklog discipline (this repo)\n\nUse skills/kano-agent-backlog-skill/SKILL.md for any planning/backlog work.\nIf Python deps are missing, install them with python -m pip install -e skills/kano-agent-backlog-skill (add [dev] when developing the skill itself).\nBefore any code change, create/update items in _kano/backlog/items/ (Epic → Feature → UserStory → Task/Bug).\n\n\n\n                  \n                  IMPORTANT\n                  \n                \n\nStrictly English Only: All backlog item content (Context, Goal, Approach, Worklog, etc.) MUST be written in English. This is a hard requirement for this demo to ensure accessibility for all agents.\n\n\n\nEnforce the Ready gate on Task/Bug (required, non-empty): Context, Goal, Approach, Acceptance Criteria, Risks / Dependencies.\nWorklog is append-only; never rewrite history. Append a Worklog line whenever:\n\na load-bearing decision is made,\nan item state changes,\nscope/approach changes,\nor an ADR is created/linked.\n\n\nUse python skills/kano-agent-backlog-skill/scripts/kano item update-state ... for state transitions so state, updated, and Worklog stay consistent.\nNeed a new backlog product? Run python skills/kano-agent-backlog-skill/scripts/kano backlog init --product &lt;name&gt; --agent &lt;id&gt; to scaffold _kano/backlog/products/&lt;name&gt;/ before creating items.\nFor backlog/skill file operations, go through the kano CLI so audit logs capture the action (no ad-hoc file edits).\nSkill scripts refuse paths outside _kano/backlog/ or _kano/backlog_sandbox/.\nKeep backlog volume under control: only open new items for code/design changes; keep Tasks/Bugs sized to one focused session; avoid ADRs unless there is a real architectural trade-off.\nTicketing threshold (agent-decided):\n\nOpen a new Task/Bug when you will change code/docs/views/scripts.\nOpen an ADR (and link it) when a real trade-off or direction change is decided.\nOtherwise, record the discussion in an existing Worklog; ask if unsure.\n\n\nState ownership: the agent decides when to move items to InProgress or Done; humans observe and can add context.\n\nNaming and storage rules (short)\n\nStore items under _kano/backlog/items/&lt;type&gt;/&lt;bucket&gt;/ and bucket per 100 (0000, 0100, …).\nFilenames are stable: &lt;ID&gt;_&lt;slug&gt;.md (ASCII slug).\nFor Epics, create an adjacent &lt;ID&gt;_&lt;slug&gt;.index.md MOC and register it in _kano/backlog/_meta/indexes.md.\n\nViews (human-friendly)\n\nObsidian Dataview dashboards live under product view roots (e.g. _kano/backlog/products/&lt;product&gt;/views/Dashboard.md).\nGenerate the canonical dashboards via the CLI: python skills/kano-agent-backlog-skill/scripts/kano-backlog view refresh --agent &lt;id&gt; --backlog-root _kano/backlog --product &lt;name&gt;.\n\nNote: _kano/backlog/tools/*.sh are deprecated; use Python tools instead when needed (e.g. generate_demo_views.py, generate_focus_view.py).\n\n\n\nDemo principles\n\nKeep the demo backlog small and traceable; avoid ticket spam.\nAvoid unrelated refactors; every meaningful change should be explainable via a backlog item or ADR (with verification steps).\nIf you change the skill itself, commit inside the submodule skills/kano-agent-backlog-skill/ and update the parent repo submodule pointer.\nSelf-contained skill stance (this demo repo):\n\nPrefer adding automation as new kano subcommands so the skill is usable without manual setup.\nKeep _kano/backlog/tools/ for project-only dashboards/demos (wrapping skill scripts is OK when the behavior is demo-specific).\nOther projects may choose override-only usage; this repo does not. Treat the skill as the source of truth.\n\n\n\nTests\nNo tests or build steps are defined yet.\nTemporary Clause: Local-first First, No Server Implementation Yet\nEffective immediately, this project prioritizes local-first completion and hardening.\nAllowed (Encouraged)\n\nAny work that improves local-first workflows and quality, including:\n\nFile-based canonical data design, schema refinement, validation, and migration tooling\nLocal indexing/search (e.g., SQLite/FTS/sidecar ANN), ingest pipelines, and performance work\nCLI scripts, automation scripts, and developer tooling\nDocumentation, ADRs, threat models, and evaluations for future cloud/server support\nDesigning server interfaces (API/MCP schemas) as documentation/spec only\n\n\n\nNot Allowed (Hard Stop)\n\nDo not implement any server runtime or deployable server component, including but not limited to:\n\nHTTP server, REST API service, gRPC service\nMCP server (any transport)\nWeb UI that depends on a running server\nDocker/K8s deployment for a server component\nAuthentication/authorization implementation as runnable server code\n\n\nDo not add runtime dependencies whose primary purpose is server hosting (unless explicitly approved).\n\nRe-enabling Condition\n\nThis clause remains in effect until a human explicitly removes or disables it.\nAny request that appears to require server implementation must be treated as “spec-only” and should produce:\n\nan ADR and/or design doc,\na roadmap ticket proposal,\na clear note that implementation is deferred due to this clause.\n\n\n\nRationale\n\nKeep the project focused on local-first stability and usability before expanding to cloud/multi-remote deployments.\n\n\nProject backlog discipline (kano-agent-backlog-skill)\n\nUse skills/kano-agent-backlog-skill/SKILL.md for any planning/backlog work.\nBacklog root is _kano/backlog_sandbox/_tmp_tests/guide_test_backlog (items are file-first; index/logs are derived).\nBefore any code change, create/update items in _kano/backlog_sandbox/_tmp_tests/guide_test_backlog/items/ (Epic → Feature → UserStory → Task/Bug).\nEnforce the Ready gate on Task/Bug before starting; Worklog is append-only.\nUse the kano CLI (not ad-hoc edits) so audit logs capture actions:\n\nBootstrap: python skills/kano-agent-backlog-skill/scripts/kano backlog init --product &lt;name&gt; --agent &lt;agent-name&gt;\nCreate/update: python skills/kano-agent-backlog-skill/scripts/kano item create|update-state ... --agent &lt;agent-name&gt;\nViews: python skills/kano-agent-backlog-skill/scripts/kano view refresh --agent &lt;agent-name&gt; --product &lt;name&gt;\n\n\nDashboards auto-refresh after item changes by default (views.auto_refresh=true); use --no-refresh or set it to false if needed.\n\n"},"demo/claude":{"title":"claude","links":[],"tags":[],"content":"CLAUDE.md\n\n\n                  \n                  IMPORTANT\n                  \n                \n\nThis repo uses a “canonical source + adapters” layout.\n\n\n📖 Global Instructions\nFor project-wide rules, architecture, and backlog discipline, please READ AND FOLLOW:\r\n👉 @AGENTS.md\n📦 Agent Skills\nThis repo provides specialized skills in the following locations. Always read the canonical source within each skill folder:\n\nBacklog Management: @skills/kano-agent-backlog-skill/SKILL.md\nCommit Conventions: @skills/kano-commit-convention-skill/SKILL.md\n\n\n🛠️ Quick Start for Claude\n\nExplore the codebase: Read @AGENTS.md for terminal commands and style guides.\nBacklog first: Before taking any action, check the backlog in _kano/backlog/.\nCanonical skills: Refer to the skills/ directory for tool-specific instructions (e.g., @skills/kano-agent-backlog-skill/SKILL.md).\n"},"demo/index":{"title":"index","links":["AGENTS","CLAUDE"],"tags":[],"content":"kano-agent-backlog-skill-demo\n\r\n\r\n\r\n\n\nAI Agent Skills for Spec-Driven Agentic Programming | File-based backlog management | Multi-agent collaboration | Local-first architecture\n\n⚠️ VERSION 0.0.2 - TOPICS + EMBEDDING PIPELINE FOUNDATIONS ⚠️\nThis is the 0.0.2 release of the kano-agent-backlog-skill-demo - an experimental local-first, file-based backlog management system for AI agent collaboration.\nIMPORTANT DISCLAIMERS:\n\n🚧 Rapid Development: System architecture is changing frequently\n⚡ Breaking Changes: APIs, file formats, and workflows may change without notice\n🔬 Experimental: Many features are incomplete or unstable\n❌ No Guarantees: No stability, compatibility, or support guarantees\n📝 Documentation Lag: Documentation may not reflect current implementation\n\nWhat’s New in 0.0.2:\n\n✅ Core backlog item management (Epic, Feature, UserStory, Task, Bug)\n✅ Workset execution cache for per-item context\n✅ Topic-based context switching and grouping\n✅ Topic templates, cross-references, snapshots, and merge/split operations\n✅ Code snippet collection in topic materials\n✅ Deterministic brief generation from materials\n✅ ADR (Architecture Decision Record) support\n✅ Multi-agent coordination (Canonical + Adapters architecture)\n✅ Native support for Copilot, Codex, Claude, and Goose\n✅ CLI commands for all core operations\n✅ Property-based testing with Hypothesis\n🚧 SQLite indexing (experimental)\n🚧 Embedding search foundations (cross-lingual requirement, per-model index strategy)\n\nOverview\nCurrent Status: Version 0.0.2\nThis repository demonstrates an evolving approach to transform agent collaboration into a durable, auditable backlog system. The core concept is to persist planning, decisions, and work items as structured markdown files rather than losing context in chat conversations.\nWhat’s Working in 0.0.2:\n\n✅ Markdown-based work item storage with frontmatter metadata\n✅ CLI scripts for item creation, state transitions, and worklog management\n✅ Workset execution cache for per-item context isolation\n✅ Topic-based context switching with code snippet collection\n✅ Deterministic brief generation from collected materials\n✅ Plain markdown and Obsidian Dataview dashboard generation\n✅ Multi-agent coordination through shared backlog\n✅ ADR (Architecture Decision Record) workflow\n✅ Property-based testing with Hypothesis\n\nWhat’s Unstable/Incomplete:\n\n⚠️ File formats and schemas (may change)\n⚠️ CLI interfaces and commands (evolving)\n⚠️ Configuration system (in flux)\n⚠️ SQLite indexing (experimental)\n⚠️ Embedding search (experimental)\n⚠️ Documentation accuracy (catching up)\n\nKey Features\n\nLocal-first backlog: All work items stored as markdown files with frontmatter metadata\nHierarchical work items: Epic → Feature → User Story → Task/Bug\nAppend-only worklog: Auditable decision trail for each work item\nArchitecture Decision Records (ADRs): Capture significant technical decisions\nWorkset execution cache: Per-item working context with plan, notes, and deliverables to prevent agent drift\nTopic-based context switching: Group related items and documents for rapid focus area changes\nCode snippet collection: Capture and organize code references in topic materials\nDeterministic distillation: Generate briefs from collected materials for consistent context loading\nMultiple views: Obsidian Dataview dashboards and plain markdown reports\nMulti-product support: Organize backlogs for different products/projects\nMulti-agent coordination: Canonical + Adapters layout for shared backlog\nBroad compatibility: Support for Copilot, Codex, Claude, Goose, and Antigravity\n🚧 WIP: Optional SQLite index - Fast queries while keeping files as source of truth\n🚧 WIP: Embedding search - Local semantic search for backlog items (experimental)\n\nRepository Structure\n├── _kano/backlog/              # Main backlog directory (system of record)\r\n├── skills/                     # Canonical sources (git submodules or local)\r\n│   ├── kano-agent-backlog-skill/         # **CANONICAL** (single source of truth)\r\n│   └── kano-commit-convention-skill/\r\n├── .github/skills/             # GitHub Copilot adapters\r\n├── .codex/skills/              # OpenAI Codex adapters\r\n├── .claude/skills/             # Anthropic Claude adapters\r\n├── .goose/skills/              # Goose adapters\r\n├── .agent/skills/              # Google Antigravity adapters\r\n├── AGENTS.md                   # Universal guidelines and workflow rules\r\n├── CLAUDE.md                   # Claude Code root adapter (points to AGENTS.md)\r\n└── README.md                   # This file\n\nMulti-Agent Architecture: Canonical + Adapters\nThis repository uses a “canonical source + adapters” layout to maintain a single source of truth while supporting mission-critical directories for different agents.\n\nCanonical Source: Full documentation and logic stay in skills/&lt;skill-name&gt;/SKILL.md.\nAdapters: Lightweight “thin wrappers” exist in agent-specific folders (like .github/skills/ or .claude/skills/) that point back to the canonical source via links or @ references.\nUniversal Rules: AGENTS.md defines project-wide workflow rules and discipline for all agents.\nEntry Points:\n\nClaude Code: Uses CLAUDE.md as a root entrance to AGENTS.md.\nOther Agents: Use their respective .folder/skills/ adapters.\n\n\n\nSee AGENTS.md for detailed workflow enforcement rules.\nSkill Architecture: Self-Contained Design\n\nAll source code (domain library + CLI) lives under src/\nAll dependencies are unified in pyproject.toml\nThe entire skills/kano-agent-backlog-skill/ directory can be copied to any project (or used as a git submodule)\nNo external dependencies on kano-backlog-core or kano-cli projects\n\nThis follows the “Self-contained skill stance” principle defined in AGENTS.md: keep all automation and tools needed to use the skill within the skill directory itself, avoiding scattered dependencies.\nGetting Started (Experimental)\n⚠️ WARNING: This is designed for AI agent automation, not manual operation.\nPrerequisites\n\nAI agent with file system access (Amazon Kiro, Claude, ChatGPT, Cursor, Windsurf, etc.)\nPython 3.10+ (required by skills/kano-agent-backlog-skill/pyproject.toml)\nGit (for version control and submodules)\nPatience: Expect things to break or change\n\nQuick Start\n1. Clone the repository:\ngit clone github.com/dorgonman/kano-agent-backlog-skill-demo.git\ncd kano-agent-backlog-skill-demo\n2. Initialize submodules:\ngit submodule update --init --recursive\n3. Install the skill:\npython -m pip install -e skills/kano-agent-backlog-skill\n4. Verify installation:\nkano-backlog --help\n5. Explore the demo backlog:\n# List work items\nkano-backlog item list --product kano-agent-backlog-skill\n \n# View dashboard\nkano-backlog view refresh --product kano-agent-backlog-skill --agent demo\n \n# List topics\nkano-backlog topic list\n \n# List worksets\nkano-backlog workset list\nOptional: Dev Dependencies\nFor development or embedding search features:\npython -m pip install -e &quot;skills/kano-agent-backlog-skill[dev]&quot;\nNote: FAISS and sentence-transformers may require platform-specific installation.\nPrerequisite install (recommended)\nInstall the skill (and its CLI dependencies) into your environment once:\npython -m pip install -e skills/kano-agent-backlog-skill\nOptional dev/embedding dependencies can be added with extras:\npython -m pip install -e &quot;skills/kano-agent-backlog-skill[dev]&quot;\n# Install FAISS / sentence-transformers manually per platform before running embedding workflows.\nVerify Installation\nAfter installation, verify the CLI is available:\nkano-backlog --help\nkano-backlog workset --help\nkano-backlog topic --help\nAgent-First Setup\nInstead of manual installation, ask your AI agent to:\n&quot;Please help me set up the kano-agent-backlog-skill demo. \r\nInitialize the backlog structure, and show me what work items exist.&quot;\n\nThe agent should automatically:\n\nInitialize submodules if needed\nExplore the backlog structure\nShow you available work items\nGenerate current dashboard views\n\nIf something breaks, just ask:\n&quot;The backlog setup failed. Please check what went wrong and fix it.&quot;\n\nAgent Workflow (Chat-Driven)\nThis system is designed for conversational agent interaction, not manual commands.\nStarting a New Work Session\nAsk your agent:\n&quot;Please check the backlog and pick a ready task for me to work on. \r\nCreate the work item if needed and start working on it.&quot;\n\nThe agent should:\n\nScan available work items\nFind or create a suitable task\nUpdate the item to “InProgress”\nBegin implementation\nLog decisions in the worklog\n\nCreating New Work Items\nInstead of manual scripts, just say:\n&quot;I need to add a new feature for user authentication. \r\nPlease create the appropriate backlog items and start planning.&quot;\n\nOr for bugs:\n&quot;There&#039;s a bug in the login system - users can&#039;t reset passwords. \r\nPlease create a bug item and investigate the issue.&quot;\n\nChecking Progress\nAsk for status updates:\n&quot;Show me the current backlog status and what&#039;s in progress.&quot;\n\nOr:\n&quot;What work items are ready to be picked up?&quot;\n\nCompleting Work\nWhen done:\n&quot;I&#039;ve finished the authentication feature. Please update the backlog \r\nand mark the work item as complete.&quot;\n\n⚠️ Note: Script interfaces change frequently. Let the agent handle the technical details.\nBacklog Discipline\nThis demo follows these principles:\n\nReady gate enforcement: Tasks/Bugs must have all required fields before starting\nAppend-only Worklog: Never rewrite history; append new entries\nControlled volume: Only open items for actual code/design changes\nSized work items: Tasks/Bugs should fit in one focused session\nADRs for trade-offs: Only create ADRs when there’s a real architectural decision\n\nViewing the Backlog\nAsk your agent to show you the current state:\n&quot;Please show me the current backlog dashboard and highlight \r\nwhat needs attention.&quot;\n\nOr for specific views:\n&quot;What work items are currently in progress?&quot;\r\n&quot;Show me all completed work from this week.&quot;\r\n&quot;What new tasks are ready to be picked up?&quot;\n\nGenerated Views (Agent-Managed)\nThe agent automatically maintains views under product roots (e.g. _kano/backlog/products/&lt;product&gt;/views/):\n\nActive work dashboard\nNew items queue\nCompleted work history\n\nObsidian Integration (Optional)\nIf you use Obsidian, ask:\n&quot;Please set up this backlog for Obsidian Dataview integration.&quot;\n\nWork Item Types\n\nEpic: Large initiative spanning multiple features (e.g., “Milestone 0.0.2”)\nFeature: Cohesive capability (e.g., “Local-first backlog system”)\nUser Story: User-facing functionality (e.g., “Plan before code”)\nTask: Technical work item (e.g., “Add test script”)\nBug: Defect to be fixed\n\nWorkset and Topic Features\nThe backlog system includes two powerful features for managing agent execution context and preventing drift during complex work sessions.\nWorksets: Per-Item Execution Cache\nWorksets provide a focused, per-item execution context that prevents agent drift during task work. Each workset is a materialized cache bundle stored in _kano/backlog/.cache/worksets/items/&lt;item-id&gt;/ containing:\n\nplan.md: Checklist template derived from acceptance criteria\nnotes.md: Working notes with Decision: markers for ADR promotion\ndeliverables/: Staging area for work artifacts before promotion\nmeta.json: Metadata including agent, timestamps, TTL, and source references\n\nWorkset Commands\nInitialize a workset:\nkano-backlog workset init --item TASK-0042 --agent my-agent --ttl-hours 72\nGet next action from plan:\nkano-backlog workset next --item TASK-0042\nThis command is the workset’s “keep me on track” primitive:\n\nIt reads the workset’s plan.md (a checklist derived from the item’s Acceptance Criteria).\nIt returns the first unchecked checkbox item (- [ ] ...).\nIf everything is checked, it prints a completion message.\n\nIt does not automatically mark anything as done; you check items off in plan.md as you complete them, then run kano-backlog workset next again to get the next step. Use --format json if you want structured output for tooling/agent automation.\nRefresh from canonical files:\nkano-backlog workset refresh --item TASK-0042 --agent my-agent\nPromote deliverables to canonical artifacts:\nkano-backlog workset promote --item TASK-0042 --agent my-agent\n# Dry run to preview\nkano-backlog workset promote --item TASK-0042 --agent my-agent --dry-run\nDetect ADR candidates in notes:\nkano-backlog workset detect-adr --item TASK-0042\nList all worksets:\nkano-backlog workset list\nCleanup expired worksets:\nkano-backlog workset cleanup --ttl-hours 72\n# Dry run to preview\nkano-backlog workset cleanup --ttl-hours 72 --dry-run\nAgent Workflow with Worksets\nAsk your agent:\n&quot;Initialize a workset for TASK-0042 and show me the next action to take.&quot;\n\nThe agent will:\n\nCreate a workset directory with plan, notes, and deliverables\nParse acceptance criteria into a checklist in plan.md\nTrack progress through the checklist\nCapture decisions with Decision: markers in notes.md\nStage work artifacts in deliverables/\nPromote deliverables back to canonical artifacts when done\n\nExample conversation:\nUser: &quot;Start working on TASK-0042&quot;\r\nAgent: &quot;I&#039;ll initialize a workset and begin. The first step is...&quot;\r\n\r\nUser: &quot;What&#039;s next?&quot;\r\nAgent: &quot;According to the plan, the next unchecked step is...&quot;\r\n\r\nUser: &quot;I&#039;ve completed the implementation&quot;\r\nAgent: &quot;Great! I&#039;ll promote the deliverables to the canonical artifacts directory.&quot;\n\nTopics: Context Switching and Grouping\nTopics provide a higher-level grouping mechanism for related items and documents, enabling rapid context switching when focus areas change during a conversation.\nCurrent implementation (see _kano/backlog/products/kano-agent-backlog-skill/items/task/0100/KABSD-TSK-0190_topic-lifecycle-materials-buffer-workset-merge.md) stores topics in _kano/backlog/topics/&lt;topic&gt;/ so the deterministic brief.generated.md can be shared/reviewed in-repo, while keeping per-agent active-topic tracking in shared cache state.\n\nmanifest.json: Topic metadata, seed items, pinned documents\nbrief.md: Stable, human-maintained brief (do not overwrite automatically)\nbrief.generated.md: Deterministic distilled brief (generated/overwritten by kano-backlog topic distill)\nsynthesis/: Working outputs for distillation (derived)\npublish/: Prepared write-backs / patch skeletons (derived)\nmaterials/: Raw collected materials (snippets, links, extracts, logs)\n\nclips/: Code snippet references with line ranges\nlinks/: External document references\nextracts/: Cached text extracts\nlogs/: Optional collected logs\n\n\n\nNotes:\n\nRaw materials are treated as cache: by default this repo ignores _kano/backlog/topics/**/materials/ via .gitignore.\nActive topic state is stored under _kano/backlog/.cache/worksets/state.json (and topic entries under _kano/backlog/.cache/worksets/topics/).\n\nTopic Commands\nCreate a topic:\nkano-backlog topic create auth-refactor --agent my-agent\nAdd items to topic:\nkano-backlog topic add auth-refactor --item TASK-0042\nkano-backlog topic add auth-refactor --item TASK-0043\nPin documents for context:\nkano-backlog topic pin auth-refactor --doc _kano/backlog/decisions/ADR-0005-auth-strategy.md\nAdd code snippets to materials:\nkano-backlog topic add-snippet auth-refactor --file src/auth.py --start 10 --end 25\nDistill materials into brief:\nkano-backlog topic distill auth-refactor\nAudit decision write-back (writes a report into topic publish/):\nkano-backlog topic decision-audit auth-refactor\nkano-backlog topic decision-audit auth-refactor --format json\nWrite back a decision to a work item:\nkano-backlog workitem add-decision KABSD-TSK-0001 \\\n  --decision &quot;Use X over Y because ...&quot; \\\n  --source &quot;_kano/backlog/topics/auth-refactor/synthesis/decision-notes.md&quot; \\\n  --agent my-agent \\\n  --product kano-agent-backlog-skill\nSwitch active topic:\nkano-backlog topic switch auth-refactor --agent my-agent\nExport context bundle:\nkano-backlog topic export-context auth-refactor --format markdown\nkano-backlog topic export-context auth-refactor --format json\nList all topics:\nkano-backlog topic list --agent my-agent\nClose a topic (enables TTL cleanup):\nkano-backlog topic close auth-refactor --agent my-agent\nCleanup closed topics:\nkano-backlog topic cleanup --ttl-days 14 --apply\n# Dry run to preview\nkano-backlog topic cleanup --ttl-days 14\nAgent Workflow with Topics\nAsk your agent:\n&quot;Create a topic for the authentication refactor work and add the related tasks.&quot;\n\nThe agent will:\n\nCreate a topic directory with manifest and materials structure\nAdd related backlog items to the topic’s seed_items\nPin relevant ADRs and design documents\nCollect code snippets into materials/clips/\nGenerate a distilled brief from collected materials\nSwitch to the topic when you change focus\nExport context bundles for loading into working memory\n\nExample conversation:\nUser: &quot;I&#039;m switching to work on the payment flow&quot;\r\nAgent: &quot;I&#039;ll create a topic and gather the relevant context...&quot;\r\nAgent: &quot;Topic &#039;payment-flow&#039; created with 3 tasks and 2 ADRs. Here&#039;s the brief...&quot;\r\n\r\nUser: &quot;Show me the authentication work&quot;\r\nAgent: &quot;Switching to topic &#039;auth-refactor&#039;... Here are the 5 tasks and relevant code snippets...&quot;\n\nTopic Lifecycle\n\nCreate: Initialize topic with manifest\nCollect: Add items, pin documents, collect code snippets\nDistill: Generate deterministic brief from materials\nSwitch: Change active topic for context switching\nExport: Generate context bundles for agent consumption\nClose: Mark topic as closed when work is complete\nCleanup: Remove raw materials after TTL (brief.md persists)\n\nWorksets vs Topics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeatureWorksetsTopicsScopeSingle itemMultiple items + documentsPurposeExecution cacheContext groupingLifetimeTask durationWork area durationContentPlan, notes, deliverablesItems, docs, snippets, briefUse CasePrevent drift during taskSwitch focus areasStorage.cache/worksets/items/topics/&lt;topic&gt;/ (materials ignored; active topic tracked in _kano/backlog/.cache/worksets/state.json)\nConfiguration\nBacklog configuration is in _kano/backlog/products/&lt;product&gt;/_config/config.toml (product-specific) and _kano/backlog/_shared/defaults.toml (shared defaults).\nContributing (Pre-Alpha)\nCurrent Status: This is version 0.0.2 - an experimental demo repository in rapid development.\nBefore Contributing:\n\nExpect frequent breaking changes\nCheck recent commits for current state\nUnderstand this is experimental software\nNo stability guarantees\n\nHow to Contribute:\n\nOpen issues for bugs/suggestions\nDiscuss major changes before implementing\nExpect your contributions may be refactored heavily\nFocus on core concepts rather than implementation details\n\nFor the main skill development, see kano-agent-backlog-skill (also experimental).\nRoadmap\nVersion 0.0.2 (Current):\n\n✅ Core backlog management\n✅ Workset and topic features\n✅ Multi-agent collaboration patterns\n✅ CLI commands\n✅ Property-based testing\n\nFuture Versions:\n\n🔮 Stable file format schema\n🔮 Enhanced SQLite indexing\n🔮 Production-ready embedding search\n🔮 Web UI for backlog visualization\n🔮 Git integration for version control\n🔮 Export/import for backlog migration\n🔮 Plugin system for custom workflows\n🔮 Performance optimizations\n\nNote: Roadmap is subject to change based on experimentation and feedback.\nLicense\nMIT\nSee the individual skill repositories for license information.\nLearn More\n\nSkill Documentation: skills/kano-agent-backlog-skill/SKILL.md (after initializing submodules)\nAgent Guidelines: AGENTS.md\nExample Backlogs: Explore _kano/backlog/products/ for real-world examples\nADR Directory: _kano/backlog/decisions/ for architectural decisions\n\nTroubleshooting\nBacklog not found:\n\nEnsure you’re in the repository root\nCheck _kano/backlog/ directory exists\nInitialize a product: kano-backlog admin init --product my-product --agent my-agent\n\nAgent asks for help:\n\nPoint agent to AGENTS.md for guidelines\nReference skills/kano-agent-backlog-skill/SKILL.md for detailed instructions\nShow agent the example backlogs in _kano/backlog/products/\n\nFrequently Asked Questions\nQ: Is this production-ready?\r\nA: No. This is version 0.0.2 - experimental software. Use at your own risk.\nQ: Can I use this for my project?\r\nA: Yes, but expect breaking changes. Copy the skill directory or use as a git submodule.\nQ: How do I migrate my existing backlog?\r\nA: Migration tools are not yet available. Manual conversion required.\nQ: Does this work with [my favorite agent]?\r\nA: It should work with any agent that has file system access and can run CLI commands. Tested with Amazon Kiro, Claude, Cursor, Windsurf, and others.\nQ: Why file-based instead of a database?\r\nA: Files are human-readable, version-controllable, and don’t require a server. Optional SQLite indexes provide query performance without lock-in.\nQ: Can multiple agents work on the same backlog simultaneously?\r\nA: Yes, but coordination is manual. File conflicts may occur. Use git for version control and conflict resolution.\nPhilosophy\nThis experimental demo explores a “backlog-first” approach where:\n\nPlanning before coding: Work items are created and refined before implementation begins\nDurable context: Planning and decisions persist in files, not lost in chat conversations\nAuditable trail: Every change is logged with timestamps and agent identity\nRecoverable state: Any agent can pick up where another left off\nHuman-readable source of truth: Markdown files are the canonical source, not databases\nOptional indexes: SQLite and embedding indexes enable powerful queries without lock-in\nMulti-agent coordination: Shared backlog enables multiple agents to collaborate effectively\nContext isolation: Worksets prevent drift during complex tasks\nRapid context switching: Topics enable quick focus area changes\n\nVersion 0.0.2 Status: These principles are being tested and refined. Implementation is evolving but demonstrates the core concepts in action. The _kano/backlog/ directory contains real-world examples of this philosophy applied to the development of the system itself.\nDual-Readability Design (Topic &amp; Snapshot)\nA key challenge in modern development is that Humans struggle with focus (hard to jump between contexts) while Agents struggle with collaboration (hard to share implicit context).\nThis project checks every artifact against a Dual-Readability Principle:\n\nHuman-Readable: High-level summaries, clear checklists, and “manager-friendly” reports (e.g., brief.md, Report_pm.md) so humans can make rapid decisions without reading code.\nAgent-Readable: Structural precision, file paths, line numbers, and explicit “Next Step” markers (e.g., manifest.json, stub_inventory) so agents can act without hallucinating.\n\nTopics and Snapshots are the concrete implementation of this philosophy:\n\nTopics: Allow humans to “load” a focus area mentally in seconds, while giving Agents a precise list of files and snippets to load into their context window.\nSnapshots: Give humans a trustable “State of the Union”, while giving Agents a literal checklist of TODOs and NotImplementedErrors to attack next.\n\nThe Agent Semantic Memory Model\nTo help Agents (and humans) navigate the complexity of software development, this project maps its features to the standard cognitive memory model:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMemory TypeEquivalentPurposeLifespanAnalogyShort-Term(Working Memory)WorksetExecution StatePrevents drift during a single task. Contains the immediate plan, scratchpad notes, and temp files.Hours(Task duration)The “scratchpad” or “RAM” cleared after use.Medium-Term(Contextual)TopicFocus ScopeGroups related files, specs, and definitions for a feature. Allows rapid context switching.Days/Weeks(Feature duration)The “project folder” on your desk.Long-Term(Semantic)ADR &amp; CanonicalInstitutional KnowledgeImmutable decisions (ADRs) and the source code itself. The “Soul” of the project.Permanent(Project duration)The “library archives” or “textbooks”.\nWhy this matters:\n\nAgents often “forget” instructions (Short-term failure). → Solution: Worksets force them to read a plan.md.\nAgents often “hallucinate” unrelated files (Medium-term failure). → Solution: Topics bind them to a specific manifest.json.\nAgents often “overwrite” architectural rules (Long-term failure). → Solution: ADRs provide immutable constraints.\n\nBuilt with Multi-Agent Collaboration\nThis project is itself a demonstration of multi-agent collaboration in action. The codebase has been developed through iterative collaboration with multiple AI coding assistants, each bringing unique strengths to the development process:\n\nCodex - Early prototyping and concept exploration\nGitHub Copilot - Code completion, suggestions, and inline assistance\nGoogle Antigravity - Alternative perspectives and design discussions\nAmazon Q - AWS infrastructure guidance and cloud architecture\nAmazon Kiro - Primary development agent and implementation\nCursor - Inline editing, refactoring, and code navigation\nWindsurf - Additional development support and testing\nOpenCode - Testing, validation, and quality assurance\n\nHow Multi-Agent Collaboration Works\nThe backlog system you see here was used to coordinate work across these agents, proving the concept through real-world usage:\n\nShared Backlog: All agents work from the same file-based backlog in _kano/backlog/\nWorklog Trail: Every decision and state change is recorded in append-only worklogs\nADR Documentation: Architectural decisions are captured in _kano/backlog/decisions/\nContext Switching: Topics enable agents to quickly load relevant context for different work areas\nWorkset Isolation: Per-item worksets prevent agents from drifting during complex tasks\n\nAgent Collaboration Patterns\nSequential Handoff:\nAgent A: Creates Epic and Feature items\r\nAgent B: Breaks down into User Stories\r\nAgent C: Implements Tasks with worksets\r\nAgent D: Reviews and updates worklogs\n\nParallel Work:\nAgent A: Works on TASK-0042 (auth module)\r\nAgent B: Works on TASK-0043 (payment module)\r\nBoth: Update shared backlog independently\r\nBoth: Coordinate through ADRs and worklogs\n\nContext Switching:\nAgent: Switches from &quot;auth-refactor&quot; topic to &quot;payment-flow&quot; topic\r\nAgent: Loads new context bundle with relevant items and documents\r\nAgent: Continues work with full context of new focus area\n\nWhy This Matters\nTraditional agent collaboration loses context in chat conversations. This system demonstrates:\n\nDurable Context: Planning and decisions persist in files, not chat\nAuditable Trail: Every change is logged with timestamps and agent identity\nRecoverable State: Any agent can pick up where another left off\nScalable Coordination: Multiple agents can work on different areas simultaneously\nHuman Oversight: Humans can review, approve, or redirect at any checkpoint\n\nEvery feature, decision, and trade-off in this codebase is documented in the _kano/backlog/ directory - a living example of the “backlog-first” philosophy in action."},"examples/index":{"title":"Examples & Templates","links":["examples/tokenizer_telemetry_demo.py","templates/AGENTS.block","templates/CLAUDE.block","templates/snapshot_report_developer","templates/snapshot_report_pm","templates/snapshot_report_qa"],"tags":[],"content":"Examples &amp; Templates\nCode examples and templates\n\nREADME\nREADME\nREADME\nREADME\nREADME\nREADME\n"},"index":{"title":"index","links":["AGENTS","CLAUDE"],"tags":[],"content":"kano-agent-backlog-skill-demo\n\r\n\r\n\r\n\n\nAI Agent Skills for Spec-Driven Agentic Programming | File-based backlog management | Multi-agent collaboration | Local-first architecture\n\n⚠️ VERSION 0.0.2 - TOPICS + EMBEDDING PIPELINE FOUNDATIONS ⚠️\nThis is the 0.0.2 release of the kano-agent-backlog-skill-demo - an experimental local-first, file-based backlog management system for AI agent collaboration.\nIMPORTANT DISCLAIMERS:\n\n🚧 Rapid Development: System architecture is changing frequently\n⚡ Breaking Changes: APIs, file formats, and workflows may change without notice\n🔬 Experimental: Many features are incomplete or unstable\n❌ No Guarantees: No stability, compatibility, or support guarantees\n📝 Documentation Lag: Documentation may not reflect current implementation\n\nWhat’s New in 0.0.2:\n\n✅ Core backlog item management (Epic, Feature, UserStory, Task, Bug)\n✅ Workset execution cache for per-item context\n✅ Topic-based context switching and grouping\n✅ Topic templates, cross-references, snapshots, and merge/split operations\n✅ Code snippet collection in topic materials\n✅ Deterministic brief generation from materials\n✅ ADR (Architecture Decision Record) support\n✅ Multi-agent coordination (Canonical + Adapters architecture)\n✅ Native support for Copilot, Codex, Claude, and Goose\n✅ CLI commands for all core operations\n✅ Property-based testing with Hypothesis\n🚧 SQLite indexing (experimental)\n🚧 Embedding search foundations (cross-lingual requirement, per-model index strategy)\n\nOverview\nCurrent Status: Version 0.0.2\nThis repository demonstrates an evolving approach to transform agent collaboration into a durable, auditable backlog system. The core concept is to persist planning, decisions, and work items as structured markdown files rather than losing context in chat conversations.\nWhat’s Working in 0.0.2:\n\n✅ Markdown-based work item storage with frontmatter metadata\n✅ CLI scripts for item creation, state transitions, and worklog management\n✅ Workset execution cache for per-item context isolation\n✅ Topic-based context switching with code snippet collection\n✅ Deterministic brief generation from collected materials\n✅ Plain markdown and Obsidian Dataview dashboard generation\n✅ Multi-agent coordination through shared backlog\n✅ ADR (Architecture Decision Record) workflow\n✅ Property-based testing with Hypothesis\n\nWhat’s Unstable/Incomplete:\n\n⚠️ File formats and schemas (may change)\n⚠️ CLI interfaces and commands (evolving)\n⚠️ Configuration system (in flux)\n⚠️ SQLite indexing (experimental)\n⚠️ Embedding search (experimental)\n⚠️ Documentation accuracy (catching up)\n\nKey Features\n\nLocal-first backlog: All work items stored as markdown files with frontmatter metadata\nHierarchical work items: Epic → Feature → User Story → Task/Bug\nAppend-only worklog: Auditable decision trail for each work item\nArchitecture Decision Records (ADRs): Capture significant technical decisions\nWorkset execution cache: Per-item working context with plan, notes, and deliverables to prevent agent drift\nTopic-based context switching: Group related items and documents for rapid focus area changes\nCode snippet collection: Capture and organize code references in topic materials\nDeterministic distillation: Generate briefs from collected materials for consistent context loading\nMultiple views: Obsidian Dataview dashboards and plain markdown reports\nMulti-product support: Organize backlogs for different products/projects\nMulti-agent coordination: Canonical + Adapters layout for shared backlog\nBroad compatibility: Support for Copilot, Codex, Claude, Goose, and Antigravity\n🚧 WIP: Optional SQLite index - Fast queries while keeping files as source of truth\n🚧 WIP: Embedding search - Local semantic search for backlog items (experimental)\n\nRepository Structure\n├── _kano/backlog/              # Main backlog directory (system of record)\r\n├── skills/                     # Canonical sources (git submodules or local)\r\n│   ├── kano-agent-backlog-skill/         # **CANONICAL** (single source of truth)\r\n│   └── kano-commit-convention-skill/\r\n├── .github/skills/             # GitHub Copilot adapters\r\n├── .codex/skills/              # OpenAI Codex adapters\r\n├── .claude/skills/             # Anthropic Claude adapters\r\n├── .goose/skills/              # Goose adapters\r\n├── .agent/skills/              # Google Antigravity adapters\r\n├── AGENTS.md                   # Universal guidelines and workflow rules\r\n├── CLAUDE.md                   # Claude Code root adapter (points to AGENTS.md)\r\n└── README.md                   # This file\n\nMulti-Agent Architecture: Canonical + Adapters\nThis repository uses a “canonical source + adapters” layout to maintain a single source of truth while supporting mission-critical directories for different agents.\n\nCanonical Source: Full documentation and logic stay in skills/&lt;skill-name&gt;/SKILL.md.\nAdapters: Lightweight “thin wrappers” exist in agent-specific folders (like .github/skills/ or .claude/skills/) that point back to the canonical source via links or @ references.\nUniversal Rules: AGENTS.md defines project-wide workflow rules and discipline for all agents.\nEntry Points:\n\nClaude Code: Uses CLAUDE.md as a root entrance to AGENTS.md.\nOther Agents: Use their respective .folder/skills/ adapters.\n\n\n\nSee AGENTS.md for detailed workflow enforcement rules.\nSkill Architecture: Self-Contained Design\n\nAll source code (domain library + CLI) lives under src/\nAll dependencies are unified in pyproject.toml\nThe entire skills/kano-agent-backlog-skill/ directory can be copied to any project (or used as a git submodule)\nNo external dependencies on kano-backlog-core or kano-cli projects\n\nThis follows the “Self-contained skill stance” principle defined in AGENTS.md: keep all automation and tools needed to use the skill within the skill directory itself, avoiding scattered dependencies.\nGetting Started (Experimental)\n⚠️ WARNING: This is designed for AI agent automation, not manual operation.\nPrerequisites\n\nAI agent with file system access (Amazon Kiro, Claude, ChatGPT, Cursor, Windsurf, etc.)\nPython 3.10+ (required by skills/kano-agent-backlog-skill/pyproject.toml)\nGit (for version control and submodules)\nPatience: Expect things to break or change\n\nQuick Start\n1. Clone the repository:\ngit clone github.com/dorgonman/kano-agent-backlog-skill-demo.git\ncd kano-agent-backlog-skill-demo\n2. Initialize submodules:\ngit submodule update --init --recursive\n3. Install the skill:\npython -m pip install -e skills/kano-agent-backlog-skill\n4. Verify installation:\nkano-backlog --help\n5. Explore the demo backlog:\n# List work items\nkano-backlog item list --product kano-agent-backlog-skill\n \n# View dashboard\nkano-backlog view refresh --product kano-agent-backlog-skill --agent demo\n \n# List topics\nkano-backlog topic list\n \n# List worksets\nkano-backlog workset list\nOptional: Dev Dependencies\nFor development or embedding search features:\npython -m pip install -e &quot;skills/kano-agent-backlog-skill[dev]&quot;\nNote: FAISS and sentence-transformers may require platform-specific installation.\nPrerequisite install (recommended)\nInstall the skill (and its CLI dependencies) into your environment once:\npython -m pip install -e skills/kano-agent-backlog-skill\nOptional dev/embedding dependencies can be added with extras:\npython -m pip install -e &quot;skills/kano-agent-backlog-skill[dev]&quot;\n# Install FAISS / sentence-transformers manually per platform before running embedding workflows.\nVerify Installation\nAfter installation, verify the CLI is available:\nkano-backlog --help\nkano-backlog workset --help\nkano-backlog topic --help\nAgent-First Setup\nInstead of manual installation, ask your AI agent to:\n&quot;Please help me set up the kano-agent-backlog-skill demo. \r\nInitialize the backlog structure, and show me what work items exist.&quot;\n\nThe agent should automatically:\n\nInitialize submodules if needed\nExplore the backlog structure\nShow you available work items\nGenerate current dashboard views\n\nIf something breaks, just ask:\n&quot;The backlog setup failed. Please check what went wrong and fix it.&quot;\n\nAgent Workflow (Chat-Driven)\nThis system is designed for conversational agent interaction, not manual commands.\nStarting a New Work Session\nAsk your agent:\n&quot;Please check the backlog and pick a ready task for me to work on. \r\nCreate the work item if needed and start working on it.&quot;\n\nThe agent should:\n\nScan available work items\nFind or create a suitable task\nUpdate the item to “InProgress”\nBegin implementation\nLog decisions in the worklog\n\nCreating New Work Items\nInstead of manual scripts, just say:\n&quot;I need to add a new feature for user authentication. \r\nPlease create the appropriate backlog items and start planning.&quot;\n\nOr for bugs:\n&quot;There&#039;s a bug in the login system - users can&#039;t reset passwords. \r\nPlease create a bug item and investigate the issue.&quot;\n\nChecking Progress\nAsk for status updates:\n&quot;Show me the current backlog status and what&#039;s in progress.&quot;\n\nOr:\n&quot;What work items are ready to be picked up?&quot;\n\nCompleting Work\nWhen done:\n&quot;I&#039;ve finished the authentication feature. Please update the backlog \r\nand mark the work item as complete.&quot;\n\n⚠️ Note: Script interfaces change frequently. Let the agent handle the technical details.\nBacklog Discipline\nThis demo follows these principles:\n\nReady gate enforcement: Tasks/Bugs must have all required fields before starting\nAppend-only Worklog: Never rewrite history; append new entries\nControlled volume: Only open items for actual code/design changes\nSized work items: Tasks/Bugs should fit in one focused session\nADRs for trade-offs: Only create ADRs when there’s a real architectural decision\n\nViewing the Backlog\nAsk your agent to show you the current state:\n&quot;Please show me the current backlog dashboard and highlight \r\nwhat needs attention.&quot;\n\nOr for specific views:\n&quot;What work items are currently in progress?&quot;\r\n&quot;Show me all completed work from this week.&quot;\r\n&quot;What new tasks are ready to be picked up?&quot;\n\nGenerated Views (Agent-Managed)\nThe agent automatically maintains views under product roots (e.g. _kano/backlog/products/&lt;product&gt;/views/):\n\nActive work dashboard\nNew items queue\nCompleted work history\n\nObsidian Integration (Optional)\nIf you use Obsidian, ask:\n&quot;Please set up this backlog for Obsidian Dataview integration.&quot;\n\nWork Item Types\n\nEpic: Large initiative spanning multiple features (e.g., “Milestone 0.0.2”)\nFeature: Cohesive capability (e.g., “Local-first backlog system”)\nUser Story: User-facing functionality (e.g., “Plan before code”)\nTask: Technical work item (e.g., “Add test script”)\nBug: Defect to be fixed\n\nWorkset and Topic Features\nThe backlog system includes two powerful features for managing agent execution context and preventing drift during complex work sessions.\nWorksets: Per-Item Execution Cache\nWorksets provide a focused, per-item execution context that prevents agent drift during task work. Each workset is a materialized cache bundle stored in _kano/backlog/.cache/worksets/items/&lt;item-id&gt;/ containing:\n\nplan.md: Checklist template derived from acceptance criteria\nnotes.md: Working notes with Decision: markers for ADR promotion\ndeliverables/: Staging area for work artifacts before promotion\nmeta.json: Metadata including agent, timestamps, TTL, and source references\n\nWorkset Commands\nInitialize a workset:\nkano-backlog workset init --item TASK-0042 --agent my-agent --ttl-hours 72\nGet next action from plan:\nkano-backlog workset next --item TASK-0042\nThis command is the workset’s “keep me on track” primitive:\n\nIt reads the workset’s plan.md (a checklist derived from the item’s Acceptance Criteria).\nIt returns the first unchecked checkbox item (- [ ] ...).\nIf everything is checked, it prints a completion message.\n\nIt does not automatically mark anything as done; you check items off in plan.md as you complete them, then run kano-backlog workset next again to get the next step. Use --format json if you want structured output for tooling/agent automation.\nRefresh from canonical files:\nkano-backlog workset refresh --item TASK-0042 --agent my-agent\nPromote deliverables to canonical artifacts:\nkano-backlog workset promote --item TASK-0042 --agent my-agent\n# Dry run to preview\nkano-backlog workset promote --item TASK-0042 --agent my-agent --dry-run\nDetect ADR candidates in notes:\nkano-backlog workset detect-adr --item TASK-0042\nList all worksets:\nkano-backlog workset list\nCleanup expired worksets:\nkano-backlog workset cleanup --ttl-hours 72\n# Dry run to preview\nkano-backlog workset cleanup --ttl-hours 72 --dry-run\nAgent Workflow with Worksets\nAsk your agent:\n&quot;Initialize a workset for TASK-0042 and show me the next action to take.&quot;\n\nThe agent will:\n\nCreate a workset directory with plan, notes, and deliverables\nParse acceptance criteria into a checklist in plan.md\nTrack progress through the checklist\nCapture decisions with Decision: markers in notes.md\nStage work artifacts in deliverables/\nPromote deliverables back to canonical artifacts when done\n\nExample conversation:\nUser: &quot;Start working on TASK-0042&quot;\r\nAgent: &quot;I&#039;ll initialize a workset and begin. The first step is...&quot;\r\n\r\nUser: &quot;What&#039;s next?&quot;\r\nAgent: &quot;According to the plan, the next unchecked step is...&quot;\r\n\r\nUser: &quot;I&#039;ve completed the implementation&quot;\r\nAgent: &quot;Great! I&#039;ll promote the deliverables to the canonical artifacts directory.&quot;\n\nTopics: Context Switching and Grouping\nTopics provide a higher-level grouping mechanism for related items and documents, enabling rapid context switching when focus areas change during a conversation.\nCurrent implementation (see _kano/backlog/products/kano-agent-backlog-skill/items/task/0100/KABSD-TSK-0190_topic-lifecycle-materials-buffer-workset-merge.md) stores topics in _kano/backlog/topics/&lt;topic&gt;/ so the deterministic brief.generated.md can be shared/reviewed in-repo, while keeping per-agent active-topic tracking in shared cache state.\n\nmanifest.json: Topic metadata, seed items, pinned documents\nbrief.md: Stable, human-maintained brief (do not overwrite automatically)\nbrief.generated.md: Deterministic distilled brief (generated/overwritten by kano-backlog topic distill)\nsynthesis/: Working outputs for distillation (derived)\npublish/: Prepared write-backs / patch skeletons (derived)\nmaterials/: Raw collected materials (snippets, links, extracts, logs)\n\nclips/: Code snippet references with line ranges\nlinks/: External document references\nextracts/: Cached text extracts\nlogs/: Optional collected logs\n\n\n\nNotes:\n\nRaw materials are treated as cache: by default this repo ignores _kano/backlog/topics/**/materials/ via .gitignore.\nActive topic state is stored under _kano/backlog/.cache/worksets/state.json (and topic entries under _kano/backlog/.cache/worksets/topics/).\n\nTopic Commands\nCreate a topic:\nkano-backlog topic create auth-refactor --agent my-agent\nAdd items to topic:\nkano-backlog topic add auth-refactor --item TASK-0042\nkano-backlog topic add auth-refactor --item TASK-0043\nPin documents for context:\nkano-backlog topic pin auth-refactor --doc _kano/backlog/decisions/ADR-0005-auth-strategy.md\nAdd code snippets to materials:\nkano-backlog topic add-snippet auth-refactor --file src/auth.py --start 10 --end 25\nDistill materials into brief:\nkano-backlog topic distill auth-refactor\nAudit decision write-back (writes a report into topic publish/):\nkano-backlog topic decision-audit auth-refactor\nkano-backlog topic decision-audit auth-refactor --format json\nWrite back a decision to a work item:\nkano-backlog workitem add-decision KABSD-TSK-0001 \\\n  --decision &quot;Use X over Y because ...&quot; \\\n  --source &quot;_kano/backlog/topics/auth-refactor/synthesis/decision-notes.md&quot; \\\n  --agent my-agent \\\n  --product kano-agent-backlog-skill\nSwitch active topic:\nkano-backlog topic switch auth-refactor --agent my-agent\nExport context bundle:\nkano-backlog topic export-context auth-refactor --format markdown\nkano-backlog topic export-context auth-refactor --format json\nList all topics:\nkano-backlog topic list --agent my-agent\nClose a topic (enables TTL cleanup):\nkano-backlog topic close auth-refactor --agent my-agent\nCleanup closed topics:\nkano-backlog topic cleanup --ttl-days 14 --apply\n# Dry run to preview\nkano-backlog topic cleanup --ttl-days 14\nAgent Workflow with Topics\nAsk your agent:\n&quot;Create a topic for the authentication refactor work and add the related tasks.&quot;\n\nThe agent will:\n\nCreate a topic directory with manifest and materials structure\nAdd related backlog items to the topic’s seed_items\nPin relevant ADRs and design documents\nCollect code snippets into materials/clips/\nGenerate a distilled brief from collected materials\nSwitch to the topic when you change focus\nExport context bundles for loading into working memory\n\nExample conversation:\nUser: &quot;I&#039;m switching to work on the payment flow&quot;\r\nAgent: &quot;I&#039;ll create a topic and gather the relevant context...&quot;\r\nAgent: &quot;Topic &#039;payment-flow&#039; created with 3 tasks and 2 ADRs. Here&#039;s the brief...&quot;\r\n\r\nUser: &quot;Show me the authentication work&quot;\r\nAgent: &quot;Switching to topic &#039;auth-refactor&#039;... Here are the 5 tasks and relevant code snippets...&quot;\n\nTopic Lifecycle\n\nCreate: Initialize topic with manifest\nCollect: Add items, pin documents, collect code snippets\nDistill: Generate deterministic brief from materials\nSwitch: Change active topic for context switching\nExport: Generate context bundles for agent consumption\nClose: Mark topic as closed when work is complete\nCleanup: Remove raw materials after TTL (brief.md persists)\n\nWorksets vs Topics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeatureWorksetsTopicsScopeSingle itemMultiple items + documentsPurposeExecution cacheContext groupingLifetimeTask durationWork area durationContentPlan, notes, deliverablesItems, docs, snippets, briefUse CasePrevent drift during taskSwitch focus areasStorage.cache/worksets/items/topics/&lt;topic&gt;/ (materials ignored; active topic tracked in _kano/backlog/.cache/worksets/state.json)\nConfiguration\nBacklog configuration is in _kano/backlog/products/&lt;product&gt;/_config/config.toml (product-specific) and _kano/backlog/_shared/defaults.toml (shared defaults).\nContributing (Pre-Alpha)\nCurrent Status: This is version 0.0.2 - an experimental demo repository in rapid development.\nBefore Contributing:\n\nExpect frequent breaking changes\nCheck recent commits for current state\nUnderstand this is experimental software\nNo stability guarantees\n\nHow to Contribute:\n\nOpen issues for bugs/suggestions\nDiscuss major changes before implementing\nExpect your contributions may be refactored heavily\nFocus on core concepts rather than implementation details\n\nFor the main skill development, see kano-agent-backlog-skill (also experimental).\nRoadmap\nVersion 0.0.2 (Current):\n\n✅ Core backlog management\n✅ Workset and topic features\n✅ Multi-agent collaboration patterns\n✅ CLI commands\n✅ Property-based testing\n\nFuture Versions:\n\n🔮 Stable file format schema\n🔮 Enhanced SQLite indexing\n🔮 Production-ready embedding search\n🔮 Web UI for backlog visualization\n🔮 Git integration for version control\n🔮 Export/import for backlog migration\n🔮 Plugin system for custom workflows\n🔮 Performance optimizations\n\nNote: Roadmap is subject to change based on experimentation and feedback.\nLicense\nMIT\nSee the individual skill repositories for license information.\nLearn More\n\nSkill Documentation: skills/kano-agent-backlog-skill/SKILL.md (after initializing submodules)\nAgent Guidelines: AGENTS.md\nExample Backlogs: Explore _kano/backlog/products/ for real-world examples\nADR Directory: _kano/backlog/decisions/ for architectural decisions\n\nTroubleshooting\nBacklog not found:\n\nEnsure you’re in the repository root\nCheck _kano/backlog/ directory exists\nInitialize a product: kano-backlog admin init --product my-product --agent my-agent\n\nAgent asks for help:\n\nPoint agent to AGENTS.md for guidelines\nReference skills/kano-agent-backlog-skill/SKILL.md for detailed instructions\nShow agent the example backlogs in _kano/backlog/products/\n\nFrequently Asked Questions\nQ: Is this production-ready?\r\nA: No. This is version 0.0.2 - experimental software. Use at your own risk.\nQ: Can I use this for my project?\r\nA: Yes, but expect breaking changes. Copy the skill directory or use as a git submodule.\nQ: How do I migrate my existing backlog?\r\nA: Migration tools are not yet available. Manual conversion required.\nQ: Does this work with [my favorite agent]?\r\nA: It should work with any agent that has file system access and can run CLI commands. Tested with Amazon Kiro, Claude, Cursor, Windsurf, and others.\nQ: Why file-based instead of a database?\r\nA: Files are human-readable, version-controllable, and don’t require a server. Optional SQLite indexes provide query performance without lock-in.\nQ: Can multiple agents work on the same backlog simultaneously?\r\nA: Yes, but coordination is manual. File conflicts may occur. Use git for version control and conflict resolution.\nPhilosophy\nThis experimental demo explores a “backlog-first” approach where:\n\nPlanning before coding: Work items are created and refined before implementation begins\nDurable context: Planning and decisions persist in files, not lost in chat conversations\nAuditable trail: Every change is logged with timestamps and agent identity\nRecoverable state: Any agent can pick up where another left off\nHuman-readable source of truth: Markdown files are the canonical source, not databases\nOptional indexes: SQLite and embedding indexes enable powerful queries without lock-in\nMulti-agent coordination: Shared backlog enables multiple agents to collaborate effectively\nContext isolation: Worksets prevent drift during complex tasks\nRapid context switching: Topics enable quick focus area changes\n\nVersion 0.0.2 Status: These principles are being tested and refined. Implementation is evolving but demonstrates the core concepts in action. The _kano/backlog/ directory contains real-world examples of this philosophy applied to the development of the system itself.\nDual-Readability Design (Topic &amp; Snapshot)\nA key challenge in modern development is that Humans struggle with focus (hard to jump between contexts) while Agents struggle with collaboration (hard to share implicit context).\nThis project checks every artifact against a Dual-Readability Principle:\n\nHuman-Readable: High-level summaries, clear checklists, and “manager-friendly” reports (e.g., brief.md, Report_pm.md) so humans can make rapid decisions without reading code.\nAgent-Readable: Structural precision, file paths, line numbers, and explicit “Next Step” markers (e.g., manifest.json, stub_inventory) so agents can act without hallucinating.\n\nTopics and Snapshots are the concrete implementation of this philosophy:\n\nTopics: Allow humans to “load” a focus area mentally in seconds, while giving Agents a precise list of files and snippets to load into their context window.\nSnapshots: Give humans a trustable “State of the Union”, while giving Agents a literal checklist of TODOs and NotImplementedErrors to attack next.\n\nThe Agent Semantic Memory Model\nTo help Agents (and humans) navigate the complexity of software development, this project maps its features to the standard cognitive memory model:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMemory TypeEquivalentPurposeLifespanAnalogyShort-Term(Working Memory)WorksetExecution StatePrevents drift during a single task. Contains the immediate plan, scratchpad notes, and temp files.Hours(Task duration)The “scratchpad” or “RAM” cleared after use.Medium-Term(Contextual)TopicFocus ScopeGroups related files, specs, and definitions for a feature. Allows rapid context switching.Days/Weeks(Feature duration)The “project folder” on your desk.Long-Term(Semantic)ADR &amp; CanonicalInstitutional KnowledgeImmutable decisions (ADRs) and the source code itself. The “Soul” of the project.Permanent(Project duration)The “library archives” or “textbooks”.\nWhy this matters:\n\nAgents often “forget” instructions (Short-term failure). → Solution: Worksets force them to read a plan.md.\nAgents often “hallucinate” unrelated files (Medium-term failure). → Solution: Topics bind them to a specific manifest.json.\nAgents often “overwrite” architectural rules (Long-term failure). → Solution: ADRs provide immutable constraints.\n\nBuilt with Multi-Agent Collaboration\nThis project is itself a demonstration of multi-agent collaboration in action. The codebase has been developed through iterative collaboration with multiple AI coding assistants, each bringing unique strengths to the development process:\n\nCodex - Early prototyping and concept exploration\nGitHub Copilot - Code completion, suggestions, and inline assistance\nGoogle Antigravity - Alternative perspectives and design discussions\nAmazon Q - AWS infrastructure guidance and cloud architecture\nAmazon Kiro - Primary development agent and implementation\nCursor - Inline editing, refactoring, and code navigation\nWindsurf - Additional development support and testing\nOpenCode - Testing, validation, and quality assurance\n\nHow Multi-Agent Collaboration Works\nThe backlog system you see here was used to coordinate work across these agents, proving the concept through real-world usage:\n\nShared Backlog: All agents work from the same file-based backlog in _kano/backlog/\nWorklog Trail: Every decision and state change is recorded in append-only worklogs\nADR Documentation: Architectural decisions are captured in _kano/backlog/decisions/\nContext Switching: Topics enable agents to quickly load relevant context for different work areas\nWorkset Isolation: Per-item worksets prevent agents from drifting during complex tasks\n\nAgent Collaboration Patterns\nSequential Handoff:\nAgent A: Creates Epic and Feature items\r\nAgent B: Breaks down into User Stories\r\nAgent C: Implements Tasks with worksets\r\nAgent D: Reviews and updates worklogs\n\nParallel Work:\nAgent A: Works on TASK-0042 (auth module)\r\nAgent B: Works on TASK-0043 (payment module)\r\nBoth: Update shared backlog independently\r\nBoth: Coordinate through ADRs and worklogs\n\nContext Switching:\nAgent: Switches from &quot;auth-refactor&quot; topic to &quot;payment-flow&quot; topic\r\nAgent: Loads new context bundle with relevant items and documents\r\nAgent: Continues work with full context of new focus area\n\nWhy This Matters\nTraditional agent collaboration loses context in chat conversations. This system demonstrates:\n\nDurable Context: Planning and decisions persist in files, not chat\nAuditable Trail: Every change is logged with timestamps and agent identity\nRecoverable State: Any agent can pick up where another left off\nScalable Coordination: Multiple agents can work on different areas simultaneously\nHuman Oversight: Humans can review, approve, or redirect at any checkpoint\n\nEvery feature, decision, and trade-off in this codebase is documented in the _kano/backlog/ directory - a living example of the “backlog-first” philosophy in action."},"references/bases":{"title":"bases","links":[],"tags":[],"content":"Obsidian Bases (plugin-free views)\nThis skill currently ships Dataview examples and a plugin-free view generator. If you want to reduce plugin dependencies, Obsidian Bases can replace many Dataview table-style dashboards.\nNote: Bases is an Obsidian core feature, but it is newer than Dataview and may have limitations depending on your Obsidian version and platform.\nWhat Bases can cover well\n\nTable-style lists of items from a folder (e.g. _kano/backlog/items)\nFiltering by simple frontmatter fields (e.g. type, state, priority, area, iteration)\nSorting and grouping (where supported by your Bases version)\n\nWhat might still require alternatives\n\nMore complex computed fields\nParent/child tree rendering from parent links (Dataview is still better here)\nCustom JS logic (DataviewJS-only)\n\nFor tree/MOC, keep Epic .index.md files and Obsidian wikilinks in the body.\nRecommended frontmatter conventions for Bases\nTo keep Bases filtering predictable, prefer:\n\nScalars: id, type, title, state, priority, parent, area, iteration, created, updated\nArrays: tags, decisions\n\nAvoid relying on nested objects for filtering. Keep nested objects only for external references if needed.\nCreate a Base for “InProgress Work”\nIn Obsidian:\n\nCreate a new Base\nSet the source folder to _kano/backlog/items\nAdd columns: id, type, title, state, priority, parent, area, iteration, updated\nAdd a filter:\n\nstate is one of: Proposed, Planned, Ready, InProgress, Review, Blocked\n(Optionally hide Done and Dropped)\n\n\nSave the Base in your vault\n\nKeep a zero-plugin fallback\nEven if you adopt Bases, keep these generator commands as a no-plugin fallback for sharing/CI artifacts:\n\npython skills/kano-agent-backlog-skill/scripts/backlog/view_generate.py --groups &quot;New,InProgress&quot; --title &quot;InProgress Work&quot; --output _kano/backlog/views/Dashboard_PlainMarkdown_Active.md\npython skills/kano-agent-backlog-skill/scripts/backlog/view_generate.py --groups &quot;New&quot; --title &quot;New Work&quot; --output _kano/backlog/views/Dashboard_PlainMarkdown_New.md\npython skills/kano-agent-backlog-skill/scripts/backlog/view_generate.py --groups &quot;Done&quot; --title &quot;Done Work&quot; --output _kano/backlog/views/Dashboard_PlainMarkdown_Done.md\n"},"references/context_graph":{"title":"context_graph","links":[],"tags":[],"content":"Context Graph (Derived) and Graph-assisted retrieval\nThis project is file-first: Markdown work items and ADRs are the canonical source of truth.\nA Context Graph is a derived representation of how those artifacts relate, designed to help agents retrieve and assemble the right context without prompt bloat.\nWhat we mean by “Context Graph”\nA Context Graph is a directed, typed graph:\n\nNodes: artifacts (WorkItem, ADR, Worklog entry, chunks, code files, commits)\nEdges: explicit relationships (parent, decision_ref, relates, blocks, blocked_by, mentions, etc.)\n\nThis is a weak graph first approach:\n\nv1 uses only explicit, structured relationships already present in frontmatter or known file conventions\nno LLM-based entity extraction\nno server/MCP requirement (local-first)\n\nWhy it matters (vs vector-only RAG)\nVector/FTS retrieval finds “similar” text, but can miss:\n\nthe parent chain (task → story → feature → epic)\nthe ADR that explains the decision\nthe blockers/depends chain\n\nGraph-assisted retrieval uses vector/FTS to find seed nodes, then expands along known edges to pull in the load-bearing neighbors.\nMinimal Graph-assisted retrieval flow\n\nSeed retrieval\n\nFTS/embeddings return top-N chunks/nodes (seed set)\n\n\nGraph expansion\n\nexpand via allowlisted edge types\nlimit traversal depth (k-hop) and fanout\n\n\nRe-rank\n\nprioritize ADR decision sections and item title/acceptance\ndownweight noisy worklog-only matches\n\n\nContext packing\n\nassemble “seed + neighbors” into a compact, traceable context pack\n\n\n\nSuggested node / edge model (v1)\nNode types (initial):\n\nwork_item\nadr\nchunk (optional, for embedding/fts indexing)\n\nEdge types (initial):\n\nparent (child → parent)\ndecision_ref (work_item → adr)\nrelates (work_item → work_item)\nblocks / blocked_by\n\nStorage (derived)\nTwo equivalent ways to store the derived graph:\n\nReuse the SQLite index: materialize edges into a links-like table and query it for traversal\nSidecar JSONL: graph_nodes.jsonl + graph_edges.jsonl under &lt;backlog-root&gt;/_index/\n\nEither way:\n\ngraph artifacts must be safe to delete\nthe build must be repeatable from canonical Markdown (or from the SQLite index derived from Markdown)\n\nConfiguration knobs\nRecommended config keys (names indicative; see product ADR for final schema):\n\nretrieval.mode: file_scan | sqlite | hybrid\nretrieval.graph.enabled: boolean\nretrieval.graph.k_hop: int\nretrieval.graph.edge_allowlist: list\nretrieval.weights: doctype/section/state weights\n\nReferences\n\nProduct ADR (planned): Graph-assisted retrieval with a derived Context Graph\nreferences/indexing.md for the derived indexing layer\n"},"references/embedding_pipeline":{"title":"embedding_pipeline","links":[],"tags":[],"content":"Embedding Pipeline Configuration\nThe embedding pipeline enables semantic search and similarity analysis across backlog items through vector embeddings. This document describes the TOML configuration schema for the pipeline components.\nConfiguration Structure\nThe embedding pipeline configuration consists of four main sections:\n\n[chunking] - Text segmentation settings\n[tokenizer] - Token counting and budget management\n[embedding] - Vector embedding generation\n[vector] - Vector storage and retrieval\n\n[chunking] Section\nControls how documents are split into chunks for processing.\n[chunking]\ntarget_tokens = 256      # Target tokens per chunk\nmax_tokens = 512         # Maximum tokens per chunk (hard limit)\noverlap_tokens = 32      # Token overlap between adjacent chunks\nversion = &quot;chunk-v1&quot;     # Chunking algorithm version\nParameters\n\ntarget_tokens (integer, default: 256): Preferred number of tokens per chunk. The chunker aims for this size but may create smaller chunks at natural boundaries.\nmax_tokens (integer, default: 512): Hard limit on chunk size. Chunks exceeding this limit will be truncated.\noverlap_tokens (integer, default: 32): Number of tokens to overlap between consecutive chunks to preserve context across boundaries.\nversion (string, default: “chunk-v1”): Algorithm version identifier for reproducible chunking behavior.\n\nConstraints\n\noverlap_tokens must be less than target_tokens\ntarget_tokens must be less than or equal to max_tokens\nAll values must be positive integers\n\n[tokenizer] Section\nConfigures token counting for budget management and chunking.\n[tokenizer]\nadapter = &quot;heuristic&quot;              # Tokenizer adapter type\nmodel = &quot;text-embedding-3-small&quot;   # Model for token counting\nmax_tokens = 8192                  # Maximum tokens per model context (optional)\nParameters\n\nadapter (string, default: “heuristic”): Tokenizer implementation\n\n&quot;heuristic&quot;: Fast approximation based on character counts and language detection\n&quot;tiktoken&quot;: Precise tokenization using OpenAI’s tiktoken library\n\n\nmodel (string, default: “text-embedding-3-small”): Model identifier for token counting rules\nmax_tokens (integer, optional): Override model’s default context limit\n\nAvailable Models\nCommon models and their default token limits:\n\ntext-embedding-3-small: 8,192 tokens\ntext-embedding-3-large: 8,192 tokens\ntext-embedding-ada-002: 8,192 tokens\n\n[embedding] Section\nConfigures vector embedding generation.\n[embedding]\nprovider = &quot;noop&quot;           # Embedding provider\nmodel = &quot;noop-embedding&quot;    # Embedding model\ndimension = 1536            # Vector dimension\nParameters\n\nprovider (string, default: “noop”): Embedding service provider\n\n&quot;noop&quot;: Testing provider that generates random vectors\n&quot;openai&quot;: OpenAI embedding API\n\n\nmodel (string): Model identifier for embedding generation\ndimension (integer, default: 1536): Vector dimension size\n\nProvider-Specific Models\nNoOp Provider (for testing):\n\nnoop-embedding: Generates consistent random vectors\n\nOpenAI Provider:\n\ntext-embedding-3-small: 1536 dimensions\ntext-embedding-3-large: 3072 dimensions\ntext-embedding-ada-002: 1536 dimensions\n\nAdditional Options\nProvider-specific options can be added under [embedding.options]:\n[embedding.options]\napi_key = &quot;sk-...&quot;          # OpenAI API key (for openai provider)\nbase_url = &quot;https://...&quot;    # Custom API endpoint\ntimeout = 30                # Request timeout in seconds\n[vector] Section\nConfigures vector storage and retrieval.\n[vector]\nbackend = &quot;sqlite&quot;      # Vector storage backend\npath = &quot;.cache/vector&quot;  # Storage path (relative to product root)\ncollection = &quot;backlog&quot;  # Collection/table name\nmetric = &quot;cosine&quot;       # Distance metric for similarity\nParameters\n\nbackend (string, default: “sqlite”): Vector storage implementation\n\n&quot;noop&quot;: In-memory storage for testing\n&quot;sqlite&quot;: SQLite-based persistent storage\n\n\npath (string, default: “.cache/vector”): Storage location\n\nRelative paths are resolved from the product root\nAbsolute paths are used as-is\n\n\ncollection (string, default: “backlog”): Collection or table name for organizing vectors\nmetric (string, default: “cosine”): Distance metric for similarity calculations\n\n&quot;cosine&quot;: Cosine similarity (recommended for most text embeddings)\n&quot;l2&quot;: Euclidean distance\n&quot;ip&quot;: Inner product\n\n\n\nBackend-Specific Options\nAdditional options can be configured under [vector.options]:\n[vector.options]\n# SQLite-specific options\ntimeout = 10.0              # Connection timeout\njournal_mode = &quot;WAL&quot;        # SQLite journal mode\nConfiguration Examples\nTesting/Development Profile\nMinimal configuration for local development and testing:\n[chunking]\ntarget_tokens = 256\nmax_tokens = 512\noverlap_tokens = 32\n \n[tokenizer]\nadapter = &quot;heuristic&quot;\nmodel = &quot;text-embedding-3-small&quot;\n \n[embedding]\nprovider = &quot;noop&quot;\nmodel = &quot;noop-embedding&quot;\ndimension = 1536\n \n[vector]\nbackend = &quot;sqlite&quot;\npath = &quot;.cache/vector&quot;\ncollection = &quot;backlog&quot;\nmetric = &quot;cosine&quot;\nProduction Profile with OpenAI\nConfiguration for production use with OpenAI embeddings:\n[chunking]\ntarget_tokens = 512\nmax_tokens = 1024\noverlap_tokens = 64\n \n[tokenizer]\nadapter = &quot;tiktoken&quot;\nmodel = &quot;text-embedding-3-small&quot;\nmax_tokens = 8192\n \n[embedding]\nprovider = &quot;openai&quot;\nmodel = &quot;text-embedding-3-small&quot;\ndimension = 1536\n \n[embedding.options]\napi_key = &quot;${OPENAI_API_KEY}&quot;\ntimeout = 30\n \n[vector]\nbackend = &quot;sqlite&quot;\npath = &quot;.cache/vector&quot;\ncollection = &quot;backlog&quot;\nmetric = &quot;cosine&quot;\nHigh-Capacity Configuration\nFor large documents and detailed analysis:\n[chunking]\ntarget_tokens = 1024\nmax_tokens = 2048\noverlap_tokens = 128\n \n[tokenizer]\nadapter = &quot;tiktoken&quot;\nmodel = &quot;text-embedding-3-large&quot;\nmax_tokens = 8192\n \n[embedding]\nprovider = &quot;openai&quot;\nmodel = &quot;text-embedding-3-large&quot;\ndimension = 3072\n \n[vector]\nbackend = &quot;sqlite&quot;\npath = &quot;.cache/vector&quot;\ncollection = &quot;backlog&quot;\nmetric = &quot;cosine&quot;\nConfiguration Loading\nThe pipeline configuration is loaded through the standard config system:\n\nBase configuration: Default values from PipelineConfig class\nProduct config: Values from _kano/backlog/products/{product}/_config/config.toml\nEnvironment overrides: Environment variables (if supported)\n\nConfig File Location\nPlace embedding configuration in your product’s config file:\n_kano/backlog/products/{product}/_config/config.toml\n\nValidation\nThe configuration is validated when loaded:\n\nRequired fields must be present\nNumeric values must be within valid ranges\nProvider and model combinations must be supported\nTokenizer and embedding models must be compatible\n\nDebugging Configuration\nUse the CLI to inspect the effective configuration:\nkano-backlog config show --product {product}\nCLI Integration\nThe embedding pipeline integrates with the CLI through these commands:\n# Build index for entire product\nkano-backlog embedding build --product {product}\n \n# Index specific file\nkano-backlog embedding build /path/to/file.md --product {product}\n \n# Index raw text\nkano-backlog embedding build --text &quot;content&quot; --source-id &quot;doc-1&quot; --product {product}\n \n# Query the index\nkano-backlog embedding query &quot;search terms&quot; --product {product}\n \n# Check index status\nkano-backlog embedding status --product {product}\nPerformance Considerations\nChunking Strategy\n\nSmaller chunks (256-512 tokens): Better for precise matching, more chunks to process\nLarger chunks (1024+ tokens): Better for context preservation, fewer API calls\n\nToken Budgets\n\nSet max_tokens based on your embedding model’s context limit\nUse overlap_tokens to preserve context across chunk boundaries\nMonitor token usage to optimize costs with paid embedding providers\n\nVector Storage\n\nSQLite backend scales to millions of vectors for local-first use\nUse appropriate metric for your embedding model (cosine for most text embeddings)\nConsider storage location (path) for performance and backup requirements\n\nTroubleshooting\nCommon Issues\n\nConfiguration validation errors: Check parameter types and ranges\nModel compatibility: Ensure tokenizer and embedding models are compatible\nAPI authentication: Verify API keys and endpoints for external providers\nStorage permissions: Ensure write access to the configured vector path\nToken limits: Verify chunk sizes don’t exceed model context limits\n\nDebug Commands\n# Validate configuration\nkano-backlog config validate --product {product}\n \n# Test embedding pipeline\nkano-backlog embedding build --text &quot;test&quot; --source-id &quot;test&quot; --product {product}\n \n# Check vector backend status\nkano-backlog embedding status --product {product}"},"references/index":{"title":"Technical References","links":["references/bases","references/context_graph","references/embedding_pipeline","references/indexing","references/logging","references/processes","references/schema","references/templates","references/views","references/workflow","references/indexing_schema.json"],"tags":[],"content":"Technical References\nTechnical documentation and references\n\nREADME\nREADME\nREADME\nREADME\nREADME\nREADME\nREADME\nREADME\nREADME\nREADME\nREADME\n"},"references/indexing":{"title":"indexing","links":[],"tags":[],"content":"Indexing (Optional)\nThis skill is file-first: the source of truth is Markdown files under _kano/backlog/.\nIndexing is an optional, rebuildable layer that exists to make agents and humans faster at:\n\nfinding relevant items quickly (filters, sorting, parent/child traversal),\nproducing views/reports reproducibly,\npowering later retrieval workflows (e.g. embeddings/RAG).\n\nThe index must never become a write-path requirement for normal operation.\nWhat is indexed\nThe SQLite index is derived from:\n\nItems: _kano/backlog/items/**/*.md\nDecisions/ADRs: _kano/backlog/decisions/**/*.md (linked via item frontmatter decisions)\n\nSchema references:\n\nreferences/indexing_schema.sql\nreferences/indexing_schema.json\n\nArtifacts (where generated files live)\nGenerated artifacts should be treated as build outputs:\n\nDefault location: &lt;product-root&gt;/.cache/ (e.g. _kano/backlog/products/&lt;product&gt;/.cache/)\nTest/experiments: _kano/backlog_sandbox/\n\nThis demo repo gitignores index artifacts:\n\n_kano/backlog/products/*/.cache/\n_kano/backlog_sandbox/\n\nConfig keys (index.*)\nIndexing is disabled by default (file-first remains the default workflow).\nIn _kano/backlog/products/&lt;product&gt;/_config/config.toml:\n[index]\nenabled = false\nbackend = &quot;sqlite&quot;\npath = null\nmode = &quot;rebuild&quot;\n\nindex.enabled: feature flag (default false)\nindex.backend: sqlite (default) or postgres (optional/future)\nindex.path: DB file path override; null uses &lt;product-root&gt;/.cache/index.sqlite3\nindex.mode: rebuild or incremental (best-effort)\n\nBuild / rebuild workflow (SQLite)\nBuild the SQLite index from files:\n# Build index for specific product\nkano-backlog admin index build --product &lt;product-name&gt;\n \n# Build all product indexes\nkano-backlog admin index build\n \n# Force rebuild even if exists\nkano-backlog admin index build --product &lt;product-name&gt; --force\n \n# Build with vector index\nkano-backlog admin index build --product &lt;product-name&gt; --vectors\nRefresh (incremental update):\n# Refresh specific product\nkano-backlog admin index refresh --product &lt;product-name&gt;\n \n# Refresh all products\nkano-backlog admin index refresh\nCheck index status:\n# Status for specific product\nkano-backlog admin index status --product &lt;product-name&gt;\n \n# Status for all products\nkano-backlog admin index status\nSafety guarantees:\n\nThe indexer must not modify any source Markdown files.\nDeleting the DB is safe; it can always be rebuilt.\n\nVector Search Integration\nThe index system integrates with the embedding pipeline for semantic search:\n# Build vector index alongside SQLite index\nkano-backlog admin index build --product &lt;product-name&gt; --vectors\n \n# Search using vector similarity\nkano-backlog search query &quot;your search text&quot; --product &lt;product-name&gt;\n \n# Check embedding status\nkano-backlog embedding status --product &lt;product-name&gt;\nObsidian views (file-first mode)\nFile-first dashboards continue to work normally (Dataview or generated Markdown views), because items remain Markdown files.\nOptional (planned): generate Markdown dashboards from DB queries to reduce dependence on Obsidian plugins.\nIf you enable index.enabled=true, kano-backlog view refresh --source auto can use the SQLite index\r\nwhen present, and falls back to file scan when the DB is missing.\nContext graph (Graph-assisted retrieval)\nA context graph is a derived, structured view of how artifacts relate (items, ADRs, dependencies, refs).\r\nIt enables Graph-assisted retrieval: retrieve seed nodes via FTS/embeddings, then expand via graph edges (k-hop)\r\nto assemble a higher-quality context pack.\n\nConcept/spec: references/context_graph.md\n\nDB-first is out of scope\nUsing a database as the source of truth would require new UI/export tooling and changes to the human-in-the-loop workflow.\r\nThis skill intentionally keeps DB usage as an optional, derived index layer."},"references/logging":{"title":"logging","links":[],"tags":[],"content":"Audit Logging (Agent Tool Invocations)\nLog location (default)\n\nRoot: _kano/backlog/_shared/logs/agent_tools/ (cross-product shared)\nFile: tool_invocations.jsonl\n\nLog format (JSONL)\nEach line is a JSON object:\n{\r\n  &quot;version&quot;: 1,\r\n  &quot;timestamp&quot;: &quot;2026-01-04T10:50:12Z&quot;,\r\n  &quot;tool&quot;: &quot;shell_command&quot;,\r\n  &quot;cwd&quot;: &quot;D:/_work/_Kano/kano-agent-backlog-skill-demo&quot;,\r\n  &quot;status&quot;: &quot;ok&quot;,\r\n  &quot;exit_code&quot;: 0,\r\n  &quot;duration_ms&quot;: 123,\r\n  &quot;command_args&quot;: [&quot;python&quot;, &quot;script.py&quot;, &quot;--flag&quot;, &quot;***&quot;],\r\n  &quot;replay_command&quot;: &quot;python script.py --flag ***&quot;,\r\n  &quot;notes&quot;: &quot;optional&quot;\r\n}\n\nRequired fields\n\nversion\ntimestamp\ntool\ncwd\nstatus\ncommand_args\nreplay_command\n\nOptional fields\n\nexit_code\nduration_ms\nnotes\nerror\n\nRedaction rules\nRedact any value that is likely sensitive. Defaults include:\n\nFlags: --token, --api-key, --secret, --password, --passwd, --pwd,\r\n--client-secret, --access-key, --authorization, --bearer, --cookie\nKey-value pairs: token=..., api_key=..., secret=..., password=...\nEnv-style keys (case-insensitive): *_TOKEN, *_KEY, *_SECRET, *_PASSWORD\n\nRedaction replaces the value with *** while leaving the flag/key intact.\nRotation and retention (defaults)\n\nRotate when file size exceeds 5 MB.\nKeep the last 10 rotated files.\nFile naming: tool_invocations.jsonl, tool_invocations.1.jsonl, …\n\nThese values can be made configurable later, but the defaults must exist.\nScript integration\nBacklog and filesystem scripts call scripts/logging/audit_runner.py at\r\nentrypoint so every invocation appends an audit log entry.\nEnvironment overrides\nSet these environment variables to adjust logging without code changes:\n\nKANO_AUDIT_LOG_DISABLED=1 to disable audit logging.\nKANO_AUDIT_LOG_ROOT to override the log directory.\nKANO_AUDIT_LOG_FILE to override the log filename.\nKANO_AUDIT_LOG_MAX_BYTES to override rotation size (bytes).\nKANO_AUDIT_LOG_MAX_FILES to override the number of rotated files kept.\n\nConfig defaults\nLogging defaults are read from your product config (with environment overrides):\n\n_kano/backlog/products/&lt;product&gt;/_config/config.toml\n\n[log]\nverbosity = &quot;info&quot;\ndebug = false\nVerbosity behavior:\n\ndebug: log all tool invocations\ninfo: log successful + failed invocations\nwarn / warning: log only warnings/errors (currently: failures)\nerror: log only failures\noff / none / disabled: disable audit logging\n\nPrecedence: environment overrides &gt; config defaults."},"references/processes":{"title":"processes","links":[],"tags":[],"content":"Process Profiles\nProcess profiles define work item types, states, and transitions for the local\r\nbacklog. They are intended to be human-readable and easy to adjust for agent\r\nworkflows.\nSuggested schema (TOML)\n# Built-in Azure Boards Agile profile for kano-agent-backlog-skill.\nid = &quot;builtin/azure-boards-agile&quot;\nname = &quot;Azure Boards Agile&quot;\ndescription = &quot;Default Agile-like workflow for agent-managed backlog items.&quot;\ndefault_state = &quot;Proposed&quot;\nstates = [\n    &quot;Proposed&quot;,\n    &quot;Planned&quot;,\n    &quot;Ready&quot;,\n    &quot;InProgress&quot;,\n    &quot;Review&quot;,\n    &quot;Blocked&quot;,\n    &quot;Done&quot;,\n    &quot;Dropped&quot;\n]\nterminal_states = [&quot;Done&quot;, &quot;Dropped&quot;]\n \n[[work_item_types]]\ntype = &quot;Epic&quot;\nslug = &quot;epic&quot;\n \n[[work_item_types]]\ntype = &quot;Feature&quot;\nslug = &quot;feature&quot;\n \n[[work_item_types]]\ntype = &quot;UserStory&quot;\nslug = &quot;userstory&quot;\n \n[[work_item_types]]\ntype = &quot;Task&quot;\nslug = &quot;task&quot;\n \n[[work_item_types]]\ntype = &quot;Bug&quot;\nslug = &quot;bug&quot;\n \n[transitions]\nProposed = [&quot;Planned&quot;, &quot;Dropped&quot;]\nPlanned = [&quot;Ready&quot;, &quot;Dropped&quot;]\nReady = [&quot;InProgress&quot;, &quot;Dropped&quot;]\nInProgress = [&quot;Review&quot;, &quot;Blocked&quot;, &quot;Dropped&quot;]\nReview = [&quot;Done&quot;, &quot;InProgress&quot;]\nBlocked = [&quot;InProgress&quot;, &quot;Dropped&quot;]\nDone = []\nDropped = []\nNotes\n\nwork_item_types should align with item frontmatter type.\nstates should align with state values used in items.\nKeep transitions permissive for agent autonomy; tighten only if needed.\n\nState semantics (standard set)\nThese semantics apply to built-ins that use the default KABSD state set:\n\nProposed: not ready to start; needs more discovery/confirmation.\nPlanned: approved for the plan; detail refinement can proceed, but not started.\nReady: Ready gate passed (typically for Task/Bug before start).\nInProgress: work started.\nBlocked: work started but blocked.\nReview: work complete pending review/verification.\nDone: work complete and accepted.\nDropped: work intentionally stopped.\n\nConfig selection\nUse process.profile and process.path in your product config:\n\n_kano/backlog/products/&lt;product&gt;/_config/config.toml\n\nto choose a built-in profile or a custom file.\nUse kano-backlog config show --product &lt;product&gt; to confirm the effective merged config.\nBuilt-in profiles\n\nreferences/processes/azure-boards-agile.toml → builtin/azure-boards-agile\nreferences/processes/scrum.toml → builtin/scrum\nreferences/processes/cmmi.toml → builtin/cmmi\nreferences/processes/jira-default.toml → builtin/jira-default\n\nCustom profiles\nUse the template as a starting point and store it under your product config\r\narea (recommended: _kano/backlog/products/&lt;product&gt;/_config/processes/), then point\r\nprocess.path to that file.\nTemplate:\n\nreferences/processes/template.toml\n\nExample config:\n[process]\npath = &quot;_kano/backlog/products/&lt;product&gt;/_config/processes/custom.toml&quot;"},"references/schema":{"title":"schema","links":[],"tags":[],"content":"Backlog Schema\nProcess-defined types and states\nItem types and states come from the active process profile. See\r\nreferences/processes.md and the profile selected via\r\n_kano/backlog/_config/config.json (process.profile or process.path).\nWhen scripts or docs need workflow details, they should load the profile\r\nspecified in config (built-in or custom) instead of hardcoding the list.\nParent rules (default)\n\nEpic → Feature\nFeature → UserStory\nUserStory → Task or Bug\nFeature → Bug (allowed)\nTask → Task (optional sub-task)\nEpic has no parent\n\nThese defaults align with built-in profiles; custom processes may define\r\ndifferent parent relationships.\nParent state sync (forward-only)\nWhen a child item state changes, parents can auto-advance forward-only:\n\nNever downgrade parent state automatically.\nNever change child states based on parent edits.\nReady/Planned children advance parents to Planned (not Ready).\nAny InProgress/Review/Blocked child advances parent to InProgress.\nAll Done ⇒ parent Done; all Dropped ⇒ parent Dropped; mix Done/Dropped ⇒ parent Done.\n\nReady gate (required, non-empty)\nTo move to Ready, each item must include:\n\nContext\nGoal\nApproach\nAcceptance Criteria\nRisks / Dependencies\n\nFile naming\n\n&lt;ID&gt;_&lt;slug&gt;.md\nSlug: ASCII, hyphen-separated\nID prefixes:\n\nKABSD-EPIC-\nKABSD-FTR-\nKABSD-USR-\nKABSD-TSK-\nKABSD-BUG-\n\n\nPrefix derivation:\n\nSource: config/profile.env → PROJECT_NAME.\nSplit on non-alphanumeric separators and camel-case boundaries, take first letters.\nIf only one letter, use the first letter plus the next consonant (A/E/I/O/U skipped).\nIf still short, use the first two letters.\nUppercase the result (example: kano-agent-backlog-skill-demo → KABSD).\n\n\nStore files under _kano/backlog/items/&lt;type&gt;/&lt;bucket&gt;/ by item type.\nBucket names use the lower bound of each 100 range:\n\n0000, 0100, 0200, …\n\n\nFor Epic, create &lt;ID&gt;_&lt;slug&gt;.index.md in the same folder.\n\nFrontmatter (minimum)\n---\r\nid: KABSD-TSK-0001\r\ntype: Task\r\ntitle: &quot;Short title&quot;\r\nstate: Proposed\r\npriority: P2\r\nparent: KABSD-USR-0001\r\narea: general\r\niteration: null\r\ntags: []\r\ncreated: 2026-01-02\r\nupdated: 2026-01-02\r\nowner: null\r\nexternal:\r\n  azure_id: null\r\n  jira_key: null\r\nlinks:\r\n  relates: []\r\n  blocks: []\r\n  blocked_by: []\r\ndecisions: []\r\n---\n\nImmutable fields\n\nid, type, created must not be changed after creation.\n\nConfig defaults (baseline)\nBaseline config lives at _kano/backlog/_config/config.json and defaults to:\n{\r\n  &quot;log&quot;: { &quot;verbosity&quot;: &quot;info&quot;, &quot;debug&quot;: false },\r\n  &quot;process&quot;: { &quot;profile&quot;: &quot;builtin/azure-boards-agile&quot;, &quot;path&quot;: null },\r\n  &quot;sandbox&quot;: { &quot;root&quot;: &quot;_kano/backlog_sandbox&quot; },\r\n  &quot;index&quot;: { &quot;enabled&quot;: false, &quot;backend&quot;: &quot;sqlite&quot;, &quot;path&quot;: null, &quot;mode&quot;: &quot;rebuild&quot; }\r\n}\n\nConfig overrides (environment)\n\nKANO_BACKLOG_CONFIG_PATH: override config file path (must be under _kano/backlog or _kano/backlog_sandbox).\nAudit log env overrides (highest precedence over config defaults):\n\nKANO_AUDIT_LOG_DISABLED\nKANO_AUDIT_LOG_ROOT\nKANO_AUDIT_LOG_FILE\nKANO_AUDIT_LOG_MAX_BYTES\nKANO_AUDIT_LOG_MAX_FILES\n\n\n\nParent reference format (collision-safe)\n\nSame-product parent: use the display ID in parent (e.g., KABSD-FTR-0002).\nParent is intentionally intra-product: it powers hierarchy and parent state sync; cross-product “parent” relationships are usually not desired.\nCross-product relationships should be expressed via links.relates, links.blocks, links.blocked_by instead of parent.\n\nFor collision-safe references in links/content, use id@uidshort (e.g., KABSD-FTR-0002@019b8f52) or full uid (019b8f52-9fde-7162-bd19-e9b8310526fc).\n\n\n\nValidation\n\nThe workitem_update_state.py script validates that parent resolves uniquely within the current product.\nIf unresolved or ambiguous, the script aborts with guidance to keep parent intra-product and use links.* (with id@uidshort / full uid) for cross-product relationships.\n\nRationale\n\nDisplay IDs are human-friendly and stable for day-to-day work within a product.\nParent drives hierarchy and state propagation; limiting it to a product keeps behavior predictable.\nCross-product collisions are expected in a multi-product platform; disambiguated refs in links ensure correctness without sacrificing readability in the common case.\n"},"references/templates":{"title":"templates","links":[],"tags":[],"content":"Templates\nWork item template\nPlace the file under _kano/backlog/items/&lt;type&gt;/.\r\nUse the ID prefix derived from config/profile.env → PROJECT_NAME (example: kano-agent-backlog-skill-demo → KABSD).\n---\r\nid: KABSD-TSK-0001\r\ntype: Task\r\ntitle: &quot;Short title&quot;\r\nstate: Proposed\r\npriority: P2\r\nparent: KABSD-USR-0001\r\narea: general\r\niteration: null\r\ntags: []\r\ncreated: 2026-01-02\r\nupdated: 2026-01-02\r\nowner: null\r\nexternal:\r\n  azure_id: null\r\n  jira_key: null\r\nlinks:\r\n  relates: []\r\n  blocks: []\r\n  blocked_by: []\r\ndecisions: []\r\n---\r\n\r\n# Context\r\n\r\n# Goal\r\n\r\n# Non-Goals\r\n\r\n# Approach\r\n\r\n# Alternatives\r\n\r\n# Acceptance Criteria\r\n\r\n# Risks / Dependencies\r\n\r\n# Worklog\r\n\r\n2026-01-02 10:00 [agent=&lt;AGENT_NAME&gt;] Created from discussion: &lt;summary&gt;.\n\nImportant: Replace &lt;AGENT_NAME&gt; with your actual agent identity. See SKILL.md “Agent Identity Determination” for how to determine your identity. NEVER use example values like codex, antigravity, or auto.\nWorklog line format\nYYYY-MM-DD HH:MM [agent=&lt;AGENT_NAME&gt;] &lt;message&gt;\r\nYYYY-MM-DD HH:MM [agent=&lt;AGENT_NAME&gt;] [model=&lt;MODEL_NAME&gt;] &lt;message&gt;\n\nAgent Identity: Provide the actual runtime agent identity explicitly in Worklog entries; do not copy placeholders or examples. See SKILL.md for details.\nModel (Optional): When available, include the model used by the agent (e.g., claude-sonnet-4.5, gpt-5.1, gemini-3.0-high). This provides additional context for audit trails and debugging.\nADR template\n---\r\nid: ADR-0001\r\ntitle: &quot;Decision title&quot;\r\nstatus: Proposed\r\ndate: 2026-01-02\r\nrelated_items: []\r\nsupersedes: null\r\nsuperseded_by: null\r\n---\r\n\r\n# Decision\r\n\r\n# Context\r\n\r\n# Options Considered\r\n\r\n# Pros / Cons\r\n\r\n# Consequences\r\n\r\n# Follow-ups\n"},"references/views":{"title":"views","links":[],"tags":[],"content":"Views (Obsidian Dataview)\nIf you want fewer plugin dependencies, consider Obsidian Bases as a plugin-free alternative for table-style dashboards. See references/bases.md.\nTree by parent\nUse Dataview or DataviewJS to build parent/child views from parent fields.\r\nAvoid encoding hierarchy in folders.\r\nQueries should target _kano/backlog/items to include all item subfolders.\r\nHide Done/Dropped items in views by default (view-level archive).\nExample: List Epic items\ntable id, state, priority, iteration\nfrom &quot;_kano/backlog/items&quot;\nwhere type = &quot;Epic&quot; and state != &quot;Done&quot; and state != &quot;Dropped&quot;\nsort created asc\nExample: Items by iteration\ntable id, type, state, priority\nfrom &quot;_kano/backlog/items&quot;\nwhere iteration != null and state != &quot;Done&quot; and state != &quot;Dropped&quot;\nsort iteration asc, priority asc\nInProgress view (no Dataview required)\nGenerate plain Markdown lists (no Dataview required).\nscripts/backlog/view_generate.py supports two data sources:\n\nFile scan (default file-first behavior)\nSQLite index (when index.enabled=true and the DB exists)\n\nWhen --source auto is used, it prefers SQLite when available, otherwise falls back to scanning files.\npython scripts/backlog/view_generate.py --source auto --groups &quot;New,InProgress&quot; --title &quot;InProgress Work&quot; --output _kano/backlog/views/Dashboard_PlainMarkdown_Active.md\npython scripts/backlog/view_generate.py --source auto --groups &quot;New&quot; --title &quot;New Work&quot; --output _kano/backlog/views/Dashboard_PlainMarkdown_New.md\npython scripts/backlog/view_generate.py --source auto --groups &quot;Done&quot; --title &quot;Done Work&quot; --output _kano/backlog/views/Dashboard_PlainMarkdown_Done.md\nOr refresh all standard dashboards (and optionally refresh the SQLite index first):\npython scripts/backlog/view_refresh_dashboards.py --backlog-root _kano/backlog --agent &lt;agent-name&gt;\nOutputs:\n\n_kano/backlog/views/Dashboard_PlainMarkdown_Active.md (New + InProgress/Review/Blocked)\n_kano/backlog/views/Dashboard_PlainMarkdown_New.md (New only)\n_kano/backlog/views/Dashboard_PlainMarkdown_Done.md (Done + Dropped)\n\nDemo: DBIndex vs NoDBIndex\nIf you want to show the difference between:\n\nNoDBIndex: file scan only\nDBIndex: query SQLite index (optional, derived)\n\nGenerate both variants by running the same view generator with different --source values:\npython scripts/backlog/view_generate.py --source files --groups &quot;New,InProgress&quot; --title &quot;InProgress Work (Demo: NoDBIndex)&quot; --output _kano/backlog/views/_demo/Dashboard_Demo_NoDBIndex_Active.md\npython scripts/backlog/view_generate.py --source sqlite --groups &quot;New,InProgress&quot; --title &quot;InProgress Work (Demo: DBIndex)&quot; --output _kano/backlog/views/_demo/Dashboard_Demo_DBIndex_Active.md\nOr use the dedicated generator (preferred):\r\npython scripts/backlog/view_generate_demo.py --backlog-root _kano/backlog --agent &lt;agent-name&gt;\nIf you only want a tag view:\npython scripts/backlog/view_generate_tag.py --backlog-root _kano/backlog --source auto --tags \\&quot;versioning,release\\&quot; --output _kano/backlog/views/_demo/Dashboard_Demo_Tags_Versioning.md --agent &lt;agent-name&gt;"},"references/workflow":{"title":"workflow","links":[],"tags":[],"content":"Workflow SOP\nA) Planning (discussion → tickets)\n\nCreate or update Epic for the milestone.\nSplit into Features (capabilities).\nSplit into UserStories (user perspective).\nSplit into Tasks/Bugs (single focused coding sessions).\nFill Ready gate sections for each Task/Bug.\nAppend Worklog entry: “Created from discussion: …” (scripts require --agent).\n\nB) Ready gate\n\nMove to Ready only after required sections are complete.\nNo code changes until the item is Ready.\n\nC) Execution\n\nSet state to InProgress.\nAppend Worklog for important decisions or changes.\nIf a decision is architectural, create ADR and link it:\n\nAdd ADR id to item decisions: []\nAppend Worklog entry referencing the ADR\n\n\n\nConflict Guard\n\nOwner Locking: Items in InProgress are locked to their owner.\nAuto-Assignment: When moving to InProgress, if no owner is set, you become the owner.\nCollaboration: To hand off work, the current owner must change the owner field or move the item out of InProgress (e.g. to Review or Planned).\n\nD) Completion\n\nMove state to Review → Done.\nAppend a Worklog summary with:\n\nWhat changed\nRelated items and ADRs\n\n\n\nD.1) Parent sync (forward-only)\n\nWhen a child state changes, parents can be auto-advanced forward-only.\nParent edits never force child states.\nUse --no-sync-parent if you need to keep parent state unchanged for a manual re-plan.\n\nE) Scope change\n\nDo not rewrite a ticket into a different task.\nSplit into a new ticket and link via links.relates.\nAppend a Worklog entry explaining the split.\n\nF) File operations\n\nUse scripts/backlog/* or scripts/fs/* for backlog/skill artifacts.\nScripts only operate under _kano/backlog/ or _kano/backlog_sandbox/ to keep audit logs clean.\n"},"skill/changelog":{"title":"changelog","links":[],"tags":[],"content":"Changelog\nAll notable changes to kano-agent-backlog-skill will be documented in this file.\nThis project uses Git tags as releases: vX.Y.Z.\n[Unreleased]\n[0.0.2] - 2026-01-19\nAdded\n\nTopic templates/archetypes with variable substitution and CLI integration.\nTopic cross-references (related_topics) with bidirectional linking.\nTopic snapshots (create/list/restore/cleanup) for checkpointing.\nTopic merge/split operations with dry-run support and history preservation.\n\nChanged\n\nTopic distillation renders human-readable seed item listings (ID/title/type/state) while keeping UID mapping in HTML comments.\nArtifact attachment resolves items in product layout (_kano/backlog/products/&lt;product&gt;/items/...) when --backlog-root-override is used with --product.\n\nDocumentation\n\nRelease notes for GitHub Releases: skills/kano-agent-backlog-skill/docs/releases/0.0.2.md.\n\n[0.0.1] - 2026-01-15\nAdded\n\nOptional SQLite index layer (rebuildable) to accelerate reads and view generation.\nDBIndex vs NoDBIndex demo dashboards under _kano/backlog/views/_demo/.\nDemo tool for recent/iteration focus views (_kano/backlog/tools/generate_focus_view.py).\nFirst-run bootstrap (scripts/backlog/bootstrap_init_project.py) + templates to enable the backlog system in a repo.\nviews.auto_refresh config flag (default: true) to keep dashboards up to date automatically.\n\nDocumentation\n\nRelease notes for GitHub Releases: skills/kano-agent-backlog-skill/docs/releases/0.0.1.md.\n\nChanged\n\nUnified generated dashboards to prefer SQLite when enabled/available and fall back to file scan.\nKept scripts/backlog/view_generate_demo.py self-contained; demo repo tool is a thin wrapper.\nMutating scripts auto-refresh dashboards by default; scripts/fs/* now also require --agent for auditability.\n\nFixed\n\nquery_sqlite_index.py --sql validation (SELECT/WITH detection).\n\nAdded\n\nLocal-first backlog structure under _kano/backlog/ (items, decisions/ADRs, views).\nWork item scripts: create items, validate Ready gate, update state with append-only Worklog.\nAudit logging for tool invocations with redaction and rotation.\nPlain Markdown dashboards + Obsidian Dataview/Bases demo views.\nConfig system under _kano/backlog/_config/config.json.\n\nChanged\n\nEnforced explicit --agent for Worklog-writing scripts and auditability.\n\nSecurity\n\nSecret redaction and log rotation defaults for audit logs.\n"},"skill/index":{"title":"index","links":[],"tags":[],"content":"Kano Agent Backlog Skill (local-first)\nScope\nUse this skill to:\n\nPlan new work by creating backlog items before code changes.\nMaintain hierarchy and relationships via parent links, as defined by the active process profile.\nRecord decisions with ADRs and link them to items.\nKeep a durable, append-only worklog for project evolution.\n\nAgent compatibility: read the whole skill\n\nAlways load the entire SKILL.md before acting; some agent shells only fetch the first ~100 lines by default.\nIf your client truncates, fetch in chunks (e.g., lines 1-200, 200-400, …) until you see the footer marker END_OF_SKILL_SENTINEL.\nIf you cannot confirm the footer marker, stop and ask for help; do not proceed with partial rules.\nWhen generating per-agent guides, preserve this read-all requirement so downstream agents stay in sync.\n\nNon-negotiables\n\nPlanning before coding: create/update items and meet the Ready gate before making code changes.\nWorklog is append-only; never rewrite history.\nUpdate Worklog whenever:\n\na discussion produces a clear decision or direction,\nan item state changes,\nscope/approach changes,\nor an ADR is created/linked.\n\n\nArchive by view: hide Done/Dropped items in views by default; do not move files unless explicitly requested.\nBacklog volume control:\n\nOnly create items for work that changes code or design decisions.\nAvoid new items for exploratory discussion; record in existing Worklog instead.\nKeep Tasks/Bugs sized for a single focused session.\nAvoid ADRs unless a real architectural trade-off is made.\n\n\nTicketing threshold (agent-decided):\n\nOpen a new Task/Bug when you will change code/docs/views/scripts.\nOpen an ADR (and link it) when a real trade-off or direction change is decided.\nOtherwise, record the discussion in an existing Worklog; ask if unsure.\n\n\nTicket type selection (keep it lightweight):\n\nEpic: multi-release or multi-team milestone spanning multiple Features.\nFeature: a new capability that delivers multiple UserStories.\nUserStory: a single user-facing outcome that requires multiple Tasks.\nTask: a single focused implementation or doc change (typically one session).\nExample: “End-to-end embedding pipeline” = Epic; “Pluggable vector backend” = Feature; “MVP chunking pipeline” = UserStory; “Implement tokenizer adapter” = Task.\n\n\nBug vs Task triage (when fixing behavior):\n\nIf you are correcting a behavior that was previously marked Done and the behavior violates the original intent/acceptance (defect or regression), open a Bug and link it to the original item.\nIf the change is a new requirement/scope change beyond the original acceptance, open a Task/UserStory (or Feature) instead, and link it for traceability.\n\n\nBug origin tracing (when diagnosing a defect/regression):\n\nRecord when the issue started and the evidence path you used to determine it.\nPrefer VCS-backed evidence when available:\n\nlast-known-good revision (commit hash or tag)\nfirst-known-bad revision (commit hash or tag)\nsuspected introducing change(s) (commit hash) and why (e.g., git blame on specific lines)\n\n\nIf git history is unavailable (zip export, shallow clone, missing remote), explicitly record that limitation and what alternative evidence you used (e.g., release notes, timestamps, reproduction reports).\nKeep evidence lightweight: record commit hashes + 1–2 line summaries; avoid pasting large diffs into Worklog. Attach artifacts when needed.\nSuggested Worklog template:\n\nBug origin: last_good=&lt;sha|tag&gt;, first_bad=&lt;sha|tag&gt;, suspect=&lt;sha&gt; (reason: blame &lt;path&gt;:&lt;line&gt;), evidence=&lt;git log/blame/bisect|other&gt;\n\n\n\n\nState ownership: the agent decides when to move items to InProgress or Done; humans observe and can add context.\nState semantics: Proposed = needs discovery/confirmation; Planned = approved but not started; Ready gate applies before start.\nHierarchy is in frontmatter links, not folder nesting; avoid moving files to reflect scope changes.\nFilenames stay stable; use ASCII slugs.\nNever include secrets in backlog files or logs.\nLanguage: backlog and documentation content must be English-only (no CJK), to keep parsing and cross-agent collaboration deterministic.\nAgent Identity: In Worklog and audit logs, use your own identity (e.g., [agent=antigravity]), never copy [agent=codex] blindly.\nAlways provide an explicit --agent value for auditability (some commands currently default to cli, but do not rely on it).\nModel attribution (optional but preferred): provide --model &lt;name&gt; (or env KANO_AGENT_MODEL / KANO_MODEL) when it is known deterministically.\n\nDo not guess model names; if unknown, omit the [model=...] segment.\n\n\nAgent Identity Protocol: Supply --agent &lt;ID&gt; with your real product name (e.g., cursor, copilot, windsurf, antigravity).\n\nForbidden (Placeholders): auto, user, assistant, &lt;AGENT_NAME&gt;, $AGENT_NAME.\n\n\nFile operations for backlog/skill artifacts must go through the kano-backlog CLI\r\n(python skills/kano-agent-backlog-skill/scripts/kano-backlog &lt;command&gt;) so audit logs capture the action.\nSkill scripts only operate on paths under _kano/backlog/ or _kano/backlog_sandbox/;\r\nrefuse other paths.\nAfter modifying backlog items, refresh the plain Markdown views immediately using\r\npython skills/kano-agent-backlog-skill/scripts/kano-backlog view refresh --agent &lt;agent-id&gt; --backlog-root &lt;path&gt; so the dashboards stay current.\n\nPersona summaries/reports are available via python skills/kano-agent-backlog-skill/scripts/kano-backlog admin persona summary|report ....\n\n\npython skills/kano-agent-backlog-skill/scripts/kano-backlog workitem update-state ... auto-syncs parent states forward-only by default; use --no-sync-parent\r\nfor manual re-plans where parent state should stay put.\nAdd Obsidian [[wikilink]] references in the body (e.g., a ## Links section) so Graph/backlinks work; frontmatter alone does not create graph edges.\n\nAgent compatibility: read the whole skill\n\nAlways load the entire SKILL.md before acting; some agent shells only fetch the first ~100 lines by default.\nIf your client truncates, fetch in chunks (e.g., lines 1-200, 200-400, …) until you see the footer marker END_OF_SKILL_SENTINEL.\nIf you cannot confirm the footer marker, stop and ask for help; do not proceed with partial rules.\nWhen generating per-agent guides, preserve this read-all requirement so downstream agents stay in sync.\n\nFirst-run bootstrap (prereqs + initialization)\nBefore using this skill in a repo, the agent must confirm:\n\nPython prerequisites are available (or install them), and\nthe backlog scaffold exists for the target product/root.\n\nIf the backlog structure is missing, propose the bootstrap commands and wait for user approval before writing files.\nDeveloper vs user mode (where to declare it)\n\nPreferred source of truth: product config in _kano/backlog/products/&lt;product&gt;/_config/config.toml.\n\nmode.skill_developer: true when this repo actively develops the skill itself (this demo repo).\nmode.persona: optional string describing the primary human persona (e.g. developer, pm, qa), used only for human-facing summaries/views.\n\n\nSecondary: agent guide files (e.g., AGENTS.md / CLAUDE.md) can document expectations, but are agent-specific and not script-readable.\n\nSkill developer gate (architecture compliance)\nIf mode.skill_developer=true, before writing any skill code (in scripts/ or src/), you must:\n\nRead ADR-0013 (“Codebase Architecture and Module Boundaries”) in the product decisions folder.\nFollow the folder rules defined in ADR-0013:\n\nscripts/ is executable-only: no reusable module code.\nsrc/ is import-only: core logic lives here, never executed directly.\nAll agent-callable operations go through scripts/kano-backlog CLI.\n\n\nPlace new code in the correct package:\n\nModels/config/errors → src/kano_backlog_core/\nUse-cases (create/update/view) → src/kano_backlog_ops/\nStorage backends → src/kano_backlog_adapters/\nCLI commands → src/kano_backlog_cli/commands/\n\n\n\nViolating these boundaries will be flagged in code review.\nPrerequisite install (Python)\nDetect:\n\nRun python skills/kano-agent-backlog-skill/scripts/kano-backlog doctor --format plain.\n\nIf packages are missing, install once (recommended):\n\nDefault: python -m pip install -e skills/kano-agent-backlog-skill\nSkill contributors: python -m pip install -e skills/kano-agent-backlog-skill[dev]\nOptional heavy dependencies (FAISS, sentence-transformers) should be installed manually per platform requirements before running the CLI against embedding features.\n\nBacklog initialization (file scaffold + config + dashboards)\nDetect (multi-product / platform layout):\n\nProduct initialized if _kano/backlog/products/&lt;product&gt;/_config/config.toml exists (or confirm via python skills/kano-agent-backlog-skill/scripts/kano-backlog doctor --product &lt;product&gt;).\n\nBootstrap:\n\nRun python skills/kano-agent-backlog-skill/scripts/kano-backlog admin init --product &lt;product&gt; --agent &lt;agent-id&gt; [--backlog-root &lt;path&gt;] to scaffold _kano/backlog/products/&lt;product&gt;/ (items/, decisions/, views/, _config/, _meta/, _index/).\nThe init command derives a project prefix, writes _config/config.toml, and refreshes dashboards so views exist immediately after initialization.\nManual fallback (only if automation is unavailable): follow _kano/backlog/README.md to copy the template scaffold, then refresh views via kano-backlog view refresh.\n\nOptional LLM analysis over deterministic reports\nThis skill can optionally append an LLM-generated analysis to a deterministic report.\r\nThe deterministic report is the SSOT; analysis is treated as a derived artifact.\n\nDeterministic report: views/Report_&lt;persona&gt;.md\nDerived LLM output: views/_analysis/Report_&lt;persona&gt;_LLM.md (gitignored by default)\nDeterministic prompt artifact: views/_analysis/Report_&lt;persona&gt;_analysis_prompt.md\n\nEnable by config (per product):\n\nanalysis.llm.enabled = true\n\nExecution:\n\nThe default workflow is: generate the deterministic report → use it as SSOT → fill in the analysis template.\n\nThe skill generates a deterministic prompt file to guide the analysis, and a derived markdown file with placeholder headings.\n\n\nOptional automation: when analysis.llm.enabled = true in config, view refresh generates views/snapshots/_analysis/Report_&lt;persona&gt;_analysis_prompt.md (deterministic prompt) and Report_&lt;persona&gt;_LLM.md (template or LLM output)\nNever pass API keys as CLI args; keep secrets in env vars to avoid leaking into audit logs.\n\nID prefix derivation\n\nSource of truth:\n\nProduct config: _kano/backlog/products/&lt;product&gt;/_config/config.toml (product.name, product.prefix), or\nRepo config (single-product): _kano/backlog/_config/config.toml (product.name, product.prefix).\n\n\nDerivation:\n\nSplit product.name on non-alphanumeric separators and camel-case boundaries.\nTake the first letter of each segment.\nIf only one letter, take the first letter plus the next consonant (A/E/I/O/U skipped).\nIf still short, use the first two letters.\nUppercase the result.\n\n\nExample: product.name=kano-agent-backlog-skill-demo → KABSD.\n\nRecommended layout\nThis skill supports both single-product and multi-product layouts:\n\nSingle-product (repo-level): _kano/backlog/\nMulti-product (monorepo): _kano/backlog/products/&lt;product&gt;/\n\nWithin each backlog root:\n\n_meta/ (schema, conventions)\nitems/&lt;type&gt;/&lt;bucket&gt;/ (work items)\ndecisions/ (ADR files)\nviews/ (dashboards / generated Markdown)\n\nItem bucket folders (per 100)\n\nStore items under _kano/backlog/items/&lt;type&gt;/&lt;bucket&gt;/.\nBucket names use 4 digits for the lower bound of each 100 range.\n\nExample: 0000, 0100, 0200, 0300, …\n\n\nExample path:\n\n_kano/backlog/items/task/0000/KABSD-TSK-0007_define-secret-provider-validation.md\n\n\n\nIndex/MOC files\n\nFor Epic, create an adjacent index file:\n\n&lt;ID&gt;_&lt;slug&gt;.index.md\n\n\nIndex files should render a tree using Dataview/DataviewJS and rely on parent links.\nTrack epic index files in _kano/backlog/_meta/indexes.md (type, item_id, index_file, updated, notes).\n\nReferences\n\nReference index: REFERENCE.md\nSchema and rules: references/schema.md\nTemplates: references/templates.md\nWorkflow SOP: references/workflow.md\nView patterns: references/views.md\nObsidian Bases (plugin-free): references/bases.md\nContext Graph + Graph-assisted retrieval: references/context_graph.md\n\nIf the backlog structure is missing, propose creation and wait for user approval before writing files.\nKano CLI entrypoints (current surface)\nscripts/ exposes a single executable: scripts/kano-backlog. The CLI is intentionally organized as nested command groups so agents can discover operations via --help on-demand (instead of hard-coding the full command surface into this skill).\nHelp-driven discovery (preferred)\nRun these in order, expanding only what you need:\n\npython skills/kano-agent-backlog-skill/scripts/kano-backlog --help\n\nShows top-level groups (e.g., backlog, item, state, worklog, view) and global options.\n\n\npython skills/kano-agent-backlog-skill/scripts/kano-backlog &lt;group&gt; --help\n\nShows subcommands for that group.\n\n\npython skills/kano-agent-backlog-skill/scripts/kano-backlog &lt;group&gt; &lt;command&gt; --help\n\nShows required args/options for that command.\n\n\n\nGuideline: do not paste large --help output into chat; inspect it locally and run the command.\nCanonical examples (keep these few memorized)\n\nBootstrap:\n\npython skills/kano-agent-backlog-skill/scripts/kano-backlog doctor --format plain\npython skills/kano-agent-backlog-skill/scripts/kano-backlog admin init --product &lt;name&gt; --agent &lt;id&gt;\n\n\nDaily workflow:\n\npython skills/kano-agent-backlog-skill/scripts/kano-backlog workitem create --type task --title &quot;...&quot; --agent &lt;id&gt; --product &lt;name&gt;\npython skills/kano-agent-backlog-skill/scripts/kano-backlog workitem set-ready &lt;item-id&gt; --context &quot;...&quot; --goal &quot;...&quot; --approach &quot;...&quot; --acceptance-criteria &quot;...&quot; --risks &quot;...&quot; --product &lt;name&gt;\npython skills/kano-agent-backlog-skill/scripts/kano-backlog workitem validate &lt;item-id&gt; --product &lt;name&gt;\npython skills/kano-agent-backlog-skill/scripts/kano-backlog workitem update-state &lt;item-ref&gt; --state InProgress --agent &lt;id&gt; --message &quot;...&quot; --product &lt;name&gt;\npython skills/kano-agent-backlog-skill/scripts/kano-backlog workitem attach-artifact &lt;item-id&gt; --path &lt;file&gt; --shared --agent &lt;id&gt; --product &lt;name&gt; [--note &quot;...&quot;]\npython skills/kano-agent-backlog-skill/scripts/kano-backlog view refresh --agent &lt;id&gt; --product &lt;name&gt;\n\n\nBacklog integrity checks:\n\npython skills/kano-agent-backlog-skill/scripts/kano-backlog admin validate uids --product &lt;name&gt;\n\n\n\nConflict handling policy (configurable)\nUse product config to control how duplicate IDs and UIDs are handled by maintenance commands\r\nsuch as admin links normalize-ids.\n\nConfig keys (product _config/config.toml):\n\nconflict_policy.id_conflict: default rename (rename duplicate IDs).\nconflict_policy.uid_conflict: default trash_shorter (move shorter duplicate content to _trash/).\n\n\ntrash_shorter uses _trash/&lt;YYYYMMDD&gt;/... under the product root; items get a Worklog entry.\n\nSandbox workflow (isolated experimentation)\nFor testing, prototyping, or demos without affecting production backlog:\n\nCreate: python skills/kano-agent-backlog-skill/scripts/kano-backlog admin sandbox init &lt;sandbox-name&gt; --product &lt;source-product&gt; --agent &lt;id&gt;\nUse: python skills/kano-agent-backlog-skill/scripts/kano-backlog workitem create --product &lt;sandbox-name&gt; ... (same CLI, different product)\nCleanup: rm -rf _kano/backlog_sandbox/&lt;sandbox-name&gt; (git will ignore this directory)\nRationale: Sandboxes mirror production structure but live in _kano/backlog_sandbox/, so changes never leak into _kano/backlog/.\n\nArtifacts policy (local-first)\n\nStorage locations:\n\nShared across products: _kano/backlog/_shared/artifacts/&lt;ITEM_ID&gt;/ (use --shared).\nProduct-local: _kano/backlog/products/&lt;product&gt;/artifacts/&lt;ITEM_ID&gt;/ (use --no-shared).\n\n\nUsage:\n\nAttach via workitem attach-artifact — copies the file and appends a Worklog link.\nPrefer lightweight, text-first artifacts (Markdown, Mermaid, small images). Use Git LFS for large binaries if needed.\n\n\nGit policy:\n\nCommit human-readable artifacts that aid review. Avoid committing generated binaries unless justified.\nSandboxes under _kano/backlog_sandbox/ are gitignored; artifacts there are ephemeral.\nFor derived analysis, store under views/_analysis/ (gitignored by default), and keep deterministic reports in views/.\n\n\nLinking:\n\nThe CLI appends a Markdown link relative to the item file. Optionally add a ## Links section for richer context.\n\n\n\nState update helper\n\nUse python skills/kano-agent-backlog-skill/scripts/kano-backlog workitem update-state ... to update state + append Worklog.\nPrefer --action on kano-backlog state transition for the common transitions (start, ready, review, done, block, drop).\nUse python skills/kano-agent-backlog-skill/scripts/kano-backlog workitem validate &lt;item-id&gt; to check the Ready gate explicitly.\n\nTopic and Workset workflow (context management)\nWhen to use Topics\nTopics are shareable context buffers for multi-step work that spans multiple work items or requires exploratory research before creating formal backlog items.\nUse Topics when:\n\nExploring a complex problem that may result in multiple work items\nCollecting code snippets, logs, and materials across multiple sessions\nCollaborating across agents/sessions with a shared context\nRefactoring work that requires tracking multiple code locations\n\nTopic lifecycle:\n\nCreate: python skills/kano-agent-backlog-skill/scripts/kano-backlog topic create &lt;topic-name&gt; --agent &lt;id&gt;\n\nCreates _kano/backlog/topics/&lt;topic&gt;/ with manifest.json, brief.md, brief.generated.md, notes.md, and materials/ subdirectories\n\n\nCollect materials:\n\nAdd items: topic add &lt;topic-name&gt; --item &lt;ITEM_ID&gt;\nAdd code snippets: topic add-snippet &lt;topic-name&gt; --file &lt;path&gt; --start &lt;line&gt; --end &lt;line&gt; --agent &lt;id&gt;\nPin docs: topic pin &lt;topic-name&gt; --doc &lt;path&gt;\n\n\nDistill: python skills/kano-agent-backlog-skill/scripts/kano-backlog topic distill &lt;topic-name&gt;\n\n\nGenerates/overwrites deterministic brief.generated.md from collected materials\nbrief.md is a stable, human-maintained brief (do not overwrite it automatically)\n\n\nSwitch context: python skills/kano-agent-backlog-skill/scripts/kano-backlog topic switch &lt;topic-name&gt; --agent &lt;id&gt;\n\nSets active topic (affects config overlays and workset behavior)\n\n\nClose: python skills/kano-agent-backlog-skill/scripts/kano-backlog topic close &lt;topic-name&gt; --agent &lt;id&gt;\n\nMarks topic as closed; eligible for TTL cleanup\n\n\nCleanup: python skills/kano-agent-backlog-skill/scripts/kano-backlog topic cleanup --ttl-days &lt;N&gt; [--dry-run]\n\nRemoves raw materials from closed topics older than TTL\n\n\n\nTopic snapshots (retention policy):\n\nSnapshots are intended for milestone checkpoints (pre-merge/split/restore, risky bulk edits), not every small edit.\nTo prevent noise, keep only the latest snapshot per topic in this demo repo.\nAfter creating a snapshot (or periodically), prune all but the newest snapshot:\n\npython skills/kano-agent-backlog-skill/scripts/kano-backlog topic snapshot cleanup &lt;topic-name&gt; --ttl-days 0 --keep-latest 1 --apply\n\n\n\nTopic structure:\n_kano/backlog/topics/&lt;topic&gt;/\r\n  manifest.json          # refs to items/docs/snippets, status, timestamps\r\n  brief.md               # stable, human-maintained brief (do not overwrite automatically)\r\n  brief.generated.md     # deterministic distilled brief (generated/overwritten by `topic distill`)\r\n  notes.md               # freeform notes (backward compat)\r\n  materials/             # raw collection (gitignored by default)\r\n    clips/               # code snippet refs + cached text\r\n    links/               # urls / notes\r\n    extracts/            # extracted paragraphs\r\n    logs/                # build logs / command outputs\r\n  synthesis/             # intermediate drafts\r\n  publish/               # prepared write-backs (patches/ADRs)\r\n  config.toml            # optional topic-specific config overrides\n\nWhen to use Worksets\nWorksets are per-item working directories (cached, derived data) for a single backlog item.\nUse Worksets when:\n\nStarting work on a specific Task/Bug/UserStory\nNeed scratch space for deliverables (patches, test artifacts, etc.)\nWant item-specific config overrides (rare)\n\nWorkset lifecycle:\n\nInitialize: python skills/kano-agent-backlog-skill/scripts/kano-backlog workset init &lt;ITEM_ID&gt; --agent &lt;id&gt; [--ttl-hours &lt;N&gt;]\n\nCreates _kano/backlog/.cache/worksets/items/&lt;ITEM_ID&gt;/ with meta.json, plan.md, notes.md, deliverables/\n\n\nWork: Store scratch files in deliverables/ (patches, test outputs, etc.)\nRefresh: python skills/kano-agent-backlog-skill/scripts/kano-backlog workset refresh &lt;ITEM_ID&gt; --agent &lt;id&gt;\n\nUpdates refreshed_at timestamp\n\n\nCleanup: python skills/kano-agent-backlog-skill/scripts/kano-backlog workset cleanup --ttl-hours &lt;N&gt; [--dry-run]\n\nRemoves stale worksets older than TTL\n\n\n\nWorkset structure:\n_kano/backlog/.cache/worksets/items/&lt;ITEM_ID&gt;/\r\n  meta.json              # workset metadata (item_id, agent, timestamps, ttl)\r\n  plan.md                # execution plan template\r\n  notes.md               # work notes with Decision: marker guidance\r\n  deliverables/          # scratch outputs (patches, logs, test artifacts)\r\n  config.toml            # optional item-specific config overrides\n\nTopic vs Workset decision guide\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScenarioUse TopicUse WorksetExploring before creating items✅ Yes❌ NoMulti-item refactor✅ Yes❌ NoCollecting code snippets across files✅ Yes❌ NoShared context for collaboration✅ Yes❌ NoSingle item scratch space❌ No✅ YesItem-specific deliverables❌ No✅ YesVersion-controlled distillation✅ Yes (brief.generated.md)❌ No\nBest practice: Start exploration in a Topic, create work items as scope clarifies, then use Worksets for individual item execution.\nActive topic and config overlays\n\nActive topic state is shared across agents: _kano/backlog/.cache/worksets/state.json\nWhen an agent has an active topic, config resolution includes topic overrides:\n\nLayer order: defaults → product → topic → workset → runtime\nTopic config: _kano/backlog/topics/&lt;topic&gt;/config.toml\nUse for temporary overrides (e.g., switch default_product during exploration)\n\n\nInspect active topic: python skills/kano-agent-backlog-skill/scripts/kano-backlog topic list --agent &lt;id&gt;\nInspect shared topic state: python skills/kano-agent-backlog-skill/scripts/kano-backlog topic show-state --agent &lt;id&gt; --format json\n\nMaterials buffer (Topic-specific)\n\nReference-first snippet collection: Avoid large copy-paste; store file+line+hash+optional snapshot\nSnippet refs include:\n\nfile: relative path from workspace root\nlines: [start, end] (1-based inclusive)\nhash: sha256:... of content for staleness check\ncached_text: optional snapshot (use --snapshot to include)\nrevision: git commit hash if available\n\n\n\nHuman decision materials vs. machine manifest\nDual-Readability Design: Every artifact checks against both human and agent readability:\n\nHuman-Readable: High-level summaries, clear checklists, “manager-friendly” reports for rapid decision-making\nAgent-Readable: Structural precision, file paths, line numbers, explicit markers for action without hallucination\n\nImplementation in Topics:\n\nTreat manifest.json as machine-oriented metadata:\n\nseed_items: UUID list for precise agent reference\nsnippet_refs: file+line+hash for deterministic loading\npinned_docs: absolute paths for unambiguous reference\n\n\nKeep brief.generated.md deterministic and tool-owned (generated/overwritten by topic distill):\n\nReadable item titles (e.g., “KABSD-TSK-0042: Implement tokenizer adapter”)\nIf available, include item path and keep UID in a hidden HTML comment for deterministic mapping\nMaterials index with items/docs/snippets sorted for repeatability\n\n\nKeep brief.md human-oriented and stable (do not overwrite automatically):\n\nContext summary and key decisions\nOptional: include a human-friendly materials list (do not duplicate raw snippet text)\n\n\nPut human-facing decision support in _kano/backlog/topics/&lt;topic&gt;/notes.md (and/or pinned docs), e.g.:\n\nDecision to make\nOptions + trade-offs\nEvidence (ADR links, snippet refs, benchmark/log artifacts)\nRecommendation + follow-ups\n\n\nStaleness detection: Compare current file hash with stored hash to detect if code changed\nDistillation: topic distill generates deterministic brief.generated.md with a repeatable materials index\n\n\nEND_OF_SKILL_SENTINEL"},"skill/readme":{"title":"readme","links":["docs/workset","docs/topic"],"tags":[],"content":"kano-agent-backlog-skill\n\r\n\r\n\r\n\n\nAI Agent Skills for Spec-Driven Agentic Programming | Local-first backlog | Multi-agent collaboration | Durable decision trail\n\nLocal-first backlog + decision trail for agent collaboration.\nTurn chat-only context (trade-offs, decisions, why-not-that-option) into durable engineering assets, so your agent writes code only after capturing what to do, why, and how to verify.\n\nCode can be rewritten. Lost decisions can’t.\n\nAgent-first quickstart\nThis repo is meant to be used with an AI agent. The goal is not just to generate code, but to keep an auditable trail of intent, constraints, and decisions.\nCopy/paste: agent instructions\nPaste this into your agent’s system prompt / project instructions, ex: AGENTS.md:\nYou are an engineering agent working in this repository.\n \nRules of engagement:\n- Read and follow SKILL.md.\n- Before changing any code, ensure there is a Task/Bug item for the change and that it is Ready (Context, Goal, Approach, Acceptance Criteria, Risks / Dependencies are non-empty).\n- Use kano-backlog commands to create/update items and append Worklog entries; Worklog is append-only.\n- Use a workset while implementing: init -&gt; next -&gt; edit plan.md (check off completed steps) -&gt; next -&gt; repeat.\n- If a decision is load-bearing, create an ADR and link it from the item.\n- Prefer deterministic, repo-grounded outputs; do not invent file paths or backlog state.\nThe execution loop (minimal)\n# One-time setup in a repo\nkano-backlog admin init --product &lt;my-product&gt; --agent &lt;agent-id&gt;\n \n# Start work (tickets-first)\nkano-backlog item create --type task --title &quot;...&quot; --agent &lt;agent-id&gt; --product &lt;my-product&gt;\nkano-backlog item set-ready &lt;ITEM_ID&gt; --product &lt;my-product&gt; \\\n  --context &quot;...&quot; --goal &quot;...&quot; --approach &quot;...&quot; --acceptance-criteria &quot;...&quot; --risks &quot;...&quot;\nkano-backlog item update-state &lt;ITEM_ID&gt; --state InProgress --agent &lt;agent-id&gt; --product &lt;my-product&gt;\n \n# Prevent drift while implementing\nkano-backlog workset init --item &lt;ITEM_ID&gt; --agent &lt;agent-id&gt;\nkano-backlog workset next --item &lt;ITEM_ID&gt;\n \n# (Edit _kano/backlog/.cache/worksets/items/&lt;ITEM_ID&gt;/plan.md to check off steps)\n# Then run `kano-backlog workset next --item &lt;ITEM_ID&gt;` again.\n \n# Write back anything worth keeping\nkano-backlog workset promote --item &lt;ITEM_ID&gt; --agent &lt;agent-id&gt;\nkano-backlog view refresh --product &lt;my-product&gt; --agent &lt;agent-id&gt;\nTip: most commands support --format json for structured agent/tool integration.\nWhat this is\nkano-agent-backlog-skill is an Agent Skill bundle (centered around SKILL.md) that guides/constrains an agent into a “tickets first” workflow:\n\nCreate/update a work item (Epic/Feature/UserStory/Task/Bug) before any code change\nCapture key decisions via append-only Worklog entries or ADRs, and link them together\nEnforce a Ready gate so each item has the minimum shippable context (Context/Goal/Approach/Acceptance/Risks)\nOptional Obsidian views (Dataview / Bases) so humans can inspect, intervene, and review\n\nThis skill is local-first: you can start without Jira / Azure Boards and still keep engineering discipline.\nWhy you might want it\nIf any of these sound familiar, this helps:\n\nYou made an architecture choice, but later forgot why you didn’t pick the other option\nThe agent output works, but maintenance feels like archaeology (missing rationale and constraints)\nRequirement changes force you back into chat history to understand impact\nYou want the agent as a teammate, but you end up acting as the “human memory cache”\n\nGoal: convert “evaporating context” into searchable, linkable, auditable files in your repo.\nThe Dual-Readability Principle\nTopics and Snapshots are designed to solve two problems at once:\n\nHuman Overload: Humans cannot keep entire repo states in their head. We need high-level summaries (brief.md, Reports) to make decisions.\nAgent Coordination: Agents cannot “see” the repo like we do. They need explicit lists of files, line numbers, and “stub inventories” to know what to do next.\n\nBy enforcing Dual-Readability (Markdown for humans, JSON/Structured data for Agents), we create a shared workspace where:\n\nHumans provide direction (via Briefs).\nAgents provide evidence (via Snapshots).\nBoth can understand the other’s output without translation.\n\nWhat you get (implemented)\n\nSKILL.md: the workflow and rules (planning-before-coding, Ready gate, worklog discipline)\nreferences/schema.md: item types, states, naming, minimal frontmatter\nreferences/templates.md: work item / ADR templates\nreferences/workflow.md: SOP (when to create items, when to record decisions, how to converge)\nreferences/views.md: Obsidian view patterns (Dataview + Bases)\nkano-backlog: Typer-based CLI entrypoint with subcommands:\n\nadmin - Backlog bootstrap + maintenance helpers (init, adr, index, schema, release, …)\nitem / workitem - Create/manage work items (Epic/Feature/UserStory/Task/Bug) + write-backs\nworklog - Append worklog entries\nworkset - Per-item execution cache (init/refresh/next/promote/cleanup/detect-adr)\ntopic - Context grouping (templates, snapshots, merge/split, cross-references, switch/export)\nview - Generate dashboards and reports\nsnapshot - Evidence snapshots (read-only capture)\nconfig - Inspect/validate layered config\nembedding - Embedding pipeline operations (build/query/status)\nsearch - Vector similarity search\ntokenizer - Tokenizer adapter configuration/testing\nbenchmark - Deterministic benchmark harness\nchangelog - Generate changelog from backlog\ndoctor - Environment/backlog health checks\n\n\nsrc/kano_backlog_core: canonical models/storage helpers\nsrc/kano_backlog_ops: use-cases (create/update/view/workset/topic)\nsrc/kano_backlog_cli: CLI wiring (commands + utilities)\n\nNote: backlog bootstrap and maintenance commands are grouped under kano-backlog admin ....\nOptionally, create _kano/backlog/ in your project repo to store items, ADRs, views, and helper scripts as the system of record.\nInstall and run\nFrom this repository root:\npython -m pip install -e .\nkano-backlog --help\nNo-install option (useful for agents/tools that run scripts directly):\npython scripts/kano-backlog --help\nQuick start (see value in ~5 minutes)\n\nRun kano-backlog admin init --product &lt;my-product&gt; --agent &lt;id&gt; to scaffold _kano/backlog/products/&lt;my-product&gt;/\n(Optional) Open the repo in Obsidian and enable Dataview or Bases\nOpen _kano/backlog/products/&lt;my-product&gt;/views/ (or regenerate them with kano-backlog view refresh --agent &lt;id&gt; --product &lt;my-product&gt;)\nBefore any code change, create a Task/Bug and satisfy the Ready gate\nWhen a load-bearing decision happens, append a Worklog line; create an ADR when it’s truly architectural and link it\n\nWorkset and Topic Usage\nWorksets: Focused Execution\nWorksets prevent agent drift during task execution by providing a structured working context:\n# Initialize workset for a task\nkano-backlog workset init --item TASK-0042 --agent kiro\n \n# Get next action from plan\nkano-backlog workset next --item TASK-0042\n \n# Detect decisions that should become ADRs\nkano-backlog workset detect-adr --item TASK-0042\n \n# Promote deliverables to canonical artifacts\nkano-backlog workset promote --item TASK-0042 --agent kiro\n \n# Clean up expired worksets\nkano-backlog workset cleanup --ttl-hours 72\nSee workset.md for complete documentation.\nTopics: Context Switching\nTopics enable rapid context switching when focus areas change.\nUnlike worksets (which live entirely under _kano/backlog/.cache/worksets/items/&lt;item-id&gt;/), topics are stored under _kano/backlog/topics/&lt;topic&gt;/ so the deterministic brief.md can be shared/reviewed. Raw materials under materials/ are treated as cache.\n# Create a topic for related work\nkano-backlog topic create auth-refactor --agent kiro\n \n# Add items to the topic\nkano-backlog topic add auth-refactor --item TASK-0042\nkano-backlog topic add auth-refactor --item BUG-0012\n \n# Pin relevant documents\nkano-backlog topic pin auth-refactor --doc _kano/backlog/decisions/ADR-0015.md\n \n# Collect a code snippet reference (optional cached snapshot)\nkano-backlog topic add-snippet auth-refactor --file src/auth.py --start 10 --end 25 --agent kiro --snapshot\n \n# Distill deterministic brief.generated.md from collected materials\nkano-backlog topic distill auth-refactor\n \n# Generate a decision write-back audit report (writes to topic publish/)\nkano-backlog topic decision-audit auth-refactor --format plain\n \n# Switch active topic (per-agent pointer lives in cache)\nkano-backlog topic switch auth-refactor --agent kiro\n \n# Export context bundle\nkano-backlog topic export-context auth-refactor --format json\n \n# Close and later cleanup closed topics\nkano-backlog topic close auth-refactor --agent kiro\nkano-backlog topic cleanup --ttl-days 14\nkano-backlog topic cleanup --ttl-days 14 --apply\n \n# Write back a decision to a work item (appends to the item&#039;s Decisions section + Worklog)\nkano-backlog workitem add-decision KABSD-TSK-0001 \\\n  --decision &quot;Use X over Y because ...&quot; \\\n  --source &quot;_kano/backlog/topics/auth-refactor/synthesis/decision-notes.md&quot; \\\n  --agent kiro \\\n  --product &lt;my-product&gt;\nSee topic.md for complete documentation.\nSkill version\nShow the current skill version:\npython -c &quot;import pathlib; print((pathlib.Path(&#039;VERSION&#039;)).read_text().strip())&quot;\nExternal references\n\nAgent skills overview (Anthropic/Claude): platform.claude.com/docs/en/agents-and-tools/agent-skills/overview\nVersioning policy: VERSIONING.md\nRelease notes: CHANGELOG.md\n\nContributing\nPRs welcome, with one rule: don’t turn this into another Jira.\nThe point is to preserve decisions and acceptance, not to worship process.\nLicense\nMIT"},"skill/reference":{"title":"reference","links":[],"tags":[],"content":"Reference Index\nThe references/ folder is intentionally split into multiple small files so an agent (or a human) can load only what’s needed.\nFiles under references/\n\nschema.md: item types, states, naming rules, Ready gate\ntemplates.md: work item + ADR templates\nworkflow.md: SOP for planning/decisions/worklog\nviews.md: view patterns (Dataview + plain Markdown generators)\nbases.md: Obsidian Bases notes (plugin-free table-style views)\nlogging.md: audit log schema, redaction, rotation defaults\nprocesses.md: process profile schema and examples\nindexing.md: optional indexing layer (artifacts, config, rebuild workflow)\ncontext_graph.md: context graph + Graph-assisted retrieval (weak graph)\nindexing_schema.sql: optional DB index schema (SQLite-first)\nindexing_schema.json: DB schema description (machine-readable)\n\nKano CLI (automation surface)\nscripts/ now ships a single entrypoint (scripts/kano). Subcommands map 1:1 to the ops layer:\nCore commands\n\nkano doctor: verify Python prerequisites and backlog initialization\nkano backlog init: scaffold a product backlog (directories, _config/config.json, dashboards)\nkano view refresh: regenerate dashboards (Active/New/Done)\n\nItem operations\n\nkano item read|validate: inspect canonical records\nkano item create: create items\nkano item set-ready: set Ready-gate body sections (Context/Goal/Approach/Acceptance/Risks)\nkano item update-state: state transitions + worklog append + optional dashboard refresh\n\nState and worklog\n\nkano state transition: declarative workflow actions (start, ready, review, done, block, drop)\nkano worklog append: structured worklog writes with agent/model attribution\n\nBacklog administration (nested under backlog)\n\nkano backlog index build|refresh: build/refresh SQLite index from markdown items\nkano backlog demo seed: seed demo data (1 epic → 1 feature → 3 tasks) for testing\nkano backlog persona summary|report: generate persona activity summaries/reports\nkano backlog sandbox init: scaffold isolated sandbox environments for experimentation\nkano config show|validate|migrate-json|export|init: inspect/validate/migrate/export config (TOML-first; product.* required)\n\nNo new standalone scripts will be added under scripts/; all operations flow through the unified CLI.\nRelated (demo repo convention)\nIn the demo host repo, the backlog lives under _kano/backlog/:\n\nItems: _kano/backlog/items/**\nDecisions/ADRs: _kano/backlog/decisions/**\nViews: _kano/backlog/views/**\nTools: _kano/backlog/tools/**\nProduct configs: _kano/backlog/products/&lt;product&gt;/_config/config.toml (product.name, product.prefix)\n\nVersioning\n\nVersioning policy: VERSIONING.md (Git tags vX.Y.Z)\n\nTemplates (optional)\n\ntemplates/AGENTS.block.md: snippet for Codex-style agent instructions (append/update into repo-root AGENTS.md)\ntemplates/CLAUDE.block.md: snippet for Claude-style instructions (append/update into repo-root CLAUDE.md)\n"}}