{"api/index":{"title":"index","links":[],"tags":[],"content":"Python API Reference\nCore Modules\nDomain Models\n::: kano_backlog.domain\nCLI Commands\n::: kano_backlog.cli\nWorkset Management\n::: kano_backlog.workset\nTopic Management\n::: kano_backlog.topic"},"cli/index":{"title":"index","links":[],"tags":[],"content":"CLI Reference\nkano-backlog\n                                                                               \r\n Usage: kano-backlog [OPTIONS] COMMAND [ARGS]...                               \r\n                                                                               \r\n kano-backlog: Backlog management CLI (MVP)                                    \r\n                                                                               \r\n+- Options -------------------------------------------------------------------+\r\n| --install-completion          Install completion for the current shell.     |\r\n| --show-completion             Show completion for the current shell, to     |\r\n|                               copy it or customize the installation.        |\r\n| --help                        Show this message and exit.                   |\r\n+-----------------------------------------------------------------------------+\r\n+- Commands ------------------------------------------------------------------+\r\n| doctor      Check environment health.                                       |\r\n| admin       Administrative and setup commands                               |\r\n| workitem    Work item operations                                            |\r\n| item        Work item operations (alias)                                    |\r\n| state       State transitions                                               |\r\n| worklog     Worklog operations                                              |\r\n| view        View and dashboard operations                                   |\r\n| snapshot    Snapshot and evidence operations                                |\r\n| workset     Workset cache operations                                        |\r\n| topic       Topic context operations                                        |\r\n| config      Config inspection and validation                                |\r\n| changelog   Changelog generation from backlog                               |\r\n| benchmark   Deterministic benchmark harness                                 |\r\n| embedding   Embedding pipeline operations                                   |\r\n| search      Vector similarity search                                        |\r\n| tokenizer   Tokenizer adapter configuration, testing, and diagnostics       |\r\n+-----------------------------------------------------------------------------+\r\n\n\nkano-backlog item\n                                                                               \r\n Usage: kano-backlog item [OPTIONS] COMMAND [ARGS]...                          \r\n                                                                               \r\n Work item operations (alias)                                                  \r\n                                                                               \r\n+- Options -------------------------------------------------------------------+\r\n| --help          Show this message and exit.                                 |\r\n+-----------------------------------------------------------------------------+\r\n+- Commands ------------------------------------------------------------------+\r\n| read              Read a backlog item from the canonical store.             |\r\n| validate          Validate a work item against the Ready gate.              |\r\n| add-decision      Append a decision write-back entry to a work item.        |\r\n| create            Create a new backlog work item (ops-backed                |\r\n|                   implementation).                                          |\r\n| set-ready         Set Ready-gate body sections for an existing item.        |\r\n| update-state      Update work item state via the ops layer.                 |\r\n| attach-artifact   Attach an artifact file to a work item and append a       |\r\n|                   Worklog link.                                             |\r\n+-----------------------------------------------------------------------------+\r\n\n\nkano-backlog workset\n                                                                               \r\n Usage: kano-backlog workset [OPTIONS] COMMAND [ARGS]...                       \r\n                                                                               \r\n Workset cache operations                                                      \r\n                                                                               \r\n+- Options -------------------------------------------------------------------+\r\n| --help          Show this message and exit.                                 |\r\n+-----------------------------------------------------------------------------+\r\n+- Commands ------------------------------------------------------------------+\r\n| init         Initialize a workset for an item.                              |\r\n| refresh      Refresh workset from canonical files.                          |\r\n| next         Get next unchecked action from plan.                           |\r\n| promote      Promote deliverables to canonical artifacts.                   |\r\n| cleanup      Clean up expired worksets.                                     |\r\n| list         List all worksets.                                             |\r\n| detect-adr   Detect ADR candidates in notes.                                |\r\n+-----------------------------------------------------------------------------+\r\n\n\nkano-backlog topic\n                                                                               \r\n Usage: kano-backlog topic [OPTIONS] COMMAND [ARGS]...                         \r\n                                                                               \r\n Topic context operations                                                      \r\n                                                                               \r\n+- Options -------------------------------------------------------------------+\r\n| --help          Show this message and exit.                                 |\r\n+-----------------------------------------------------------------------------+\r\n+- Commands ------------------------------------------------------------------+\r\n| create              Create a new topic, optionally from a template.         |\r\n| add                 Add an item to a topic.                                 |\r\n| pin                 Pin a document to a topic.                              |\r\n| add-snippet         Collect a code snippet reference into the topic         |\r\n|                     materials buffer.                                       |\r\n| distill             Generate/overwrite deterministic brief.generated.md     |\r\n|                     from collected materials.                               |\r\n| decision-audit      Generate a decision write-back audit report for a       |\r\n|                     topic.                                                  |\r\n| close               Mark topic as closed (enables TTL cleanup of            |\r\n|                     materials).                                             |\r\n| cleanup             Cleanup raw materials for closed topics after TTL       |\r\n|                     (dry-run by default).                                   |\r\n| merge               Merge multiple topics into a target topic.              |\r\n| switch              Switch active topic.                                    |\r\n| export-context      Export topic context bundle.                            |\r\n| list                List all topics.                                        |\r\n| add-reference       Add a reference from one topic to another               |\r\n|                     (bidirectional).                                        |\r\n| remove-reference    Remove a reference from one topic to another            |\r\n|                     (bidirectional cleanup).                                |\r\n| split               Split a topic into multiple focused subtopics.          |\r\n| list-active         List all active topics across all agents (shared        |\r\n|                     state).                                                 |\r\n| show-state          Show shared topic state (state.json contents).          |\r\n| migrate             Migrate legacy active_topic.&lt;agent&gt;.txt files to shared |\r\n|                     state.json.                                             |\r\n| cleanup-legacy      Remove legacy active_topic.&lt;agent&gt;.txt files (after     |\r\n|                     migration).                                             |\r\n| migrate-filenames   Migrate topic state filenames from {uuid}.json to       |\r\n|                     {slug}_{uuid}.json format.                              |\r\n| template            Template management commands                            |\r\n| snapshot            Topic snapshot management commands                      |\r\n+-----------------------------------------------------------------------------+\r\n\n\nkano-backlog view\n                                                                               \r\n Usage: kano-backlog view [OPTIONS] COMMAND [ARGS]...                          \r\n                                                                               \r\n View and dashboard operations                                                 \r\n                                                                               \r\n+- Options -------------------------------------------------------------------+\r\n| --help          Show this message and exit.                                 |\r\n+-----------------------------------------------------------------------------+\r\n+- Commands ------------------------------------------------------------------+\r\n| refresh   Refresh all dashboards (views) in the backlog.                    |\r\n+-----------------------------------------------------------------------------+\r\n\n\nkano-backlog admin\n                                                                               \r\n Usage: kano-backlog admin [OPTIONS] COMMAND [ARGS]...                         \r\n                                                                               \r\n Administrative and setup commands                                             \r\n                                                                               \r\n+- Options -------------------------------------------------------------------+\r\n| --help          Show this message and exit.                                 |\r\n+-----------------------------------------------------------------------------+\r\n+- Commands ------------------------------------------------------------------+\r\n| init       Create the canonical backlog scaffold for a product.             |\r\n| index      Index operations                                                 |\r\n| demo       Demo data operations                                             |\r\n| persona    Persona activity operations                                      |\r\n| sandbox    Sandbox environment operations                                   |\r\n| validate   Backlog validation helpers                                       |\r\n| links      Link maintenance helpers                                         |\r\n| items      Item maintenance helpers                                         |\r\n| adr        ADR operations                                                   |\r\n| schema     Schema validation and fixing                                     |\r\n| meta       Meta file helpers                                                |\r\n| release    Release verification workflows                                   |\r\n+-----------------------------------------------------------------------------+\r\n\n"},"demo/AGENTS":{"title":"AGENTS","links":["ADR-0037"],"tags":[],"content":"AGENTS\nExternal File Loading\nCRITICAL: When you encounter a file reference (e.g., @rules/general.md, @docs/architecture.md), use your Read tool to load it on a need-to-know basis. These references are relevant to the SPECIFIC task at hand.\nInstructions\n\nDo NOT preemptively load all references - use lazy loading based on actual need\nWhen loaded, treat content as mandatory instructions that override defaults\nFollow references recursively when the loaded file contains additional @ references\nPriority: External file content &gt; AGENTS.md defaults &gt; built-in instructions\n\nExample Usage\nUser mentions: &quot;Follow @rules/authentication.md for this task&quot;\r\nAgent: Reads @rules/authentication.md, applies its rules for this task only\n\n\nRepo purpose\nThis repo is a demo showing how to use kano-agent-backlog-skill to turn agent collaboration\r\ninto a durable, local-first backlog with an auditable decision trail (instead of losing context in chat).\nConversational-first documentation (human-agent collaboration)\nThis project‚Äôs primary value is human + AI collaboration, not just a CLI.\r\nTherefore, when writing or updating documentation (README, docs, SKILL.md, process notes), always include\r\ninstructions for how to drive the workflow through a conversation with an AI agent, not only how to run commands.\nRules:\n\nEvery workflow doc should contain both:\n\nCLI commands (for deterministic, auditable execution), and\nSuggested chat prompts (copy/paste) that a human can say to an agent.\n\n\nPrompts must be specific about inputs the agent needs: topic/item IDs, product, agent identity, expected outputs.\nDocument the expected artifacts and paths the agent will produce/update (e.g., reports under topic/publish/).\nPrefer a consistent pattern in docs:\n\n‚ÄúSay this to your agent‚Äù\n‚ÄúThe agent will do‚Äù (explicit steps)\n‚ÄúExpected output‚Äù (files/paths + how to verify)\n\n\n\nExample (decision audit + decision write-back):\n\nSay to agent: ‚ÄúRun a decision write-back audit for topic  and show me which work items are missing decisions.‚Äù\nAgent runs: kano topic decision-audit &lt;topic-name&gt; --format plain\nExpected output: _kano/backlog/topics/&lt;topic-name&gt;/publish/decision-audit.md\nSay to agent: ‚ÄúWrite back this decision to &lt;ITEM_ID&gt; and include the synthesis file as source.‚Äù\nAgent runs: kano workitem add-decision &lt;ITEM_ID&gt; --decision &quot;...&quot; --source &quot;...&quot; --agent &lt;agent-id&gt; --product &lt;product&gt;\nExpected output: updated work item with a ## Decisions section + appended Worklog entry.\n\nAgent roster (from README.md)\n\nCodex\nGitHub Copilot\nGoogle Antigravity\nAmazon Q\nAmazon Kiro\nCursor\nWindsurf\nOpenCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgentPrimary Model(s)Alternative ModelsNotesCodexCodex-Max (Nov 2025)GPT-5.2-Codex, o1/o3-reasoningOpenAI‚Äôs dedicated line for software engineering and multi-file logicGitHub CopilotGPT-5.2 (Dec 2025)Claude 4.5, Gemini 3 Pro, GPT-5.1Multi-model picker; GPT-5.2 Pro available for advanced research/reasoningGoogle AntigravityGemini 3 Flash (Dec 2025)Gemini 3 Pro, Gemini 1.5 Pro (legacy)Optimized for low-latency planning and multimodal workspace reasoningAmazon QClaude 4 Sonnet (May 2025)Claude 4.5 Sonnet, Amazon NovaEnhanced for autonomous computer use and deep codebase integrationAmazon KiroAuto-Routing (Dynamic)Claude 4.5 (Opus/Sonnet/Haiku)Intelligent routing across Claude 4 family; ~23% cheaper than direct Opus 4.5 useCursorClaude 3.5 Sonnet, GPT-5cursor-small (V2), GPT-4.1 MiniStable support for 2024/2025 frontier models; high-freq codebase indexingWindsurfSWE-1.5 (Proprietary)Claude 4, GPT-5.1, BYOKSWE-1.5 is a frontier coding model with performance near Claude 4.5OpenCodeMulti-ProviderDeepSeek V3.2, Llama 4 Scout, Mistral Large 3Open-source champion; supports 75+ providers including regional models\nDevelopment Guidelines\nDerived Data and .gitignore Rules\nCRITICAL: Always exclude derived/generated data from version control to keep repositories clean and efficient.\nWhat to Exclude\nGenerated/Derived Data (always add to .gitignore):\n\nCache directories: .cache/, __pycache__/, .pytest_cache/\nBuild artifacts: dist/, build/, *.egg-info/\nTest outputs: htmlcov/, .coverage, .hypothesis/\nVector databases: .cache/vector/, *.sqlite3 (embedding indexes)\nCompiled files: *.pyc, *.pyo, *.so\nIDE files: .vscode/, .idea/, *.swp\nOS files: .DS_Store, Thumbs.db\nEnvironment files: .env, .venv/, venv/\n\nWhat to Include\nSource of Truth (always version control):\n\nSource code: src/, tests/, scripts\nConfiguration templates: config.toml.example, default configs\nDocumentation: README.md, references/, SKILL.md\nSchema definitions: data models, API specs\nTest fixtures: static test data, benchmark corpus\n\nImplementation Rules\n\n\nBefore adding any new feature that generates data:\n\nIdentify what files/directories will be created\nAdd appropriate .gitignore entries immediately\nDocument the regeneration process\n\n\n\nCommon patterns to exclude:\n# Caches and derived data\n.cache/\n__pycache__/\n*.pyc\n \n# Build outputs\ndist/\nbuild/\n*.egg-info/\n \n# Test artifacts\n.pytest_cache/\n.coverage\nhtmlcov/\n \n# Vector/embedding indexes\n.cache/vector/\n*.sqlite3\n \n# IDE and OS\n.vscode/\n.DS_Store\n\n\nDocumentation requirement:\n\nAlways document how to regenerate excluded data\nInclude regeneration steps in README or setup docs\nProvide example commands for rebuilding indexes/caches\n\n\n\nRationale\n\nRepository size: Derived data can be large and grows over time\nMerge conflicts: Generated files often cause unnecessary conflicts\nEnvironment differences: Derived data may be platform/environment specific\nReproducibility: Source code should be sufficient to recreate all derived data\nSecurity: Avoid accidentally committing sensitive generated data\n\nQuick Start Commands\n# Install dependencies (dev mode)\npython -m pip install -e skills/kano-agent-backlog-skill[dev]\n \n# Run a specific test\npython -m pytest tests/test_chunking_mvp.py -v\n \n# Run all tests\npython -m pytest tests/ -v\n \n# Run tests with coverage\npython -m pytest tests/ --cov=skills/kano-agent-backlog-skill/src --cov-report=html\n \n# Format code\nblack skills/kano-agent-backlog-skill/src tests/\n \n# Sort imports\nisort skills/kano-agent-backlog-skill/src tests/\n \n# Check types\nmypy skills/kano-agent-backlog-skill/src\n \n# Run all linting\nblack skills/kano-agent-backlog-skill/src tests/ &amp;&amp; \\\nisort skills/kano-agent-backlog-skill/src tests/ &amp;&amp; \\\nmypy skills/kano-agent-backlog-skill/src\nCode Style Guidelines\nType Hints\nAlways use type hints from typing module: List, Dict, Optional, Any, Union, Tuple.\nExample:\nfrom typing import List, Optional, Dict\n \ndef process_items(items: List[str]) -&gt; Dict[str, Any]:\n    &quot;&quot;&quot;Process items and return a dictionary.&quot;&quot;&quot;\n    result: Dict[str, Any] = {}\n    for item in items:\n        result[item] = len(item)\n    return result\nImport Conventions\n\nOrder: standard library ‚Üí third-party ‚Üí local modules\nUse absolute imports from skill packages: from kano_backlog_core import ...\nUse relative imports within packages: from .models import ...\n\nExample:\nimport json  # Standard library\nfrom pathlib import Path  # Standard library\nfrom frontmatter import load  # Third-party\nfrom kano_backlog_core import BacklogItem  # Absolute from skill root\nfrom .models import ItemState  # Relative within package\nNaming Conventions\n\nClasses: PascalCase (e.g., CanonicalStore, ChunkingOptions)\nFunctions: snake_case (e.g., read_item, validate_config)\nConstants: UPPER_SNAKE_CASE (e.g., TYPE_DIRNAMES)\nPrivate members: leading underscore _private_var, __dunder__\nModules: snake_case (e.g., kano_backlog_core, token_counter)\n\nFormatting Rules\n\nLine length: 88 characters (Black default)\nIndentation: 4 spaces\nNo trailing whitespace\nDocstrings: triple double quotes &quot;&quot;&quot;, Google style\nType hints: after function definition, before docstring\n\nExample:\ndef process_data(data: List[Dict[str, Any]]) -&gt; Dict[str, Any]:\n    &quot;&quot;&quot;Process data and return results.\n \n    Args:\n        data: Input data to process.\n \n    Returns:\n        Dictionary of processed results.\n    &quot;&quot;&quot;\n    result: {}\n    for item in data:\n        result[item[&quot;id&quot;]] = item[&quot;value&quot;]\n    return result\nError Handling\nUse exceptions from kano_backlog_core/errors.py:\n\nItemNotFoundError - Item file doesn‚Äôt exist\nParseError - Invalid frontmatter or markdown\nValidationError - Data validation failed\nConfigError - Configuration error\nWriteError - Write operation failed\n\nExample:\nfrom kano_backlog_core.errors import ItemNotFoundError\n \ndef load_item(item_path: Path) -&gt; BacklogItem:\n    if not item_path.exists():\n        raise ItemNotFoundError(item_path, f&quot;Item file not found: {item_path}&quot;)\nDocstring Conventions\n\nTriple double quotes &quot;&quot;&quot;\nGoogle Style Guide format\nInclude Args:, Returns:, and Raises: sections\nImperative mood: ‚ÄúReturn‚Äù, not ‚ÄúReturns‚Äù\n\nExample:\ndef create_item(title: str) -&gt; BacklogItem:\n    &quot;&quot;&quot;Create a new backlog item.\n \n    Args:\n        title: The title of the item.\n \n    Returns:\n        The created BacklogItem object.\n    &quot;&quot;&quot;\n    return BacklogItem(title=title)\nTesting Guidelines\nTest Structure\n\nUse pytest as test runner\nPlace tests in tests/ directory\nTest filename: test_{module_name}.py\nUse Hypothesis for property-based testing\n\nProperty-Based Testing with Hypothesis\nfrom hypothesis import given, strategies as st, composite\nfrom kano_backlog_core import ChunkingOptions\n \n@composite\ndef valid_config_strategy(draw):\n    &quot;&quot;&quot;Generate valid config instances.&quot;&quot;&quot;\n    return ChunkingOptions(\n        target_tokens=draw(st.integers(min_value=50, max_value=2048)),\n        max_tokens=draw(st.integers(min_value=512, max_value=4096))\n    )\n \n@given(valid_config_strategy())\ndef test_valid_config_passes(config: ChunkingOptions):\n    &quot;&quot;&quot;Valid config should pass validation.&quot;&quot;&quot;\n    assert config.target_tokens &lt;= config.max_tokens\nUsing the Kano Backlog Skill\nBefore Making Changes\n\nCheck existing items:\n\npython -m kano_backlog_cli.main item list --product kano-agent-backlog-skill\n\nCreate or update work items before coding:\n\npython -m kano_backlog_cli.main item create \\\n    --type task \\\n    --title &quot;Implement X feature&quot; \\\n    --product kano-agent-backlog-skill \\\n    --agent &lt;your-agent-id&gt;\n\nEnforce the Ready gate on Task/Bug items:\n\n\nRequired fields: Context, Goal, Approach, Acceptance Criteria, Risks / Dependencies\nAll fields must be non-empty and written in English only\n\n\nUpdate item state when starting work:\n\npython -m kano_backlog_cli.main item update-state \\\n    --id KABSD-TSK-0146 \\\n    --state InProgress \\\n    --agent &lt;your-agent-id&gt;\nWorklog Discipline\nWorklog is append-only. Append when:\n\nA load-bearing decision is made\nAn item state changes\nScope/approach changes\nAn ADR is created/linked\n\nFormat:\nYYYY-MM-DD HH:MM [agent=&lt;agent-id&gt;] [model=&lt;model&gt;] description\n\nAlways provide explicit --agent &lt;id&gt; - never use placeholders like auto or &lt;AGENT_NAME&gt;.\nState Transitions\n# Move to InProgress\npython -m kano_backlog_cli.main item update-state --id &lt;ID&gt; --state InProgress --agent &lt;agent-id&gt;\n \n# Move to Done\npython -m kano_backlog_cli.main item update-state --id &lt;ID&gt; --state Done --agent &lt;agent-id&gt;\nADR Creation\npython -m kano_backlog_cli.main adr create \\\n    --title &quot;Decision title&quot; \\\n    --product kano-agent-backlog-skill \\\n    --agent &lt;agent-id&gt;\nAgent-Specific Rules\nGitHub Copilot\nFollow commit guidelines in .github/copilot-instructions.md:\n\nUse Kano backlog IDs directly in commit messages\nPreferred format: KABSD-TSK-0146: &lt;short summary&gt;\nMultiple items: KABSD-TSK-0146 KABSD-TSK-0147: &lt;short summary&gt;\nDo NOT use jira# prefix\n\nBacklog system note:\n\nThis repository uses kano-backlog as the system of record, not Jira.\nDo not add any jira# or JIRA: prefixes. Reference Kano IDs directly.\nExamples:\n\nGood: KABSD-TSK-0261: refine filename truncation\nBad: jira#KABSD-TSK-0261\n\n\n\nAgent Identity\nValid agent IDs for worklog entries:\n\ncopilot, codex, claude, goose, antigravity, cursor, windsurf, opencode, kiro, amazon-q\n\nForbidden: &lt;AGENT_NAME&gt;, $AGENT_NAME\nArchitecture Rules (ADR-0013)\nModule Boundaries\n\nkano_backlog_core/ - Import-only, no executable code\nkano_backlog_ops/ - Use cases and business logic\nkano_backlog_cli/ - Executable CLI commands\nscripts/ - Entry point scripts\n\nCross-Package Imports\nUse absolute imports from kano_backlog_core when importing from other packages:\nfrom kano_backlog_core import BacklogItem, ItemState\nNever import directly from kano_backlog_ops from CLI or scripts.\nInspector Pattern (ADR-0037)\nPrinciple: Skill core provides query surface (deterministic data extraction). All ‚Äúexpert judgment‚Äù lives in external inspector agents.\nWhat This Means:\n\nCore provides: audit, snapshot, constellation, workitem.query, doc.resolve APIs (all read-only, deterministic)\nInspector agents consume: Query APIs to produce health reports, review suggestions, refactor recommendations\nEvidence required: Every inspector finding must cite file path + line range + item/ADR ID\n\nPattern:\n# Inspector agent calls query surface\nkano-backlog query snapshot --format json &gt; snapshot.json\nhealth-inspector --input snapshot.json --output report.json\n \n# Report includes evidence\n{\n  &quot;finding_id&quot;: &quot;F-001&quot;,\n  &quot;assessment&quot;: &quot;5 tasks missing Context field&quot;,\n  &quot;evidence&quot;: [\n    {\n      &quot;item_id&quot;: &quot;KABSD-TSK-0042&quot;,\n      &quot;file&quot;: &quot;_kano/backlog/items/task/0000/KABSD-TSK-0042.md&quot;,\n      &quot;line_range&quot;: [25, 30]\n    }\n  ]\n}\nInspector Types (all external to core):\n\nHealth/Ideas: 3+3 questions, gap analysis, anti-patterns\nReviewer: Code review suggestions, best practices\nArchitect: Refactoring recommendations, design improvements\nSecurity: Threat model, vulnerability assessment\n\nKey: Inspectors are replaceable. Any agent can implement the contract. Core never hardcodes ‚Äúthis backlog is healthy/unhealthy‚Äù conclusions.\nSee ADR-0037 for full architecture.\nCommon Data Structures\n\nBacklogItem - Core work item model with frontmatter\nItemType - Enum: EPIC, FEATURE, USER_STORY, TASK, BUG\nItemState - Enum: Proposed, Planned, Ready, InProgress, Blocked, Done, Dropped\nChunkingOptions, TokenBudgetPolicy - Configuration for chunking system\n\nCommon Error Types\n\nItemNotFoundError - Item file doesn‚Äôt exist\nParseError - Invalid frontmatter or markdown\nValidationError - Data validation failed\nConfigError - Configuration error\nWriteError - Write operation failed\n\n\nRemember: This is a living document. Update it as patterns evolve in the codebase.\nCanonical + Adapters Architecture\n\n\n                  \n                  IMPORTANT\n                  \n                \n\nThis repo uses a ‚Äúcanonical source + adapters‚Äù layout to support multiple AI coding agents.\n\n\nCanonical Source (Single Source of Truth)\n\nAll skill documentation lives in: skills/&lt;skill-name&gt;/SKILL.md\nAlways read the canonical SKILL.md - adapters are just entry points\n\nAdapters (Entry Points for Different Agents)\n\nGitHub Copilot: .github/skills/&lt;skill-name&gt;/SKILL.md (thin wrapper with links to canonical)\nOpenAI Codex: .codex/skills/&lt;skill-name&gt;/SKILL.md (thin wrapper with name/description and links)\nAnthropic Claude: .claude/skills/&lt;skill-name&gt;/SKILL.md (compatible with Claude Code/Desktop)\nGoose: .goose/skills/&lt;skill-name&gt;/SKILL.md (open-source agent compatible with Claude skills)\nGoogle Antigravity: .agent/skills/&lt;skill-name&gt;/SKILL.md (native workspace skills)\nClaude Code: CLAUDE.md (root wrapper pointing back to this file)\nUniversal: AGENTS.md (this file) enforces workflow rules\nModular: Skills are self-contained in skills/ directory\n\nWorkflow Enforcement\n\nBefore using any skill: Open and read the canonical skills/&lt;skill-name&gt;/SKILL.md\nIf you only see a summary/wrapper, follow the links to canonical sections\nRun doctor or verification commands mentioned in canonical docs\n\nKey paths\n\nSkill (submodule): skills/kano-agent-backlog-skill/\n\nCanonical rules: skills/kano-agent-backlog-skill/SKILL.md ‚Üê READ THIS\nCopilot adapter: .github/skills/kano-agent-backlog-skill/SKILL.md\nCodex adapter: .codex/skills/kano-agent-backlog-skill/SKILL.md\nClaude adapter: .claude/skills/kano-agent-backlog-skill/SKILL.md\nGoose adapter: .goose/skills/kano-agent-backlog-skill/SKILL.md\nAntigravity adapter: .agent/skills/kano-agent-backlog-skill/SKILL.md\nReferences: skills/kano-agent-backlog-skill/references/\n\n\nSkill: skills/kano-commit-convention-skill/\n\nCanonical rules: skills/kano-commit-convention-skill/SKILL.md ‚Üê READ THIS\nCopilot adapter: .github/skills/kano-commit-convention-skill/SKILL.md\nCodex adapter: .codex/skills/kano-commit-convention-skill/SKILL.md\nClaude adapter: .claude/skills/kano-commit-convention-skill/SKILL.md\nGoose adapter: .goose/skills/kano-commit-convention-skill/SKILL.md\nAntigravity adapter: .agent/skills/kano-commit-convention-skill/SKILL.md\n\n\nUniversal Rules: AGENTS.md (this file)\nClaude Code: CLAUDE.md (root wrapper pointing to AGENTS.md)\nDemo backlog (system of record): _kano/backlog/\n\nItems: _kano/backlog/items/\nADRs: _kano/backlog/decisions/\nViews: _kano/backlog/products/&lt;product&gt;/views/\nTools (project-specific): _kano/backlog/tools/ (project-only views/dashboards)\n\n\n\nBacklog discipline (this repo)\n\nUse skills/kano-agent-backlog-skill/SKILL.md for any planning/backlog work.\nIf Python deps are missing, install them with python -m pip install -e skills/kano-agent-backlog-skill (add [dev] when developing the skill itself).\nBefore any code change, create/update items in _kano/backlog/items/ (Epic ‚Üí Feature ‚Üí UserStory ‚Üí Task/Bug).\n\n\n\n                  \n                  IMPORTANT\n                  \n                \n\nStrictly English Only: All backlog item content (Context, Goal, Approach, Worklog, etc.) MUST be written in English. This is a hard requirement for this demo to ensure accessibility for all agents.\n\n\n\nEnforce the Ready gate on Task/Bug (required, non-empty): Context, Goal, Approach, Acceptance Criteria, Risks / Dependencies.\nWorklog is append-only; never rewrite history. Append a Worklog line whenever:\n\na load-bearing decision is made,\nan item state changes,\nscope/approach changes,\nor an ADR is created/linked.\n\n\nUse python skills/kano-agent-backlog-skill/scripts/kano item update-state ... for state transitions so state, updated, and Worklog stay consistent.\nNeed a new backlog product? Run python skills/kano-agent-backlog-skill/scripts/kano backlog init --product &lt;name&gt; --agent &lt;id&gt; to scaffold _kano/backlog/products/&lt;name&gt;/ before creating items.\nFor backlog/skill file operations, go through the kano CLI so audit logs capture the action (no ad-hoc file edits).\nSkill scripts refuse paths outside _kano/backlog/ or _kano/backlog_sandbox/.\nKeep backlog volume under control: only open new items for code/design changes; keep Tasks/Bugs sized to one focused session; avoid ADRs unless there is a real architectural trade-off.\nTicketing threshold (agent-decided):\n\nOpen a new Task/Bug when you will change code/docs/views/scripts.\nOpen an ADR (and link it) when a real trade-off or direction change is decided.\nOtherwise, record the discussion in an existing Worklog; ask if unsure.\n\n\nState ownership: the agent decides when to move items to InProgress or Done; humans observe and can add context.\n\nNaming and storage rules (short)\n\nStore items under _kano/backlog/items/&lt;type&gt;/&lt;bucket&gt;/ and bucket per 100 (0000, 0100, ‚Ä¶).\nFilenames are stable: &lt;ID&gt;_&lt;slug&gt;.md (ASCII slug).\nFor Epics, create an adjacent &lt;ID&gt;_&lt;slug&gt;.index.md MOC and register it in _kano/backlog/_meta/indexes.md.\n\nViews (human-friendly)\n\nObsidian Dataview dashboards live under product view roots (e.g. _kano/backlog/products/&lt;product&gt;/views/Dashboard.md).\nGenerate the canonical dashboards via the CLI: python skills/kano-agent-backlog-skill/scripts/kano-backlog view refresh --agent &lt;id&gt; --backlog-root _kano/backlog --product &lt;name&gt;.\n\nNote: _kano/backlog/tools/*.sh are deprecated; use Python tools instead when needed (e.g. generate_demo_views.py, generate_focus_view.py).\n\n\n\nDemo principles\n\nKeep the demo backlog small and traceable; avoid ticket spam.\nAvoid unrelated refactors; every meaningful change should be explainable via a backlog item or ADR (with verification steps).\nIf you change the skill itself, commit inside the submodule skills/kano-agent-backlog-skill/ and update the parent repo submodule pointer.\nSelf-contained skill stance (this demo repo):\n\nPrefer adding automation as new kano subcommands so the skill is usable without manual setup.\nKeep _kano/backlog/tools/ for project-only dashboards/demos (wrapping skill scripts is OK when the behavior is demo-specific).\nOther projects may choose override-only usage; this repo does not. Treat the skill as the source of truth.\n\n\n\nTests\nNo tests or build steps are defined yet.\nTemporary Clause: Local-first First, No Server Implementation Yet\nEffective immediately, this project prioritizes local-first completion and hardening.\nAllowed (Encouraged)\n\nAny work that improves local-first workflows and quality, including:\n\nFile-based canonical data design, schema refinement, validation, and migration tooling\nLocal indexing/search (e.g., SQLite/FTS/sidecar ANN), ingest pipelines, and performance work\nCLI scripts, automation scripts, and developer tooling\nDocumentation, ADRs, threat models, and evaluations for future cloud/server support\nDesigning server interfaces (API/MCP schemas) as documentation/spec only\n\n\n\nNot Allowed (Hard Stop)\n\nDo not implement any server runtime or deployable server component, including but not limited to:\n\nHTTP server, REST API service, gRPC service\nMCP server (any transport)\nWeb UI that depends on a running server\nDocker/K8s deployment for a server component\nAuthentication/authorization implementation as runnable server code\n\n\nDo not add runtime dependencies whose primary purpose is server hosting (unless explicitly approved).\n\nRe-enabling Condition\n\nThis clause remains in effect until a human explicitly removes or disables it.\nAny request that appears to require server implementation must be treated as ‚Äúspec-only‚Äù and should produce:\n\nan ADR and/or design doc,\na roadmap ticket proposal,\na clear note that implementation is deferred due to this clause.\n\n\n\nRationale\n\nKeep the project focused on local-first stability and usability before expanding to cloud/multi-remote deployments.\n\n\nProject backlog discipline (kano-agent-backlog-skill)\n\nUse skills/kano-agent-backlog-skill/SKILL.md for any planning/backlog work.\nBacklog root is _kano/backlog_sandbox/_tmp_tests/guide_test_backlog (items are file-first; index/logs are derived).\nBefore any code change, create/update items in _kano/backlog_sandbox/_tmp_tests/guide_test_backlog/items/ (Epic ‚Üí Feature ‚Üí UserStory ‚Üí Task/Bug).\nEnforce the Ready gate on Task/Bug before starting; Worklog is append-only.\nUse the kano CLI (not ad-hoc edits) so audit logs capture actions:\n\nBootstrap: python skills/kano-agent-backlog-skill/scripts/kano backlog init --product &lt;name&gt; --agent &lt;agent-name&gt;\nCreate/update: python skills/kano-agent-backlog-skill/scripts/kano item create|update-state ... --agent &lt;agent-name&gt;\nViews: python skills/kano-agent-backlog-skill/scripts/kano view refresh --agent &lt;agent-name&gt; --product &lt;name&gt;\n\n\nDashboards auto-refresh after item changes by default (views.auto_refresh=true); use --no-refresh or set it to false if needed.\n\n"},"demo/CLAUDE":{"title":"CLAUDE","links":[],"tags":[],"content":"CLAUDE.md\n\n\n                  \n                  IMPORTANT\n                  \n                \n\nThis repo uses a ‚Äúcanonical source + adapters‚Äù layout.\n\n\nüìñ Global Instructions\nFor project-wide rules, architecture, and backlog discipline, please READ AND FOLLOW:\r\nüëâ @AGENTS.md\nüì¶ Agent Skills\nThis repo provides specialized skills in the following locations. Always read the canonical source within each skill folder:\n\nBacklog Management: @skills/kano-agent-backlog-skill/SKILL.md\nCommit Conventions: @skills/kano-commit-convention-skill/SKILL.md\n\n\nüõ†Ô∏è Quick Start for Claude\n\nExplore the codebase: Read @AGENTS.md for terminal commands and style guides.\nBacklog first: Before taking any action, check the backlog in _kano/backlog/.\nCanonical skills: Refer to the skills/ directory for tool-specific instructions (e.g., @skills/kano-agent-backlog-skill/SKILL.md).\n"},"demo/README":{"title":"README","links":["demo/AGENTS","demo/CLAUDE"],"tags":[],"content":"kano-agent-backlog-skill-demo\n\r\n\r\n\r\n\n\nAI Agent Skills for Spec-Driven Agentic Programming | File-based backlog management | Multi-agent collaboration | Local-first architecture\n\n‚ö†Ô∏è VERSION 0.0.2 - TOPICS + EMBEDDING PIPELINE FOUNDATIONS ‚ö†Ô∏è\nThis is the 0.0.2 release of the kano-agent-backlog-skill-demo - an experimental local-first, file-based backlog management system for AI agent collaboration.\nIMPORTANT DISCLAIMERS:\n\nüöß Rapid Development: System architecture is changing frequently\n‚ö° Breaking Changes: APIs, file formats, and workflows may change without notice\nüî¨ Experimental: Many features are incomplete or unstable\n‚ùå No Guarantees: No stability, compatibility, or support guarantees\nüìù Documentation Lag: Documentation may not reflect current implementation\n\nWhat‚Äôs New in 0.0.2:\n\n‚úÖ Core backlog item management (Epic, Feature, UserStory, Task, Bug)\n‚úÖ Workset execution cache for per-item context\n‚úÖ Topic-based context switching and grouping\n‚úÖ Topic templates, cross-references, snapshots, and merge/split operations\n‚úÖ Code snippet collection in topic materials\n‚úÖ Deterministic brief generation from materials\n‚úÖ ADR (Architecture Decision Record) support\n‚úÖ Multi-agent coordination (Canonical + Adapters architecture)\n‚úÖ Native support for Copilot, Codex, Claude, and Goose\n‚úÖ CLI commands for all core operations\n‚úÖ Property-based testing with Hypothesis\nüöß SQLite indexing (experimental)\nüöß Embedding search foundations (cross-lingual requirement, per-model index strategy)\n\nOverview\nCurrent Status: Version 0.0.2\nThis repository demonstrates an evolving approach to transform agent collaboration into a durable, auditable backlog system. The core concept is to persist planning, decisions, and work items as structured markdown files rather than losing context in chat conversations.\nWhat‚Äôs Working in 0.0.2:\n\n‚úÖ Markdown-based work item storage with frontmatter metadata\n‚úÖ CLI scripts for item creation, state transitions, and worklog management\n‚úÖ Workset execution cache for per-item context isolation\n‚úÖ Topic-based context switching with code snippet collection\n‚úÖ Deterministic brief generation from collected materials\n‚úÖ Plain markdown and Obsidian Dataview dashboard generation\n‚úÖ Multi-agent coordination through shared backlog\n‚úÖ ADR (Architecture Decision Record) workflow\n‚úÖ Property-based testing with Hypothesis\n\nWhat‚Äôs Unstable/Incomplete:\n\n‚ö†Ô∏è File formats and schemas (may change)\n‚ö†Ô∏è CLI interfaces and commands (evolving)\n‚ö†Ô∏è Configuration system (in flux)\n‚ö†Ô∏è SQLite indexing (experimental)\n‚ö†Ô∏è Embedding search (experimental)\n‚ö†Ô∏è Documentation accuracy (catching up)\n\nKey Features\n\nLocal-first backlog: All work items stored as markdown files with frontmatter metadata\nHierarchical work items: Epic ‚Üí Feature ‚Üí User Story ‚Üí Task/Bug\nAppend-only worklog: Auditable decision trail for each work item\nArchitecture Decision Records (ADRs): Capture significant technical decisions\nWorkset execution cache: Per-item working context with plan, notes, and deliverables to prevent agent drift\nTopic-based context switching: Group related items and documents for rapid focus area changes\nCode snippet collection: Capture and organize code references in topic materials\nDeterministic distillation: Generate briefs from collected materials for consistent context loading\nMultiple views: Obsidian Dataview dashboards and plain markdown reports\nMulti-product support: Organize backlogs for different products/projects\nMulti-agent coordination: Canonical + Adapters layout for shared backlog\nBroad compatibility: Support for Copilot, Codex, Claude, Goose, and Antigravity\nüöß WIP: Optional SQLite index - Fast queries while keeping files as source of truth\nüöß WIP: Embedding search - Local semantic search for backlog items (experimental)\n\nRepository Structure\n‚îú‚îÄ‚îÄ _kano/backlog/              # Main backlog directory (system of record)\r\n‚îú‚îÄ‚îÄ skills/                     # Canonical sources (git submodules or local)\r\n‚îÇ   ‚îú‚îÄ‚îÄ kano-agent-backlog-skill/         # **CANONICAL** (single source of truth)\r\n‚îÇ   ‚îî‚îÄ‚îÄ kano-commit-convention-skill/\r\n‚îú‚îÄ‚îÄ .github/skills/             # GitHub Copilot adapters\r\n‚îú‚îÄ‚îÄ .codex/skills/              # OpenAI Codex adapters\r\n‚îú‚îÄ‚îÄ .claude/skills/             # Anthropic Claude adapters\r\n‚îú‚îÄ‚îÄ .goose/skills/              # Goose adapters\r\n‚îú‚îÄ‚îÄ .agent/skills/              # Google Antigravity adapters\r\n‚îú‚îÄ‚îÄ AGENTS.md                   # Universal guidelines and workflow rules\r\n‚îú‚îÄ‚îÄ CLAUDE.md                   # Claude Code root adapter (points to AGENTS.md)\r\n‚îî‚îÄ‚îÄ README.md                   # This file\n\nMulti-Agent Architecture: Canonical + Adapters\nThis repository uses a ‚Äúcanonical source + adapters‚Äù layout to maintain a single source of truth while supporting mission-critical directories for different agents.\n\nCanonical Source: Full documentation and logic stay in skills/&lt;skill-name&gt;/SKILL.md.\nAdapters: Lightweight ‚Äúthin wrappers‚Äù exist in agent-specific folders (like .github/skills/ or .claude/skills/) that point back to the canonical source via links or @ references.\nUniversal Rules: AGENTS.md defines project-wide workflow rules and discipline for all agents.\nEntry Points:\n\nClaude Code: Uses CLAUDE.md as a root entrance to AGENTS.md.\nOther Agents: Use their respective .folder/skills/ adapters.\n\n\n\nSee AGENTS.md for detailed workflow enforcement rules.\nSkill Architecture: Self-Contained Design\n\nAll source code (domain library + CLI) lives under src/\nAll dependencies are unified in pyproject.toml\nThe entire skills/kano-agent-backlog-skill/ directory can be copied to any project (or used as a git submodule)\nNo external dependencies on kano-backlog-core or kano-cli projects\n\nThis follows the ‚ÄúSelf-contained skill stance‚Äù principle defined in AGENTS.md: keep all automation and tools needed to use the skill within the skill directory itself, avoiding scattered dependencies.\nGetting Started (Experimental)\n‚ö†Ô∏è WARNING: This is designed for AI agent automation, not manual operation.\nPrerequisites\n\nAI agent with file system access (Amazon Kiro, Claude, ChatGPT, Cursor, Windsurf, etc.)\nPython 3.10+ (required by skills/kano-agent-backlog-skill/pyproject.toml)\nGit (for version control and submodules)\nPatience: Expect things to break or change\n\nQuick Start\n1. Clone the repository:\ngit clone github.com/dorgonman/kano-agent-backlog-skill-demo.git\ncd kano-agent-backlog-skill-demo\n2. Initialize submodules:\ngit submodule update --init --recursive\n3. Install the skill:\npython -m pip install -e skills/kano-agent-backlog-skill\n4. Verify installation:\nkano-backlog --help\n5. Explore the demo backlog:\n# List work items\nkano-backlog item list --product kano-agent-backlog-skill\n \n# View dashboard\nkano-backlog view refresh --product kano-agent-backlog-skill --agent demo\n \n# List topics\nkano-backlog topic list\n \n# List worksets\nkano-backlog workset list\nOptional: Dev Dependencies\nFor development or embedding search features:\npython -m pip install -e &quot;skills/kano-agent-backlog-skill[dev]&quot;\nNote: FAISS and sentence-transformers may require platform-specific installation.\nPrerequisite install (recommended)\nInstall the skill (and its CLI dependencies) into your environment once:\npython -m pip install -e skills/kano-agent-backlog-skill\nOptional dev/embedding dependencies can be added with extras:\npython -m pip install -e &quot;skills/kano-agent-backlog-skill[dev]&quot;\n# Install FAISS / sentence-transformers manually per platform before running embedding workflows.\nVerify Installation\nAfter installation, verify the CLI is available:\nkano-backlog --help\nkano-backlog workset --help\nkano-backlog topic --help\nAgent-First Setup\nInstead of manual installation, ask your AI agent to:\n&quot;Please help me set up the kano-agent-backlog-skill demo. \r\nInitialize the backlog structure, and show me what work items exist.&quot;\n\nThe agent should automatically:\n\nInitialize submodules if needed\nExplore the backlog structure\nShow you available work items\nGenerate current dashboard views\n\nIf something breaks, just ask:\n&quot;The backlog setup failed. Please check what went wrong and fix it.&quot;\n\nAgent Workflow (Chat-Driven)\nThis system is designed for conversational agent interaction, not manual commands.\nStarting a New Work Session\nAsk your agent:\n&quot;Please check the backlog and pick a ready task for me to work on. \r\nCreate the work item if needed and start working on it.&quot;\n\nThe agent should:\n\nScan available work items\nFind or create a suitable task\nUpdate the item to ‚ÄúInProgress‚Äù\nBegin implementation\nLog decisions in the worklog\n\nCreating New Work Items\nInstead of manual scripts, just say:\n&quot;I need to add a new feature for user authentication. \r\nPlease create the appropriate backlog items and start planning.&quot;\n\nOr for bugs:\n&quot;There&#039;s a bug in the login system - users can&#039;t reset passwords. \r\nPlease create a bug item and investigate the issue.&quot;\n\nChecking Progress\nAsk for status updates:\n&quot;Show me the current backlog status and what&#039;s in progress.&quot;\n\nOr:\n&quot;What work items are ready to be picked up?&quot;\n\nCompleting Work\nWhen done:\n&quot;I&#039;ve finished the authentication feature. Please update the backlog \r\nand mark the work item as complete.&quot;\n\n‚ö†Ô∏è Note: Script interfaces change frequently. Let the agent handle the technical details.\nBacklog Discipline\nThis demo follows these principles:\n\nReady gate enforcement: Tasks/Bugs must have all required fields before starting\nAppend-only Worklog: Never rewrite history; append new entries\nControlled volume: Only open items for actual code/design changes\nSized work items: Tasks/Bugs should fit in one focused session\nADRs for trade-offs: Only create ADRs when there‚Äôs a real architectural decision\n\nViewing the Backlog\nAsk your agent to show you the current state:\n&quot;Please show me the current backlog dashboard and highlight \r\nwhat needs attention.&quot;\n\nOr for specific views:\n&quot;What work items are currently in progress?&quot;\r\n&quot;Show me all completed work from this week.&quot;\r\n&quot;What new tasks are ready to be picked up?&quot;\n\nGenerated Views (Agent-Managed)\nThe agent automatically maintains views under product roots (e.g. _kano/backlog/products/&lt;product&gt;/views/):\n\nActive work dashboard\nNew items queue\nCompleted work history\n\nObsidian Integration (Optional)\nIf you use Obsidian, ask:\n&quot;Please set up this backlog for Obsidian Dataview integration.&quot;\n\nWork Item Types\n\nEpic: Large initiative spanning multiple features (e.g., ‚ÄúMilestone 0.0.2‚Äù)\nFeature: Cohesive capability (e.g., ‚ÄúLocal-first backlog system‚Äù)\nUser Story: User-facing functionality (e.g., ‚ÄúPlan before code‚Äù)\nTask: Technical work item (e.g., ‚ÄúAdd test script‚Äù)\nBug: Defect to be fixed\n\nWorkset and Topic Features\nThe backlog system includes two powerful features for managing agent execution context and preventing drift during complex work sessions.\nWorksets: Per-Item Execution Cache\nWorksets provide a focused, per-item execution context that prevents agent drift during task work. Each workset is a materialized cache bundle stored in _kano/backlog/.cache/worksets/items/&lt;item-id&gt;/ containing:\n\nplan.md: Checklist template derived from acceptance criteria\nnotes.md: Working notes with Decision: markers for ADR promotion\ndeliverables/: Staging area for work artifacts before promotion\nmeta.json: Metadata including agent, timestamps, TTL, and source references\n\nWorkset Commands\nInitialize a workset:\nkano-backlog workset init --item TASK-0042 --agent my-agent --ttl-hours 72\nGet next action from plan:\nkano-backlog workset next --item TASK-0042\nThis command is the workset‚Äôs ‚Äúkeep me on track‚Äù primitive:\n\nIt reads the workset‚Äôs plan.md (a checklist derived from the item‚Äôs Acceptance Criteria).\nIt returns the first unchecked checkbox item (- [ ] ...).\nIf everything is checked, it prints a completion message.\n\nIt does not automatically mark anything as done; you check items off in plan.md as you complete them, then run kano-backlog workset next again to get the next step. Use --format json if you want structured output for tooling/agent automation.\nRefresh from canonical files:\nkano-backlog workset refresh --item TASK-0042 --agent my-agent\nPromote deliverables to canonical artifacts:\nkano-backlog workset promote --item TASK-0042 --agent my-agent\n# Dry run to preview\nkano-backlog workset promote --item TASK-0042 --agent my-agent --dry-run\nDetect ADR candidates in notes:\nkano-backlog workset detect-adr --item TASK-0042\nList all worksets:\nkano-backlog workset list\nCleanup expired worksets:\nkano-backlog workset cleanup --ttl-hours 72\n# Dry run to preview\nkano-backlog workset cleanup --ttl-hours 72 --dry-run\nAgent Workflow with Worksets\nAsk your agent:\n&quot;Initialize a workset for TASK-0042 and show me the next action to take.&quot;\n\nThe agent will:\n\nCreate a workset directory with plan, notes, and deliverables\nParse acceptance criteria into a checklist in plan.md\nTrack progress through the checklist\nCapture decisions with Decision: markers in notes.md\nStage work artifacts in deliverables/\nPromote deliverables back to canonical artifacts when done\n\nExample conversation:\nUser: &quot;Start working on TASK-0042&quot;\r\nAgent: &quot;I&#039;ll initialize a workset and begin. The first step is...&quot;\r\n\r\nUser: &quot;What&#039;s next?&quot;\r\nAgent: &quot;According to the plan, the next unchecked step is...&quot;\r\n\r\nUser: &quot;I&#039;ve completed the implementation&quot;\r\nAgent: &quot;Great! I&#039;ll promote the deliverables to the canonical artifacts directory.&quot;\n\nTopics: Context Switching and Grouping\nTopics provide a higher-level grouping mechanism for related items and documents, enabling rapid context switching when focus areas change during a conversation.\nCurrent implementation (see _kano/backlog/products/kano-agent-backlog-skill/items/task/0100/KABSD-TSK-0190_topic-lifecycle-materials-buffer-workset-merge.md) stores topics in _kano/backlog/topics/&lt;topic&gt;/ so the deterministic brief.md can be shared/reviewed in-repo, while keeping the per-agent active-topic pointer in cache.\n\nmanifest.json: Topic metadata, seed items, pinned documents\nbrief.md: Stable, human-maintained brief (do not overwrite automatically)\nbrief.generated.md: Deterministic distilled brief (generated/overwritten by kano-backlog topic distill)\nsynthesis/: Working outputs for distillation (derived)\npublish/: Prepared write-backs / patch skeletons (derived)\nmaterials/: Raw collected materials (snippets, links, extracts, logs)\n\nclips/: Code snippet references with line ranges\nlinks/: External document references\nextracts/: Cached text extracts\nlogs/: Optional collected logs\n\n\n\nNotes:\n\nRaw materials are treated as cache: by default this repo ignores _kano/backlog/topics/**/materials/ via .gitignore.\nActive topic for an agent is stored under _kano/backlog/.cache/worksets/active_topic.&lt;agent&gt;.txt.\n\nTopic Commands\nCreate a topic:\nkano-backlog topic create auth-refactor --agent my-agent\nAdd items to topic:\nkano-backlog topic add auth-refactor --item TASK-0042\nkano-backlog topic add auth-refactor --item TASK-0043\nPin documents for context:\nkano-backlog topic pin auth-refactor --doc _kano/backlog/decisions/ADR-0005-auth-strategy.md\nAdd code snippets to materials:\nkano-backlog topic add-snippet auth-refactor --file src/auth.py --start 10 --end 25\nDistill materials into brief:\nkano-backlog topic distill auth-refactor\nAudit decision write-back (writes a report into topic publish/):\nkano-backlog topic decision-audit auth-refactor\nkano-backlog topic decision-audit auth-refactor --format json\nWrite back a decision to a work item:\nkano-backlog workitem add-decision KABSD-TSK-0001 \\\n  --decision &quot;Use X over Y because ...&quot; \\\n  --source &quot;_kano/backlog/topics/auth-refactor/synthesis/decision-notes.md&quot; \\\n  --agent my-agent \\\n  --product kano-agent-backlog-skill\nSwitch active topic:\nkano-backlog topic switch auth-refactor --agent my-agent\nExport context bundle:\nkano-backlog topic export-context auth-refactor --format markdown\nkano-backlog topic export-context auth-refactor --format json\nList all topics:\nkano-backlog topic list --agent my-agent\nClose a topic (enables TTL cleanup):\nkano-backlog topic close auth-refactor --agent my-agent\nCleanup closed topics:\nkano-backlog topic cleanup --ttl-days 14 --apply\n# Dry run to preview\nkano-backlog topic cleanup --ttl-days 14\nAgent Workflow with Topics\nAsk your agent:\n&quot;Create a topic for the authentication refactor work and add the related tasks.&quot;\n\nThe agent will:\n\nCreate a topic directory with manifest and materials structure\nAdd related backlog items to the topic‚Äôs seed_items\nPin relevant ADRs and design documents\nCollect code snippets into materials/clips/\nGenerate a distilled brief from collected materials\nSwitch to the topic when you change focus\nExport context bundles for loading into working memory\n\nExample conversation:\nUser: &quot;I&#039;m switching to work on the payment flow&quot;\r\nAgent: &quot;I&#039;ll create a topic and gather the relevant context...&quot;\r\nAgent: &quot;Topic &#039;payment-flow&#039; created with 3 tasks and 2 ADRs. Here&#039;s the brief...&quot;\r\n\r\nUser: &quot;Show me the authentication work&quot;\r\nAgent: &quot;Switching to topic &#039;auth-refactor&#039;... Here are the 5 tasks and relevant code snippets...&quot;\n\nTopic Lifecycle\n\nCreate: Initialize topic with manifest\nCollect: Add items, pin documents, collect code snippets\nDistill: Generate deterministic brief from materials\nSwitch: Change active topic for context switching\nExport: Generate context bundles for agent consumption\nClose: Mark topic as closed when work is complete\nCleanup: Remove raw materials after TTL (brief.md persists)\n\nWorksets vs Topics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeatureWorksetsTopicsScopeSingle itemMultiple items + documentsPurposeExecution cacheContext groupingLifetimeTask durationWork area durationContentPlan, notes, deliverablesItems, docs, snippets, briefUse CasePrevent drift during taskSwitch focus areasStorage.cache/worksets/items/topics/&lt;topic&gt;/ (materials ignored; active pointer in .cache/)\nConfiguration\nBacklog configuration is in _kano/backlog/_config/ (product-specific) or _kano/backlog/_shared/ (shared settings).\nContributing (Pre-Alpha)\nCurrent Status: This is version 0.0.2 - an experimental demo repository in rapid development.\nBefore Contributing:\n\nExpect frequent breaking changes\nCheck recent commits for current state\nUnderstand this is experimental software\nNo stability guarantees\n\nHow to Contribute:\n\nOpen issues for bugs/suggestions\nDiscuss major changes before implementing\nExpect your contributions may be refactored heavily\nFocus on core concepts rather than implementation details\n\nFor the main skill development, see kano-agent-backlog-skill (also experimental).\nRoadmap\nVersion 0.0.2 (Current):\n\n‚úÖ Core backlog management\n‚úÖ Workset and topic features\n‚úÖ Multi-agent collaboration patterns\n‚úÖ CLI commands\n‚úÖ Property-based testing\n\nFuture Versions:\n\nüîÆ Stable file format schema\nüîÆ Enhanced SQLite indexing\nüîÆ Production-ready embedding search\nüîÆ Web UI for backlog visualization\nüîÆ Git integration for version control\nüîÆ Export/import for backlog migration\nüîÆ Plugin system for custom workflows\nüîÆ Performance optimizations\n\nNote: Roadmap is subject to change based on experimentation and feedback.\nLicense\nMIT\nSee the individual skill repositories for license information.\nLearn More\n\nSkill Documentation: skills/kano-agent-backlog-skill/SKILL.md (after initializing submodules)\nAgent Guidelines: AGENTS.md\nExample Backlogs: Explore _kano/backlog/products/ for real-world examples\nADR Directory: _kano/backlog/decisions/ for architectural decisions\n\nTroubleshooting\nBacklog not found:\n\nEnsure you‚Äôre in the repository root\nCheck _kano/backlog/ directory exists\nInitialize a product: kano-backlog admin init --product my-product --agent my-agent\n\nAgent asks for help:\n\nPoint agent to AGENTS.md for guidelines\nReference skills/kano-agent-backlog-skill/SKILL.md for detailed instructions\nShow agent the example backlogs in _kano/backlog/products/\n\nFrequently Asked Questions\nQ: Is this production-ready?\r\nA: No. This is version 0.0.2 - experimental software. Use at your own risk.\nQ: Can I use this for my project?\r\nA: Yes, but expect breaking changes. Copy the skill directory or use as a git submodule.\nQ: How do I migrate my existing backlog?\r\nA: Migration tools are not yet available. Manual conversion required.\nQ: Does this work with [my favorite agent]?\r\nA: It should work with any agent that has file system access and can run CLI commands. Tested with Amazon Kiro, Claude, Cursor, Windsurf, and others.\nQ: Why file-based instead of a database?\r\nA: Files are human-readable, version-controllable, and don‚Äôt require a server. Optional SQLite indexes provide query performance without lock-in.\nQ: Can multiple agents work on the same backlog simultaneously?\r\nA: Yes, but coordination is manual. File conflicts may occur. Use git for version control and conflict resolution.\nPhilosophy\nThis experimental demo explores a ‚Äúbacklog-first‚Äù approach where:\n\nPlanning before coding: Work items are created and refined before implementation begins\nDurable context: Planning and decisions persist in files, not lost in chat conversations\nAuditable trail: Every change is logged with timestamps and agent identity\nRecoverable state: Any agent can pick up where another left off\nHuman-readable source of truth: Markdown files are the canonical source, not databases\nOptional indexes: SQLite and embedding indexes enable powerful queries without lock-in\nMulti-agent coordination: Shared backlog enables multiple agents to collaborate effectively\nContext isolation: Worksets prevent drift during complex tasks\nRapid context switching: Topics enable quick focus area changes\n\nVersion 0.0.2 Status: These principles are being tested and refined. Implementation is evolving but demonstrates the core concepts in action. The _kano/backlog/ directory contains real-world examples of this philosophy applied to the development of the system itself.\nDual-Readability Design (Topic &amp; Snapshot)\nA key challenge in modern development is that Humans struggle with focus (hard to jump between contexts) while Agents struggle with collaboration (hard to share implicit context).\nThis project checks every artifact against a Dual-Readability Principle:\n\nHuman-Readable: High-level summaries, clear checklists, and ‚Äúmanager-friendly‚Äù reports (e.g., brief.md, Report_pm.md) so humans can make rapid decisions without reading code.\nAgent-Readable: Structural precision, file paths, line numbers, and explicit ‚ÄúNext Step‚Äù markers (e.g., manifest.json, stub_inventory) so agents can act without hallucinating.\n\nTopics and Snapshots are the concrete implementation of this philosophy:\n\nTopics: Allow humans to ‚Äúload‚Äù a focus area mentally in seconds, while giving Agents a precise list of files and snippets to load into their context window.\nSnapshots: Give humans a trustable ‚ÄúState of the Union‚Äù, while giving Agents a literal checklist of TODOs and NotImplementedErrors to attack next.\n\nThe Agent Semantic Memory Model\nTo help Agents (and humans) navigate the complexity of software development, this project maps its features to the standard cognitive memory model:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMemory TypeEquivalentPurposeLifespanAnalogyShort-Term(Working Memory)WorksetExecution StatePrevents drift during a single task. Contains the immediate plan, scratchpad notes, and temp files.Hours(Task duration)The ‚Äúscratchpad‚Äù or ‚ÄúRAM‚Äù cleared after use.Medium-Term(Contextual)TopicFocus ScopeGroups related files, specs, and definitions for a feature. Allows rapid context switching.Days/Weeks(Feature duration)The ‚Äúproject folder‚Äù on your desk.Long-Term(Semantic)ADR &amp; CanonicalInstitutional KnowledgeImmutable decisions (ADRs) and the source code itself. The ‚ÄúSoul‚Äù of the project.Permanent(Project duration)The ‚Äúlibrary archives‚Äù or ‚Äútextbooks‚Äù.\nWhy this matters:\n\nAgents often ‚Äúforget‚Äù instructions (Short-term failure). ‚Üí Solution: Worksets force them to read a plan.md.\nAgents often ‚Äúhallucinate‚Äù unrelated files (Medium-term failure). ‚Üí Solution: Topics bind them to a specific manifest.json.\nAgents often ‚Äúoverwrite‚Äù architectural rules (Long-term failure). ‚Üí Solution: ADRs provide immutable constraints.\n\nBuilt with Multi-Agent Collaboration\nThis project is itself a demonstration of multi-agent collaboration in action. The codebase has been developed through iterative collaboration with multiple AI coding assistants, each bringing unique strengths to the development process:\n\nCodex - Early prototyping and concept exploration\nGitHub Copilot - Code completion, suggestions, and inline assistance\nGoogle Antigravity - Alternative perspectives and design discussions\nAmazon Q - AWS infrastructure guidance and cloud architecture\nAmazon Kiro - Primary development agent and implementation\nCursor - Inline editing, refactoring, and code navigation\nWindsurf - Additional development support and testing\nOpenCode - Testing, validation, and quality assurance\n\nHow Multi-Agent Collaboration Works\nThe backlog system you see here was used to coordinate work across these agents, proving the concept through real-world usage:\n\nShared Backlog: All agents work from the same file-based backlog in _kano/backlog/\nWorklog Trail: Every decision and state change is recorded in append-only worklogs\nADR Documentation: Architectural decisions are captured in _kano/backlog/decisions/\nContext Switching: Topics enable agents to quickly load relevant context for different work areas\nWorkset Isolation: Per-item worksets prevent agents from drifting during complex tasks\n\nAgent Collaboration Patterns\nSequential Handoff:\nAgent A: Creates Epic and Feature items\r\nAgent B: Breaks down into User Stories\r\nAgent C: Implements Tasks with worksets\r\nAgent D: Reviews and updates worklogs\n\nParallel Work:\nAgent A: Works on TASK-0042 (auth module)\r\nAgent B: Works on TASK-0043 (payment module)\r\nBoth: Update shared backlog independently\r\nBoth: Coordinate through ADRs and worklogs\n\nContext Switching:\nAgent: Switches from &quot;auth-refactor&quot; topic to &quot;payment-flow&quot; topic\r\nAgent: Loads new context bundle with relevant items and documents\r\nAgent: Continues work with full context of new focus area\n\nWhy This Matters\nTraditional agent collaboration loses context in chat conversations. This system demonstrates:\n\nDurable Context: Planning and decisions persist in files, not chat\nAuditable Trail: Every change is logged with timestamps and agent identity\nRecoverable State: Any agent can pick up where another left off\nScalable Coordination: Multiple agents can work on different areas simultaneously\nHuman Oversight: Humans can review, approve, or redirect at any checkpoint\n\nEvery feature, decision, and trade-off in this codebase is documented in the _kano/backlog/ directory - a living example of the ‚Äúbacklog-first‚Äù philosophy in action."},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0001_backlog-structure-and-moc":{"title":"Backlog structure: per-type folders and Obsidian MOC","links":["items/task/0000/KABSD-TSK-0001_project-backlog-skill"],"tags":[],"content":"Decision\nAdopt per-type folders for backlog items and use Obsidian-style MOC (manual links),\r\nwith Dataview as a supplemental auto list. Keep index files at the Epic level\r\nonly to reduce file count. Bucket items by ID range (per 100) under each type\r\nto reduce large folder sizes.\nContext\nThe backlog system needs to be readable without heavy tooling and should render\r\nclearly in Obsidian. DataviewJS was not reliable in the current setup.\nLinks\n\nRelated: KABSD-TSK-0001 Create project-backlog skill\n\nOptions Considered\n\nFlat items/ folder + DataviewJS-only index\nPer-type folders + DataviewJS index\nPer-type folders + manual MOC links + Dataview (non-JS) lists\n\nPros / Cons\n\nOption 1: simple path; but JS rendering was unreliable.\nOption 2: clearer organization; still depends on DataviewJS.\nOption 3: stable MOC links (Graph-friendly) and optional Dataview lists; slightly more manual upkeep.\n\nConsequences\n\nEpic index files must be updated when items move or are renamed.\nFeature/UserStory rely on Epic MOC links instead of their own index files.\nItem files live under _kano/backlog/items/&lt;type&gt;/&lt;bucket&gt;/ to avoid large directories.\nGraph view will reflect MOC link structure.\n\nFollow-ups\n\nKeep skills/kano-agent-backlog-skill/SKILL.md aligned with this structure.\n"},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0002_decisions-as-adr-links":{"title":"Decision handling: ADRs stay in decisions/ with item links","links":["items/feature/0000/KABSD-FTR-0001_local-backlog-system"],"tags":[],"content":"Decision\nKeep decisions as ADR documents under _kano/backlog/decisions/ and link them\r\nfrom related work items via the decisions frontmatter field and a Links entry.\r\nDo not model ADRs as backlog work items for now.\nContext\nWe discussed whether decisions should be treated as work items. The current\r\nbacklog uses ADRs to capture durable rationale without turning them into\r\nworkflow tickets. We want to preserve clarity while avoiding ticket sprawl.\nLinks\n\nRelated: KABSD-FTR-0001 Local-first backlog system\n\nOptions Considered\n\nKeep ADRs as separate docs under decisions/ and link from items.\nTreat decisions as a new work item type under items/ with states.\nHybrid: keep ADRs under decisions/ plus optional lightweight decision tickets.\n\nPros / Cons\n\nOption 1: clear separation and low overhead; requires linking discipline.\nOption 2: full backlog visibility; risks turning decisions into ticket noise.\nOption 3: flexible; extra ceremony and more items to maintain.\n\nConsequences\n\nADRs remain outside the work item state machine.\nWork items should record ADR IDs in decisions: [] and Links.\nDashboards may need a separate decision view if visibility is required.\n\nFollow-ups\n\nUpdate related items to link ADR-0002.\n"},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0003-appendix_collision-report-cli-spec":{"title":"ADR-0003-appendix_collision-report-cli-spec","links":[],"tags":[],"content":"Collision Report &amp; Resolver CLI\nID collision reporting and resolver CLI tool specifications\nOverview\nProvides two tools:\n\nworkitem_collision_report.py - Scan and report display ID collisions\nworkitem_resolve_ref.py - Interactive reference resolution\n\nworkitem_collision_report.py\nFeatures\nScans all items under _kano/backlog/items/ to find duplicate display id cases.\nUsage\n# Basic report\npython scripts/backlog/workitem_collision_report.py\n \n# JSON output\npython scripts/backlog/workitem_collision_report.py --format json\n \n# Show collisions only\npython scripts/backlog/workitem_collision_report.py --collisions-only\n \n# Specify backlog path\npython scripts/backlog/workitem_collision_report.py --backlog-root _kano/backlog\nOutput example\nID Collision Report\r\n===================\r\nGenerated: 2026-01-06 01:30\r\nScanned: 85 items\r\n\r\nCollisions Found: 1\r\n\r\nID: KABSD-TSK-0100 (2 items)\r\n  1. 019473f2 | Task | Done  | First implementation | items/tasks/0000/...\r\n  2. 01947428 | Task | New   | Second attempt      | items/tasks/0000/...\r\n  Suggestion: Use KABSD-TSK-0100@019473f2 or KABSD-TSK-0100@01947428\r\n\r\nNo other collisions found.\n\nJSON output format\n{\n  &quot;generated&quot;: &quot;2026-01-06T01:30:00&quot;,\n  &quot;total_items&quot;: 85,\n  &quot;collision_count&quot;: 1,\n  &quot;collisions&quot;: [\n    {\n      &quot;id&quot;: &quot;KABSD-TSK-0100&quot;,\n      &quot;items&quot;: [\n        {\n          &quot;uid&quot;: &quot;019473f2-79b0-7cc3-98c4-dc0c0c07398f&quot;,\n          &quot;uidshort&quot;: &quot;019473f2&quot;,\n          &quot;type&quot;: &quot;Task&quot;,\n          &quot;state&quot;: &quot;Done&quot;,\n          &quot;title&quot;: &quot;First implementation&quot;,\n          &quot;path&quot;: &quot;items/tasks/0000/KABSD-TSK-0100_first-impl.md&quot;\n        },\n        {\n          &quot;uid&quot;: &quot;01947428-1234-7abc-5678-def012345678&quot;,\n          &quot;uidshort&quot;: &quot;01947428&quot;,\n          &quot;type&quot;: &quot;Task&quot;,\n          &quot;state&quot;: &quot;New&quot;,\n          &quot;title&quot;: &quot;Second attempt&quot;,\n          &quot;path&quot;: &quot;items/tasks/0000/KABSD-TSK-0100_second-attempt.md&quot;\n        }\n      ]\n    }\n  ]\n}\nworkitem_resolve_ref.py\nFeatures\nResolves reference strings with interactive disambiguation support.\nUsage\n# Resolve reference\npython scripts/backlog/workitem_resolve_ref.py KABSD-TSK-0059\n \n# Use uidshort for exact match\npython scripts/backlog/workitem_resolve_ref.py KABSD-TSK-0100@019473f2\n \n# Interactive mode\npython scripts/backlog/workitem_resolve_ref.py KABSD-TSK-0100 --interactive\n \n# Output format\npython scripts/backlog/workitem_resolve_ref.py KABSD-TSK-0059 --format path   # path only\npython scripts/backlog/workitem_resolve_ref.py KABSD-TSK-0059 --format json   # JSON format\npython scripts/backlog/workitem_resolve_ref.py KABSD-TSK-0059 --format uid    # uid only\nOutput examples\nUnique match:\nResolved: KABSD-TSK-0059\r\n\r\n  UID:      019473f2-79b0-7cc3-98c4-dc0c0c07398f\r\n  ID:       KABSD-TSK-0059\r\n  Type:     Task\r\n  State:    Done\r\n  Title:    ULID vs UUIDv7 comparison document\r\n  Path:     items/tasks/0000/KABSD-TSK-0059_ulid-vs-uuidv7-comparison.md\r\n  Created:  2026-01-06\r\n  Updated:  2026-01-06\n\nMultiple matches (interactive mode):\nAmbiguous: 2 items match &quot;KABSD-TSK-0100&quot;\r\n\r\n  # | UID (short) | Type | State | Title\r\n  --|-------------|------|-------|------\r\n  1 | 019473f2    | Task | Done  | First implementation\r\n  2 | 01947428    | Task | New   | Second attempt\r\n\r\nEnter number to select (or &#039;q&#039; to quit): 1\r\n\r\nSelected: KABSD-TSK-0100@019473f2\r\nPath: items/tasks/0000/KABSD-TSK-0100_first-impl.md\n\nImplementation notes\nShared module\n# scripts/backlog/lib/index.py\n \nclass BacklogIndex:\n    def __init__(self, backlog_root: str):\n        self.items = self._scan_items(backlog_root)\n        self._build_indexes()\n    \n    def get_by_uid(self, uid: str) -&gt; Optional[BacklogItem]: ...\n    def get_by_uidshort(self, prefix: str) -&gt; List[BacklogItem]: ...\n    def get_by_id(self, display_id: str) -&gt; List[BacklogItem]: ...\n    def get_collisions(self) -&gt; Dict[str, List[BacklogItem]]: ...\nIntegration with existing skill scripts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExisting scriptIntegration approachworkitem_update_state.pyUse resolve_ref to allow id@uidshort parametersworkitem_create.pyAuto-generate UUIDv7 uidview_generate.pyOptionally display uidshort\nDeliverables\n\n scripts/backlog/workitem_collision_report.py\n scripts/backlog/workitem_resolve_ref.py\n scripts/backlog/lib/index.py (shared module)\n Unit tests\n"},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0003-appendix_id-resolver-spec":{"title":"ADR-0003-appendix_id-resolver-spec","links":[],"tags":[],"content":"ID Resolver Specification\nResolveRef() function specification - for resolving backlog item references\nOverview\nPer ADR-0003, all reference resolution must go through the Resolver to support:\n\nFull uid exact match\nuidshort prefix match\nDisplay id match (may return multiple items)\n\nResolveRef() function specification\nSignature\ndef resolve_ref(\n    ref: str,\n    index: BacklogIndex,\n    interactive: bool = False\n) -&gt; ResolveResult:\n    &quot;&quot;&quot;\n    Resolve a reference to one or more backlog items.\n    \n    Args:\n        ref: Reference string (uid, uidshort, id, or id@uidshort format)\n        index: Backlog index instance\n        interactive: If True, prompt user for disambiguation\n        \n    Returns:\n        ResolveResult with matched item(s) or error\n    &quot;&quot;&quot;\nInput formats\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFormatExampleDescriptionFull uid019473f2-79b0-7cc3-98c4-dc0c0c07398f36 chars, hyphensuidshort019473f28 hex charsDisplay idKABSD-TSK-0059Project prefix + type + numberid@uidshortKABSD-TSK-0059@019473f2Human-friendly format\nResolution logic\ndef resolve_ref(ref: str, index: BacklogIndex) -&gt; ResolveResult:\n    # 1. Check if ref is full uid (36 chars with hyphens)\n    if is_full_uid(ref):\n        item = index.get_by_uid(ref)\n        if item:\n            return ResolveResult(matches=[item], exact=True)\n        return ResolveResult(error=f&quot;UID not found: {ref}&quot;)\n    \n    # 2. Check if ref contains @uidshort (e.g., KABSD-TSK-0059@019473f2)\n    if &quot;@&quot; in ref:\n        id_part, uidshort = ref.split(&quot;@&quot;, 1)\n        matches = index.get_by_id(id_part)\n        matches = [m for m in matches if m.uid.startswith(uidshort)]\n        if len(matches) == 1:\n            return ResolveResult(matches=matches, exact=True)\n        elif len(matches) &gt; 1:\n            return ResolveResult(matches=matches, exact=False, \n                error=&quot;Multiple matches even with uidshort&quot;)\n        return ResolveResult(error=f&quot;No match for {ref}&quot;)\n    \n    # 3. Check if ref is uidshort (8 hex chars)\n    if is_uidshort(ref):\n        matches = index.get_by_uidshort(ref)\n        if len(matches) == 1:\n            return ResolveResult(matches=matches, exact=True)\n        elif len(matches) &gt; 1:\n            return ResolveResult(matches=matches, exact=False)\n        return ResolveResult(error=f&quot;uidshort not found: {ref}&quot;)\n    \n    # 4. Assume ref is display id\n    matches = index.get_by_id(ref)\n    if len(matches) == 1:\n        return ResolveResult(matches=matches, exact=True)\n    elif len(matches) &gt; 1:\n        return ResolveResult(matches=matches, exact=False)\n    \n    return ResolveResult(error=f&quot;ID not found: {ref}&quot;)\nOutput structure\n@dataclass\nclass ResolveResult:\n    matches: List[BacklogItem] = field(default_factory=list)\n    exact: bool = False\n    error: Optional[str] = None\n    \n@dataclass\nclass BacklogItem:\n    uid: str\n    id: str\n    uidshort: str  # derived from uid[:8]\n    type: str\n    title: str\n    state: str\n    path: str\n    created: str\n    updated: str\nIndex requirements\nResolver requires the following index query capabilities:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQueryMethodDescriptionuid -&gt; itemget_by_uid(uid)Unique matchuidshort -&gt; [items]get_by_uidshort(prefix)Prefix matchid -&gt; [items]get_by_id(id)May return multiple items\nIndex Schema (SQLite)\nCREATE TABLE items (\n    uid TEXT PRIMARY KEY,\n    id TEXT NOT NULL,\n    uidshort TEXT NOT NULL,  -- first 8 hex chars\n    type TEXT,\n    title TEXT,\n    state TEXT,\n    path TEXT UNIQUE,\n    created TEXT,\n    updated TEXT\n);\n \nCREATE INDEX idx_id ON items(id);\nCREATE INDEX idx_uidshort ON items(uidshort);\nDisambiguation\nWhen exact=False and multiple matches exist, output candidate list:\nMultiple matches for &quot;KABSD-TSK-0100&quot;:\r\n\r\n  # | ID              | UID (short)  | Type | State | Title\r\n---------------------------------------------------------------\r\n  1 | KABSD-TSK-0100 | 019473f2     | Task | Done  | First task\r\n  2 | KABSD-TSK-0100 | 01947428     | Task | New   | Second task\r\n\r\nEnter number to select, or use: KABSD-TSK-0100@019473f2\n\nCLI integration\n# Resolve and show item details\npython scripts/backlog/workitem_resolve_ref.py KABSD-TSK-0059\n \n# Resolve with uidshort hint\npython scripts/backlog/workitem_resolve_ref.py KABSD-TSK-0100@019473f2\n \n# Interactive mode\npython scripts/backlog/workitem_resolve_ref.py KABSD-TSK-0100 --interactive\n \n# Output format\npython scripts/backlog/workitem_resolve_ref.py KABSD-TSK-0059 --format json\npython scripts/backlog/workitem_resolve_ref.py KABSD-TSK-0059 --format path\nError handling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaseError messageUID not foundError: UID not found: {uid}ID not foundError: ID not found: {id}Multiple matchesAmbiguous: {count} items match &quot;{ref}&quot;. Use id@uidshort format.Invalid formatError: Invalid reference format: {ref}"},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0003-appendix_migration-plan-uid":{"title":"ADR-0003-appendix_migration-plan-uid","links":[],"tags":[],"content":"Migration Plan: Add uid to Existing Items\nA migration plan to add a UUIDv7 uid field to existing backlog items.\nOverview\nExisting backlog items currently only have id (display ID). Per ADR-0003, each item needs a uid (UUIDv7) as the immutable primary key.\nKey Decisions (ADR-0003):\n\nuid format: UUIDv7 (RFC 9562)\nFilenames remain unchanged (&lt;id&gt;_&lt;slug&gt;.md)\nuid is added only in frontmatter\n\nMigration Steps\nPhase 1: Preparation\n\n\nBack up the current backlog\ncp -r _kano/backlog _kano/backlog_backup_$(date +%Y%m%d)\n\n\nEnsure a UUIDv7 library is available\n\nPython: uuid6 package or Python 3.12+ built-in\nInstall: pip install uuid6\n\n\n\nPhase 2: Migration script\nCreate scripts/backlog/migration_add_uid.py:\n#!/usr/bin/env python3\n&quot;&quot;&quot;\nAdd uid (UUIDv7) to existing backlog items.\n \nUsage:\n    python migration_add_uid.py --dry-run  # Preview changes\n    python migration_add_uid.py --apply    # Apply changes\n&quot;&quot;&quot;\nimport uuid6  # or uuid (Python 3.12+)\n \ndef generate_uid():\n    &quot;&quot;&quot;Generate a UUIDv7 string.&quot;&quot;&quot;\n    return str(uuid6.uuid7())\n \ndef extract_uidshort(uid: str, length: int = 8) -&gt; str:\n    &quot;&quot;&quot;Extract uidshort (first N hex chars, no hyphens).&quot;&quot;&quot;\n    return uid.replace(&quot;-&quot;, &quot;&quot;)[:length]\nScript capabilities:\n\nScan all .md files under _kano/backlog/items/\nParse frontmatter\nIf uid is missing, generate a UUIDv7 and add it\nUpdate the updated field\nWrite changes back to the file\n\nPhase 3: Handle parent/link references\nBackward-compatibility strategy (incremental):\n\nKeep the existing parent field (by display id)\nOptionally add parent_uid\nA Resolver tool maps parent to the actual uid\n\n# Before migration\nparent: KABSD-FTR-0042\n \n# After migration (backward compatible)\nparent: KABSD-FTR-0042\nparent_uid: 019473f2-79b0-7cc3-98c4-dc0c0c07398f  # optional\nPhase 4: Validation\n\nRun a verification script to ensure every item has a uid\nVerify uid format correctness (UUIDv7)\nVerify dashboards render correctly\n\nFrontmatter schema changes\nNew fields\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFieldTypeRequiredNotesuidstringRequired (post-migration)UUIDv7, immutableparent_uidstringOptionalParent item‚Äôs uidaliaseslistOptionalLegacy IDs or alternate names\nExample\n---\nid: KABSD-TSK-0059\nuid: 019473f2-79b0-7cc3-98c4-dc0c0c07398f\ntype: Task\ntitle: &quot;ULID vs UUIDv7 comparison document&quot;\nstate: Done\npriority: P3\nparent: KABSD-FTR-0042\nparent_uid: 019473e8-1234-7abc-5678-def012345678  # optional\n# ... rest of frontmatter\n---\nuidshort specification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPropertyValueLength8 charactersSourceFirst 8 hex characters of uid (no hyphens)Example019473f2UsageHuman-friendly reference KABSD-TSK-0059@019473f2\nRisks / Mitigations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRiskMitigationMigration failure causing data corruptionBack up first; provide a dry-run modeuid collisionsExtremely unlikely with UUIDv7; add validation checksTool incompatibilityBackward-compatible: keep parent, add parent_uid\nImplementation order\n\n Create migration_add_uid.py\n Test in sandbox\n Dry-run preview\n Back up and perform migration\n Verify results\n Update related tools to support uid resolution\n"},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0003-appendix_ulid-vs-uuidv7-comparison":{"title":"ADR-0003-appendix_ulid-vs-uuidv7-comparison","links":[],"tags":[],"content":"ULID vs UUIDv7 Comparison\nTechnical comparison to inform ADR-0003 uid format choice\nSummary\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristicULIDUUIDv7Length128 bits128 bitsString length26 chars (Base32)36 chars (hex + hyphens)Timestamp48 bits (ms)48 bits (ms)Random part80 bits74 bits (minus version/variant)StandardizationCommunity conventionIETF RFC 9562OrderingLexicographically sortableLexicographically sortableReadabilityShorter, no hyphensStandard UUID format\nDetailed comparison\n1. Format and readability\nULID\n01AN4Z07BY79KA1307SR9X4MV3\r\n|----------|----------------|\r\n Timestamp    Randomness\r\n  (10 ch)      (16 ch)\n\n\nUses Crockford‚Äôs Base32 (excludes I, L, O, U to avoid confusion)\nUppercase, no hyphens\n26 characters\n\nUUIDv7\n017f22e2-79b0-7cc3-98c4-dc0c0c07398f\r\n|-------|    |  |    |\r\n  time  ver  var  random\n\n\nStandard UUID format (8-4-4-4-12)\nHexadecimal with hyphens\n36 characters\n\n2. Ordering characteristics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAspectULIDUUIDv7Lexicographic ordering‚úÖ Fully supported‚úÖ Fully supportedSame-millisecond orderingMonotonic incrementCounter/randomCross-machine orderingTime precision onlyTime precision only\nBoth reflect time order correctly under lexicographic sort.\n3. Collision safety\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAspectULIDUUIDv7Random entropy80 bits~74 bitsSame-ms collision rate2^-802^-74Theoretical securityExtremely highExtremely high\nIn practice, both have negligible collision probability.\n4. Library support\nULID\n\nPython: python-ulid, ulid-py\nJavaScript: ulid (official)\nGo: oklog/ulid\nCommunity-driven, broad multi-language coverage\n\nUUIDv7\n\nPython: uuid6 (backport for &lt;3.x), built-in planned in Python 3.12+\nJavaScript: uuid@9+\nGo: google/uuid\nIETF standardized (RFC 9562); mainstream UUID libraries are adding support\n\n5. uidshort prefix length guidance\nULID\n\nFirst 10 characters = timestamp part\nRecommended uidshort: 8-10 characters (covers time + partial randomness)\nExample: 01AN4Z07BY ‚Üí 8 characters 01AN4Z07\n\nUUIDv7\n\nFirst 8 characters (no hyphens) = high bits of timestamp\nRecommended uidshort: 8-12 characters (hex)\nExample: 017f22e2-79b0-7... ‚Üí 8 characters 017f22e2\n\nRecommendation\nInitial analysis recommendation: ULID\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvantageNotesShorter26 vs 36 characters; cleaner filenamesMore readableNo hyphens; visually cleanerSufficient entropy80-bit randomness; extremely low collision riskMature ecosystemYears of use; stable library supportFilename-friendlyNo special characters; compatible across filesystems\nAdvantages of UUIDv7\n\nStandardization: IETF RFC 9562; strong long-term stability\nCompatibility: Works with existing UUID infrastructure (e.g., database UUID columns)\nFuture-proofing: Python 3.12+ and mainstream libraries add native support\n\n\nFinal decision (2026-01-06)\nAdopt UUIDv7\nChoose UUIDv7 for these reasons:\n\nIETF standardization provides stronger long-term stability\nCompatible with existing UUID ecosystems\nNative support in mainstream languages is arriving\n\nuidshort length: 8 characters (hex prefix)\n\nExample: 017f22e2-79b0-7... ‚Üí 017f22e2\n\nReferences\n\nULID Spec\nRFC 9562 - UUIDv7\npython-ulid\n"},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0003_identifier-strategy-for-local-first-backlog":{"title":"Identifier strategy: sortable IDs without centralized allocation","links":["demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0003-appendix_ulid-vs-uuidv7-comparison"],"tags":[],"content":"Decision\nKeep the backlog file-first (Markdown files in repo as the source of truth) and avoid requiring a\r\ncentralized server for identifier allocation.\nAdopt a hybrid identifier strategy:\n\nuid is the immutable primary key (globally unique). Format: UUIDv7 (36 chars, hex, 48-bit timestamp + 74-bit random, RFC 9562).\nid is a human-readable display ID (sortable, short), and may collide across machines/branches.\nFilenames must include uidshort to avoid git add/add conflicts when id collides.\nAny reference resolution using id must go through a resolver that can disambiguate.\n\nContext\nA strictly increasing counter (ID from 0..N) is convenient for sorting and scanning, but in a\r\nlocal-first workflow it becomes hard to guarantee uniqueness across machines/agents without either:\n\na centralized allocator (service/server/lock), or\ncoordination rules that prevent concurrent creation.\n\nUsing only UUIDs avoids collisions but loses the natural sortable sequence in filenames and dashboards.\r\nWe want to preserve ‚Äúhuman-first‚Äù readability while keeping future options open for distributed collaboration.\nRequirements\n\nMultiple agents/machines can create items concurrently without a shared allocator.\nReferences should remain stable across renames/renumbers and across derived indexes (SQLite, embeddings).\nAgents should be able to answer ‚Äúnext / what to do next‚Äù and support triage using indexes.\n\nOptions Considered\n\nCentralized ID allocator (server or shared lock file)\nUUID-only IDs (globally unique, not naturally ordered)\nTime-sortable unique IDs (ULID / UUIDv7)\nHybrid IDs: keep sortable id + add immutable uid\n\nPros / Cons\n\nOption 1: strongest uniqueness; introduces infrastructure and single-point-of-failure; violates local-first.\nOption 2: simplest uniqueness; hurts readability and natural ordering.\nOption 3: unique + sortable; still less human-friendly than short sequential IDs; ecosystem differences.\nOption 4: keeps human-friendly filenames while enabling reliable uniqueness for merging/indexing; adds complexity.\n\nConsequences\n\nThe default workflow remains file-first and Obsidian-friendly.\n\nWork item frontmatter (minimum)\nThis is the target schema for distributed-safe identifiers (migration required; not implemented everywhere yet):\n\nRequired:\n\nuid: string (ULID or UUIDv7; immutable; unique)\nid: string (display ID; sortable; allowed to collide)\ntype, title, status/state, priority, created, updated\n\n\nRecommended:\n\ntags\nparent_uid (references use uid to avoid ambiguity)\nlinks_uid (same)\naliases (optional; for legacy IDs or future renumbering)\n\n\n\nFilename and path\nTo avoid git add/add conflicts when two branches create the same display id, filenames should be unique.\nCurrent format (sufficient for most cases):\n\n&lt;id&gt;_&lt;slug&gt;.md\nExample: KABSD-TSK-0100_implement-backlog-indexing.md\n\nSince the slug is derived from the title and describes the item‚Äôs purpose, collision risk is minimal in practice. Two items would need identical id AND identical slug to conflict.\nOptional extended format (for high-concurrency scenarios):\n\n&lt;id&gt;__&lt;uidshort&gt;_&lt;slug&gt;.md\nExample: KABSD-TSK-0100__01KE72EH4N_implement-backlog-indexing.md\n\nuidshort is a stable prefix (fixed length) derived from uid:\n\nULID: prefix of the ULID (timestamp portion is convenient)\nUUIDv7: prefix of the hex string (length TBD, e.g. 8-12)\n\nDecision (2026-01-06): Filename format remains unchanged (&lt;id&gt;_&lt;slug&gt;.md). The uid field is added to frontmatter only; renaming existing files is not required.\nResolver semantics (reference handling)\nTools must implement ResolveRef(ref):\n\nIf ref is a full uid ‚Üí unique match.\nIf ref is a uidshort ‚Üí resolve via index (uidshort -&gt; uid). If multiple matches, list candidates.\nIf ref is a display id (e.g. KANO-000123) ‚Üí resolve via index (id -&gt; [uid...]):\n\none match: return it\nmultiple matches: list candidates for human selection (type/status/title/path/created/updated).\n\n\n\nRecommended human-friendly reference format to reduce ambiguity:\n\nKANO-000123@01KE72EH4N (display id + uidshort)\n\nIndex implications\nDerived indexes must support:\n\nuid -&gt; path\nuidshort -&gt; uid\nid -&gt; [uid...] (note: potentially multiple)\n\nWhat-next behavior (agent workflow)\nWhen asked ‚Äúnext / what to do next‚Äù:\n\nPrefer continuing items in progress.\nOtherwise pick from ready items (e.g. 3-5), ordered by priority then recency/parent context.\nOutput should include id@uidshort, title, type, status/state, priority, and a first actionable step.\n\nOpen Questions / Follow-ups\n\nChoose ULID vs UUIDv7 (sorting, readability, library support, collision safety, short-prefix length).\n\nResolved (2026-01-06): Use UUIDv7. See Comparison Document.\nRationale: IETF standardized (RFC 9562), native Python 3.12+ support planned, compatible with existing UUID infrastructure.\nuidshort length: 8 characters (hex prefix) recommended.\n\n\nMigration plan for existing id-only items (add uid to frontmatter, filenames unchanged).\nShould we store both parent (id) and parent_uid during migration, or hard cutover to uid?\nAdd a collision report (group by display id) and a resolver UI/CLI for disambiguation.\n"},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0004_file-first-architecture-with-sqlite-index":{"title":"File-First Architecture with SQLite Index","links":["demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0012_workset-db-canonical-schema-reuse","_meta/canonical_schema.sql","_meta/canonical_schema.json"],"tags":[],"content":"Decision\nUse a File-First architecture where Markdown files are the single source of truth, augmented by a local SQLite database acting as a disposable, read-optimized index.\n\nSource of Truth: Markdown files tracked in Git.\nDerived Index: Local SQLite database (_kano/backlog/_index/backlog.sqlite3).\nSync Direction: STRICTLY One-Way (File ‚Üí DB). The DB is never the System of Record.\nPersistence: The SQLite file is not tracked in Git (should be .gitignored). It can be rebuilt from files at any time.\n\nContext\nAs the backlog grows, scanning hundreds or thousands of Markdown files for every query (e.g., ‚Äúfind all active tasks blocking feature X‚Äù) becomes too slow (O(N) IO operations).\nHowever, we want to maintain the benefits of text files:\n\nGit-friendly: Diff, merge, blame work natively.\nHuman-readable: Accessible without special tools.\nPortable: No database server dependency for basic access.\n\nWe need a solution that provides relational query speed (O(log N)) without compromising the file-centric workflow.\nDetailed Design\n1. Index Lifecycle\nThe index is a cache of the file state.\n\nBuild: Scan all .md files, parse frontmatter, insert into DB.\nUpdate: Check file mtime against DB record. Only re-parse modified files.\nRebuild: rm backlog.sqlite3 followed by a full build ensures 100% consistency.\n\n2. Database Schema\nThe SQLite schema is designed for query efficiency, not normalization rules that would apply to a primary store.\nitems Table\nCore metadata for filtering and sorting.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumnTypeDescriptionuidTEXT (PK)UUIDv7 (from frontmatter)idTEXTDisplay ID (e.g., KABSD-TSK-0049)typeTEXTWork item type (Feature, Task, etc.)stateTEXTCurrent state (New, InProgress, etc.)titleTEXTItem titlepathTEXTRelative path to file (unique)mtimeREALFile modification timestamp (for sync logic)content_hashTEXTHash of content (for change detection)frontmatterJSONFull frontmatter blob (flexibility)createdTEXTCreation dateupdatedTEXTLast updated date\nlinks Table\nTracks relationships for graph queries (e.g., ‚Äúall children of X‚Äù).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumnTypeDescriptionsource_uidTEXTLink source (child)target_uidTEXTLink target (parent)typeTEXTLink type (e.g., ‚Äúparent‚Äù, ‚Äúrelates_to‚Äù)\nembeddings Table (Future/Optional)\nStores vector embeddings for semantic search.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumnTypeDescriptionuidTEXTFK to items.uidchunk_indexINTSequence number of chunkembeddingBLOBFloat32 vector arraycontentTEXTChunk text content\n3. Sync Logic\nA sync script (e.g., update_index.py) runs:\n\nBefore complex operations (e.g., generate_view).\nPeriodically (if running as a daemon/watcher).\nOn-demand by user.\n\nresolve_ref and other CLI tools currently use an in-memory index (lib/index.py). They should be refactored to query SQLite when available for better scaling.\nTrade-offs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrade-offDescriptionLatencyDB state may lag behind file state until sync runs. Tooling must handle ‚Äúdirty‚Äù reads or force sync.ComplexityMaintaining sync logic (especially incremental updates) adds code complexity vs raw file scan.SpaceDuplicates metadata in DB file (negligible for text backlogs).\nConsequences\n\nTooling Update: All queries (Dashboard generation, Reference resolution) should migrate to use SQLite for reads.\nGitignore: Ensure *.sqlite3 is ignored.\nResilience: Tools must degrade gracefully if DB is corrupt or missing (fallback to file scan or auto-rebuild).\nSchema Reuse: Per ADR-0012, this canonical schema is reused by workset DBs to avoid schema drift and maintain portable context.\n\nRelated\n\nCanonical Schema: See canonical_schema.sql and canonical_schema.json for the complete schema definition.\nWorkset Schema: See ADR-0012 for how workset DBs reuse this canonical schema.\n"},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0004_per-product-isolated-index-architecture":{"title":"Per-Product Isolated Index Architecture","links":["items/feature/0000/KABSD-FTR-0010_monorepo-platform-migration","items/task/0000/KABSD-TSK-0084_update-indexer-and-resolver-for-product-isolation"],"tags":[],"content":"Context\nIn a multi-product monorepo environment, we needed to decide between two architectural approaches for the SQLite index:\n\nPlatform-level shared index: All products write to a single _kano/backlog/_index/backlog.sqlite3\nPer-product isolated index: Each product owns its own index at products/&lt;name&gt;/_index/backlog.sqlite3\n\nDecision Criteria\nWe evaluated 7 dimensions:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDimensionPer-ProductPlatformWinnerIsolationComplete separationShared namespacePer-ProductConcurrencyNo locking contentionPotential locksPer-ProductPerformancePredictable, isolatedCan degrade with loadPer-ProductScalabilityLinear per productShared bottleneckPer-ProductComplexitySimple, independentComplex coordinationPer-ProductMaintenanceEasy (per-product)Harder (shared state)Per-ProductFuture ExtensibilitySupports embedding DBDifficult to addPer-Product\nOverall Score: Per-Product = 91%, Platform = 43%\nDecision\nImplement per-product isolated SQLite indexes.\nEach product will:\n\nMaintain its own SQLite database at products/&lt;product-name&gt;/_index/backlog.sqlite3\nHave independent schema with product column for self-documentation\nSupport autonomous indexing/rebuilding without affecting other products\nEnable future cross-product aggregation via product-aware queries\n\nImplementation Details\nSchema Design\nAll tables include product column:\nCREATE TABLE items (\n  product TEXT NOT NULL,\n  id TEXT NOT NULL,\n  PRIMARY KEY (product, id),\n  ...\n);\n \nCREATE TABLE item_tags (\n  product TEXT NOT NULL,\n  item_id TEXT NOT NULL,\n  tag TEXT NOT NULL,\n  PRIMARY KEY (product, item_id, tag),\n  FOREIGN KEY (product, item_id) REFERENCES items(product, id)\n);\nIndex Rebuild Process\n\nscripts/indexing/build_sqlite_index.py --product &lt;name&gt;\nScans products/&lt;name&gt;/items/ directory\nExtracts product from file paths\nUpserts into product-specific SQLite database\nMaintains composite key integrity\n\nResolver Behavior\n\nresolve_ref(ref, index, product=None) accepts optional product filter\nIf product not specified, searches across all products\nProduct column enables cross-product queries when needed\n\nRationale\n\nIsolation ensures safety: No possibility of data leakage between products\nPredictable performance: Each product‚Äôs index grows independently\nSupports autonomy: Teams can rebuild their product‚Äôs index without coordination\nFuture-proof: Enables embedding databases per product or shared aggregation\nSimpler mental model: Products are truly independent\nBackward compatible: Product column facilitates future migrations\n\nAlternatives Considered\nPlatform-level shared index\n\nPros: Unified search across all products\nCons: Complex coordination, shared bottleneck, potential data leakage\nRejected: Architecture does not support team autonomy\n\nHybrid approach (shared metadata + per-product data)\n\nPros: Balanced complexity\nCons: Still requires coordination layer, adds complexity\nRejected: Per-product isolation is simpler and cleaner\n\nFuture Work\nWhen cross-product features are needed:\n\nCreate aggregation layer on top of per-product indexes\nOr implement global embedding database that reads from per-product sources\nProduct column in schema already prepared for this extensibility\n\nReferences\n\nKABSD-FTR-0010_monorepo-platform-migration.md: Monorepo Platform Migration feature\nKABSD-TSK-0084_update-indexer-and-resolver-for-product-isolation.md: Indexer and resolver product isolation\n"},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0005_product-column-retention-rationale":{"title":"Product Column Retention in Per-Product Indexes","links":["items/feature/0000/KABSD-FTR-0010_monorepo-platform-migration","demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0004_file-first-architecture-with-sqlite-index"],"tags":[],"content":"Context\nWhen implementing per-product isolated indexes, a question arose: if each product owns its own SQLite database, why include a product column in the schema?\nArguments for removing it:\n\nRedundant (the file path already indicates product)\nAdds storage overhead\nColumn values are always the same for a given database\n\nArguments for keeping it:\n\nSelf-documentation (data independence from file paths)\nConsistency with composite key patterns\nFuture extensibility (global embedding DB, cross-product queries)\nError detection capability\n\nDecision\nRetain the product column in all schema tables.\nEach product‚Äôs schema will include a product column despite the apparent redundancy.\nRationale\n1. Data Self-Documentation\nThe product column makes data self-describing. If a database dump or export is created, the product context is explicit in the data itself, not dependent on file path or metadata.\nSELECT * FROM items WHERE product=&#039;kano-agent-backlog-skill&#039;;\n-- vs.\nSELECT * FROM items;  -- How do you know which product this is from?\n2. Consistency with Composite Key Strategy\nUsing (product, id) as composite primary keys throughout the schema ensures:\n\nAll foreign keys are uniform: FOREIGN KEY (product, item_id)\nConsistent pattern across all tables (items, item_tags, item_links, worklog_entries)\nEasier code generation and migrations\n\n3. Uniform Schema Across Products\nAll products use identical schema structure. A tool that works with KABSD‚Äôs index works with any product‚Äôs index without modification. This is valuable for:\n\nShared tool development\nSchema migration scripts\nIndex validation and repair tools\n\n4. Future Extensibility: Global Embedding Database\nThe primary long-term value is supporting a future global embedding database:\n-- Global embedding store (future)\nCREATE TABLE embeddings (\n  product TEXT NOT NULL,\n  item_id TEXT NOT NULL,\n  embedding BLOB NOT NULL,\n  PRIMARY KEY (product, item_id),\n  FOREIGN KEY (product, item_id) REFERENCES all_items(product, id)\n);\nThis would aggregate embeddings from all products while maintaining product context. The product column enables this without schema rework.\n5. Error Detection\nIncluding product column in per-product indexes enables validation queries:\n-- Verify database integrity\nSELECT DISTINCT product FROM items;\n-- Should always return single row: kano-agent-backlog-skill\n \n-- Detect misplaced files\nSELECT product FROM items WHERE product != &#039;kano-agent-backlog-skill&#039;;\n-- Should return empty set\n6. Cross-Product Queries (Future)\nWhen analytics or reporting tools are built, they may aggregate across products:\n-- Future: Query across products via union or aggregation\nSELECT product, COUNT(*) as item_count FROM all_items GROUP BY product;\nThe product column is essential for this without painful migrations.\nImplementation\nAll tables maintain the pattern:\nCREATE TABLE items (\n  product TEXT NOT NULL,\n  id TEXT NOT NULL,\n  uid UUID,\n  type TEXT,\n  -- ... other columns\n  PRIMARY KEY (product, id)\n);\n \nCREATE TABLE item_tags (\n  product TEXT NOT NULL,\n  item_id TEXT NOT NULL,\n  tag TEXT NOT NULL,\n  PRIMARY KEY (product, item_id, tag),\n  FOREIGN KEY (product, item_id) REFERENCES items(product, id)\n);\nAlternatives Considered\nRemove product column entirely\n\nPros: Minimal storage, no apparent redundancy\nCons: Breaks future embedding DB design, harder to validate integrity, poor data self-documentation\nRejected: Future extensibility cost is too high\n\nMake product optional/nullable\n\nPros: Allows querying ‚Äúwhich product‚Äù implicitly\nCons: Makes schema ambiguous, difficult to enforce consistency\nRejected: Product should always be explicit and required\n\nFuture Work\nWhen global embedding database is implemented:\n\nProduct column in schema already prepared\nNo schema migration required\nTools can immediately work with cross-product data\n\nReferences\n\nKABSD-FTR-0010_monorepo-platform-migration.md: Monorepo Platform Migration\nADR-0004_file-first-architecture-with-sqlite-index: Per-Product Isolated Index Architecture\n"},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0005_skill-versioning-and-release-policy":{"title":"Skill Versioning and Release Policy","links":[],"tags":[],"content":"Decision\nAdopt a SemVer-inspired versioning policy for kano-agent-backlog-skill with a clear pre-1.0 roadmap:\n\nUse Git tags as the source of truth for released versions: vX.Y.Z.\nWhile &lt;1.0.0, treat releases as milestones and allow faster iteration, but still:\n\nuse Z for bugfixes and non-breaking changes,\nuse Y when we introduce intentional breaking changes (schema/CLI/layout).\n\n\nAfter 1.0.0, follow SemVer strictly:\n\nZ patch = backward-compatible bugfix only\nY minor = backward-compatible feature + optional deprecations\nX major = breaking changes (must provide migration guidance)\n\n\n\nContext\nThis repo is a demo host and development environment for an open-source skill. We need a predictable way to:\n\ncommunicate what changed,\ndecide when changes are breaking,\nalign backlog milestones with releases,\nkeep multi-agent usage stable across time.\n\nDefinitions (what counts as breaking)\nBreaking changes include (non-exhaustive):\n\nFrontmatter schema: renaming/removing required keys, changing meaning of state groups, changing defaults that alter workflow rules.\nPath/layout: moving the canonical backlog root (_kano/backlog/**), changing bucket rules, changing decisions/items separation.\nScript CLI: removing/renaming flags, changing required flags, changing default behavior that affects output determinism.\nConfig schema: renaming/removing keys under _kano/backlog/_config/config.json.\nGenerated output contracts: changing canonical dashboard filenames or section/group meaning.\n\nNon-breaking changes include:\n\nadding optional keys or sections,\nadding new scripts (without changing existing CLI),\nstrengthening validation with clearer error messages (unless it blocks previously valid projects).\n\nRelease artifacts (minimum)\nFor each release tag:\n\nUpdate the skill docs (README/REFERENCE) to match reality.\nEnsure canonical scripts work end-to-end:\n\nscripts/backlog/view_refresh_dashboards.py\nscripts/backlog/view_generate_demo.py (demo dashboards)\nscripts/backlog/workitem_update_state.py\n\n\nEnsure demo views are regenerated.\n\nOptional (recommended as we approach 0.1.0+):\n\nCHANGELOG.md in the skill repo (high-level, human readable).\nA short ‚Äúupgrade notes‚Äù section when there is any migration required.\n\nMilestone mapping (demo backlog)\nWe track releases as milestone Epics:\n\nKABSD-EPIC-0002 = v0.0.1 (core demo)\nKABSD-EPIC-0003 = v0.0.2 (indexing + resolver)\n\nFuture guideline:\n\nPatch releases (v0.0.(Z+1)) do not require new Epics; they are small fixes folded into the current milestone Epic.\nMinor bump in pre-1.0 (v0.(Y+1).0) should have a dedicated milestone Epic if it introduces breaking changes.\n\nConsequences\n\nWe will treat ‚Äúschema/CLI/layout‚Äù changes as versioned contracts.\nEach ‚Äúmilestone epic‚Äù must have acceptance criteria that match a release outcome (taggable state).\nWhen breaking changes are introduced, the release must include clear migration guidance (script or documented steps).\n"},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0006_multi-product-directory-structure":{"title":"Multi-Product Directory Structure and Naming Conventions","links":["items/feature/0000/KABSD-FTR-0010_monorepo-platform-migration","items/task/0000/KABSD-TSK-0081_execute-directory-restructuring-for-monorepo-platform","demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0016_per-product-isolated-index-architecture"],"tags":[],"content":"Context\nA monorepo containing multiple independent products (skills) needs a consistent directory layout that:\n\nKeeps products isolated from each other\nSupports independent configuration and indexing\nAllows shared tools and metadata at the project level\nScales to many products without directory explosion\nMaintains backward compatibility during migration\n\nCompeting Designs\n\n\nProject + Products model (chosen):\n_kano/backlog/\r\n  products/&lt;product-name&gt;/\r\n    _config/\r\n    items/\r\n    decisions/\r\n    views/\r\n  _shared/\r\n    defaults.json\n\n\n\nFlat product namespacing:\n_kano/backlog/\r\n  items/&lt;product&gt;-&lt;type&gt;/\r\n  decisions/&lt;product&gt;/\r\n  views/&lt;product&gt;/\n\n\n\nSingle root (pre-migration):\n_kano/backlog/\r\n  items/\r\n  decisions/\r\n  views/\n\n\n\nDecision\nImplement Project + Products hierarchical model.\nDirectory structure:\n_kano/backlog/                          # Project root\r\n‚îú‚îÄ‚îÄ products/                           # Product container\r\n‚îÇ   ‚îú‚îÄ‚îÄ kano-agent-backlog-skill/       # First product\r\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ _config/\r\n‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ config.json             # Product-specific config\r\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ items/                      # Product&#039;s backlog items\r\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ epics/0000/\r\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ features/0000/\r\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ userstories/0000/\r\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tasks/0000/\r\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tasks/0100/             # Buckets per 100 items\r\n‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ bugs/0000/\r\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ decisions/                  # Product&#039;s ADRs\r\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ views/                      # Product&#039;s dashboards\r\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ _index/\r\n‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ backlog.sqlite3         # Product-isolated index\r\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ _meta/\r\n‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ schema.md\r\n‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ conventions.md\r\n‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ indexes.md              # Epic index registry\r\n‚îÇ   ‚îÇ\r\n‚îÇ   ‚îî‚îÄ‚îÄ kano-commit-convention-skill/   # Second product\r\n‚îÇ       ‚îî‚îÄ‚îÄ ... (same structure)\r\n‚îÇ\r\n‚îú‚îÄ‚îÄ sandboxes/                          # Isolated test/demo environments\r\n‚îÇ   ‚îú‚îÄ‚îÄ kano-agent-backlog-skill/       # Can test schema changes here\r\n‚îÇ   ‚îî‚îÄ‚îÄ kano-commit-convention-skill/\r\n‚îÇ\r\n‚îú‚îÄ‚îÄ _shared/                            # Project-level shared data\r\n‚îÇ   ‚îú‚îÄ‚îÄ defaults.json                   # { &quot;default_product&quot;: &quot;...&quot; }\r\n‚îÇ   ‚îî‚îÄ‚îÄ config_template.json            # Shared config seed\r\n‚îÇ\r\n‚îú‚îÄ‚îÄ _meta/                              # Project metadata (if needed)\r\n‚îú‚îÄ‚îÄ _index/                             # Project index (optional, future)\r\n‚îú‚îÄ‚îÄ views/                              # Project-level dashboards\r\n‚îÇ   ‚îú‚îÄ‚îÄ Dashboard_PlainMarkdown_Active.md\r\n‚îÇ   ‚îú‚îÄ‚îÄ Dashboard_PlainMarkdown_New.md\r\n‚îÇ   ‚îî‚îÄ‚îÄ Dashboard_PlainMarkdown_Done.md\r\n‚îî‚îÄ‚îÄ _logs/                              # Audit logs (project-level)\r\n    ‚îî‚îÄ‚îÄ agent_tools/\r\n        ‚îî‚îÄ‚îÄ tool_invocations.jsonl\n\nRationale\n1. Isolation and Autonomy\nProducts live under products/&lt;name&gt;/:\n\nTeam A manages products/product-a/\nTeam B manages products/product-b/\nNo namespace collisions, no coordination needed\nClear ownership boundaries\n\n2. Scalability\nHierarchical structure scales linearly:\n\n2 products: 2 directories\n10 products: 10 directories\n100 products: 100 directories\nNo explosion of files at top level\n\n3. Unified Schema Across Products\nEach product has identical internal structure:\n\nAll products use items/, decisions/, views/, _config/, _meta/\nTools can be generic: ‚Äúfor each product, scan items/‚Äù\nReduces special-case logic in scripts\n\n4. Backward Compatibility\nExisting KABSD backlog migrates as:\n\nOld: _kano/backlog/items/task/0000/KABSD-TSK-0007.md\nNew: _kano/backlog/products/kano-agent-backlog-skill/items/task/0000/KABSD-TSK-0007.md\n\nPath change is clean; no file modifications required. Git correctly tracks as renames.\n5. Per-Product Isolation in Indexing\nEach product gets:\n\nOwn SQLite database: products/&lt;name&gt;/_index/backlog.sqlite3\nOwn metadata: products/&lt;name&gt;/_meta/indexes.md\nOwn config: products/&lt;name&gt;/_config/config.json\n\nRebuild product A‚Äôs index without touching product B.\n6. Flexible Sandboxing\nsandboxes/&lt;product-name&gt;/ allows safe testing:\n\nDevelop schema changes on test data\nRun migration scripts without affecting production backlog\nEasy cleanup: just delete sandbox directory\n\n7. Project-Level Aggregation (Future)\n_shared/ and _index/ support future features:\n\nGlobal embedding database\nCross-product analytics dashboards\nUnified search index (optional, opt-in)\n\nProducts remain independent; project layer is additive.\nImplementation\nPath Resolution\nAll scripts use context.py for product-aware resolution:\nfrom context import get_product_root, get_items_dir\n \nproduct_name = args.product or os.getenv(&quot;KANO_PRODUCT&quot;) or defaults[&quot;default_product&quot;]\nproduct_root = get_product_root(product_name)  # _kano/backlog/products/&lt;name&gt;\nitems_dir = get_items_dir(product_name)        # products/&lt;name&gt;/items\nConfiguration\n\nProject level: _kano/backlog/_shared/defaults.json (default product)\nProduct level: products/&lt;name&gt;/_config/config.json (product-specific)\n\nFallback chain:\n\nCLI --product flag\nKANO_PRODUCT environment variable\nProduct embedded in filename (e.g., task parsing)\ndefaults.json default_product\nHardcoded fallback: ‚Äúkano-agent-backlog-skill‚Äù\n\nCLI Integration\nAll major scripts accept --product flag:\nscripts/backlog/workitem_create.py --product kano-agent-backlog-skill --type task --title &quot;...&quot;\nscripts/backlog/index_db.py --product kano-commit-convention-skill\nAlternatives Considered\nFlat namespacing\nitems/kabsd-tasks/0000/\r\nitems/kccs-features/0000/\n\n\nCons: No clear product boundaries, harder to extend to 100 products\nRejected: Doesn‚Äôt scale well\n\nSingle legacy structure\n\nCons: Cannot coexist multiple products, forces coordination\nRejected: Defeats purpose of monorepo autonomy\n\nReferences\n\nKABSD-FTR-0010_monorepo-platform-migration.md: Monorepo Platform Migration feature\nKABSD-TSK-0081_execute-directory-restructuring-for-monorepo-platform.md: Directory migration implementation\nADR-0016_per-product-isolated-index-architecture: Per-Product Index Architecture\n"},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0007_vcs-as-source-of-truth-derived-commit-data":{"title":"VCS as Source of Truth: Derived Commit Data","links":["items/feature/0000/KABSD-FTR-0017_traceability-commit-refs-worklog-backfill","items/userstory/0000/KABSD-USR-0018_vcs-adapter-abstraction-layer","items/task/0100/KABSD-TSK-0110_evaluate-vcs-query-cache-layer"],"tags":["architecture","vcs","traceability","derived-data"],"content":"Status\nAccepted (2026-01-07)\nImplemented in Feature KABSD-FTR-0017 (Traceability: Commit Refs ‚Üí Worklog Backfill).\nContext\nBacklog items need traceability to VCS commits to answer:\n\n‚ÄúWhich commits contributed to this item?‚Äù\n‚ÄúWhat‚Äôs the latest activity timestamp for this item?‚Äù\n‚ÄúGenerate a commit timeline view filtered by item state‚Äù\n\nTwo competing approaches:\n\nWorklog Backfill: Parse VCS commits containing Refs: &lt;item-id&gt; and append them to item worklog\nDerived Data Query: Keep VCS as source of truth; backlog queries VCS on-demand for commit data\n\nDecision\nWe adopt the Derived Data Query approach:\n\nVCS commits remain the canonical source of commit history\nBacklog items do NOT store commit data in worklog (no backfill)\nQuery tools (query_commits.py, view_generate_commits.py) dynamically fetch commit data from VCS\nCommit messages use Refs: &lt;item-id&gt;, &lt;item-id&gt; pattern for traceable references\nMulti-VCS abstraction layer (scripts/vcs/) supports Git, Perforce, SVN\n\nArchitecture:\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r\n‚îÇ VCS (Git)   ‚îÇ ‚Üê Source of Truth (commit hash, author, date, message)\r\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\r\n       ‚îÇ query via adapter\r\n       ‚ñº\r\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r\n‚îÇ VCS Adapter Layer   ‚îÇ (Git/Perforce/SVN)\r\n‚îÇ - base.py           ‚îÇ\r\n‚îÇ - git_adapter.py    ‚îÇ\r\n‚îÇ - perforce_adapter.py ‚îÇ\r\n‚îÇ - svn_adapter.py    ‚îÇ\r\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\r\n       ‚îÇ query by ID/UID\r\n       ‚ñº\r\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r\n‚îÇ Query Tools         ‚îÇ\r\n‚îÇ - query_commits.py  ‚îÇ (item ‚Üí commits list)\r\n‚îÇ - view_generate_commits.py ‚îÇ (state ‚Üí commit timeline)\r\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nRationale\nWhy Derived Data (NOT Worklog Backfill)?\nPros:\n\nNo Worklog Pollution: Worklog stays clean for human-authored entries (decisions, state changes, manual notes)\nVCS is Authoritative: No sync issues between VCS history and backlog; VCS is already immutable and auditable\nDeduplication: Single commit referencing multiple items doesn‚Äôt create N duplicate worklog entries\nTime-travel Queries: Can query commits by date range without modifying backlog files\nMulti-VCS Support: Abstraction layer allows querying Git, Perforce, SVN uniformly\n\nCons:\n\nQuery Cost: Every view generation requires VCS query (mitigated by future cache layer, see TSK-0110)\nVCS Dependency: Backlog alone doesn‚Äôt show commit history (requires VCS access)\nComplexity: Multi-VCS adapter abstraction adds code complexity\n\nWhy Multi-VCS Abstraction?\nReal-world projects may use multiple VCS systems (monorepos with Git, legacy Perforce depots, SVN archives). The adapter pattern provides:\n\nUniform interface: VCSAdapter.query_commits(ref_pattern, since, until, max_count)\nFuture-proof: Easy to add new VCS types without changing query tools\nTestable: Mock adapters for unit tests\n\nConsequences\nImmediate Impact (Feature 0017)\nImplemented:\n\n‚úÖ scripts/vcs/base.py: VCSAdapter abstract class, Commit dataclass, detect_vcs()\n‚úÖ scripts/vcs/git_adapter.py: Git implementation using git log --grep\n‚úÖ scripts/vcs/perforce_adapter.py: Perforce using p4 changes -l\n‚úÖ scripts/vcs/svn_adapter.py: SVN using svn log --xml\n‚úÖ scripts/backlog/query_commits.py: Resolve item ‚Üí query VCS ‚Üí output text/JSON\n‚úÖ scripts/backlog/view_generate_commits.py: Generate commit timeline views by state\n\nCommit Message Convention:\nfeat: implement VCS adapter abstraction\r\n\r\nRefs: KABSD-TSK-0105, KABSD-FTR-0017\n\nPattern: Refs: &lt;id&gt;[, &lt;id&gt;]* (case-insensitive, extracted via regex)\nFuture Work\nCache Layer (TSK-0110):\n\nEvaluation pending: SQLite cache vs. file-based cache\nCache invalidation: TTL-based + manual clear\nConfig: vcs.cache.enabled, vcs.cache.backend, vcs.cache.ttl\nNote: Cache is derived data; VCS remains source of truth\n\nIntegration:\n\nDashboard auto-refresh: View generators can be called from view_refresh_dashboards.py\nWorklog hints: Query tools can suggest worklog entries (human decides whether to add)\nADR references: Commits referencing ADRs can link to decision artifacts\n\nBreaking Changes\nNone. This is a new capability; existing backlog items are unaffected.\nAlternatives Considered\n1. Worklog Backfill (Original Design)\nApproach: Parse VCS commits and append to item worklog:\n2026-01-07 14:08 [agent=vcs-bot] Commit 2048e1c: Test commit for VCS adapter\nRejected because:\n\nWorklog pollution: Noisy with many commits\nSync burden: Requires periodic backfill script runs\nDeduplication issue: Multi-item commits create duplicate entries\nNot time-travel friendly: Can‚Äôt query ‚Äúcommits since yesterday‚Äù without re-parsing\n\n2. Commit Index Table (Persistent Storage)\nApproach: Store commits in SQLite commits(hash, author, date, message, item_refs) table.\nDeferred to TSK-0110 (cache evaluation):\n\nWould solve query performance\nRequires cache invalidation strategy\nSchema migration burden (needs TSK-0111 framework first)\n\n3. VCS-Native Tools Only\nApproach: Use git log --grep &quot;KABSD-&quot; directly; no backlog integration.\nRejected because:\n\nNot multi-VCS portable\nNo item-to-commits resolution (requires manual filtering)\nNo state-based filtering (can‚Äôt generate ‚ÄúInProgress items with commits‚Äù view)\n\nReferences\n\nFeature: KABSD-FTR-0017\nUserStory: KABSD-USR-0018\nTasks: KABSD-TSK-0105 (Git), KABSD-TSK-0106 (Perforce), KABSD-TSK-0107 (SVN), KABSD-TSK-0108 (query_commits.py), KABSD-TSK-0109 (view_generate_commits.py)\nFuture Work: KABSD-TSK-0110 (VCS Query Cache Evaluation)\nDepends On: None (standalone capability)\n\nAppendix: Example Usage\nQuery Commits for an Item\n# Text format\npython skills/kano-agent-backlog-skill/scripts/backlog/query_commits.py \\\n  --item KABSD-TSK-0105\n \n# JSON format\npython skills/kano-agent-backlog-skill/scripts/backlog/query_commits.py \\\n  --item KABSD-TSK-0105 --format json\nGenerate Commit Timeline View\n# All &quot;Done&quot; items with commits\npython skills/kano-agent-backlog-skill/scripts/backlog/view_generate_commits.py \\\n  --state Done --output _kano/backlog/views/commits_done.md\n \n# All &quot;InProgress&quot; items (useful for daily standup)\npython skills/kano-agent-backlog-skill/scripts/backlog/view_generate_commits.py \\\n  --state InProgress --output _kano/backlog/views/commits_active.md\nCommit Message Pattern\nfeat(vcs): add Perforce adapter with p4 changes parsing\r\n\r\nLong description of the change...\r\n\r\nRefs: KABSD-TSK-0106, KABSD-FTR-0017\n\n\nPattern is case-insensitive: refs:, Refs:, REFS: all work\nMultiple items: comma-separated Refs: ITEM-1, ITEM-2, ITEM-3\nDeduplication: Same commit appears once even if queried by multiple item IDs\n"},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0008_sqlite-schema-migration-framework":{"title":"SQLite Schema Migration Framework","links":["items/task/0100/KABSD-TSK-0111_implement-sqlite-schema-migration-framework","demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0004_file-first-architecture-with-sqlite-index","demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0012_workset-db-canonical-schema-reuse","items/task/0100/KABSD-TSK-0110_evaluate-vcs-query-cache-layer"],"tags":["architecture","database","migration","schema-evolution"],"content":"Status\nAccepted (2026-01-07)\nImplemented in Task KABSD-TSK-0111 (Implement SQLite Schema Migration Framework).\nContext\nThe SQLite index schema (introduced in ADR-0004) needs to evolve:\n\nCurrent need: Add VCS cache tables (vcs_commits, vcs_cache_metadata) for TSK-0110\nFuture needs: Embeddings tables, worklog full-text search indexes, external system sync tables\n\nExisting mechanism (ad-hoc):\n# build_sqlite_index.py (before ADR-0008)\ntry:\n    cols = [row[1] for row in conn.execute(&quot;PRAGMA table_info(item_links)&quot;).fetchall()]\n    if &quot;target_uid&quot; not in cols:\n        conn.execute(&quot;ALTER TABLE item_links ADD COLUMN target_uid TEXT&quot;)\nexcept sqlite3.OperationalError:\n    pass\nProblems:\n\nNo version tracking (schema_version is written but never read)\nHard-coded migrations in apply_schema() (not scalable)\nNo migration ordering or idempotency guarantees\nFragile try-except wrapping breaks on constraint violations\n\nRisk: Adding VCS cache tables without a migration framework could break existing DBs or create inconsistent schemas across environments.\nDecision\nWe adopt a Flyway-style migration framework:\n\nNumbered SQL migration files in references/migrations/\nVersion detection via schema_meta.schema_version (integer)\nAuto-upgrade on build_sqlite_index.py --mode rebuild\nMigration runner applies pending migrations sequentially\nBase schema (indexing_schema.sql) initializes version to 0\n\nArchitecture:\nreferences/\r\n  indexing_schema.sql          ‚Üê Base schema (creates tables, version=0)\r\n  migrations/\r\n    001_add_vcs_cache_tables.sql      ‚Üê Version 1\r\n    002_add_embeddings_fts.sql        ‚Üê Version 2\r\n    003_add_external_sync.sql         ‚Üê Version 3 (future)\n\nMigration Runner Logic:\ndef get_current_version(conn) -&gt; int:\n    &quot;&quot;&quot;Return schema version (0 if fresh DB).&quot;&quot;&quot;\n    try:\n        row = conn.execute(\n            &quot;SELECT value FROM schema_meta WHERE key=&#039;schema_version&#039;&quot;\n        ).fetchone()\n        return int(row[0]) if row else 0\n    except sqlite3.OperationalError:\n        return 0  # schema_meta doesn&#039;t exist yet\n \ndef apply_migrations(conn):\n    &quot;&quot;&quot;Apply pending migrations in order.&quot;&quot;&quot;\n    current_version = get_current_version(conn)\n    migration_dir = Path(&quot;references/migrations&quot;)\n    migrations = sorted(migration_dir.glob(&quot;*.sql&quot;))\n    \n    for migration_file in migrations:\n        version = int(migration_file.stem.split(&quot;_&quot;)[0])  # &quot;001_*.sql&quot; ‚Üí 1\n        if version &gt; current_version:\n            print(f&quot;Applying migration {version}: {migration_file.name}&quot;)\n            conn.executescript(migration_file.read_text())\n            conn.execute(\n                &quot;INSERT OR REPLACE INTO schema_meta(key, value) VALUES(?, ?)&quot;,\n                (&quot;schema_version&quot;, str(version))\n            )\n            conn.commit()\n \ndef apply_schema(conn):\n    &quot;&quot;&quot;Apply base schema + migrations.&quot;&quot;&quot;\n    conn.executescript(load_schema_sql())  # Creates schema_meta with version=0\n    apply_migrations(conn)                 # Upgrade to latest\nRationale\nWhy Flyway-Style Migrations?\nPros:\n\nExplicit Versioning: Each migration increments version; easy to track schema state\nIdempotent: Migrations run exactly once (version check prevents re-runs)\nOrdered Execution: Sorted filenames guarantee deterministic application order\nGit-Friendly: Migration files are plain SQL, diff-able and reviewable\nSimple Mental Model: Familiar to developers (like Alembic, Liquibase, Django migrations)\n\nCons:\n\nNo Rollback: Down migrations not supported (users must delete DB and rebuild)\nManual Numbering: Developers must coordinate version numbers (low risk in local-first design)\n\nWhy Not Alembic/SQLAlchemy?\n\nOverkill: Requires ORM layer; we use raw SQL for simplicity\nPython-based migrations: SQL migrations are easier to audit and portable\nDependency bloat: Alembic + SQLAlchemy adds significant dependencies\n\nWhy Not ‚ÄúDelete and Rebuild‚Äù?\nCurrent approach is rm backlog.sqlite3 &amp;&amp; rebuild. Why not keep this?\nFor small backlogs (&lt;100 items): Delete-and-rebuild is fine (fast, simple).\nFor large backlogs (&gt;1000 items): Rebuild takes &gt;10 seconds. Migrations enable:\n\nIncremental mode: Only re-index changed files (faster)\nPreserve derived data: Cache tables (VCS commits, embeddings) don‚Äôt need full rebuild\nProduction stability: Breaking schema changes are painful if rebuild is the only option\n\nDecision: Support both. Migrations for gradual evolution; delete-rebuild as nuclear option.\nConsequences\nImmediate Impact (Task 0111)\nImplemented:\n\n‚úÖ get_current_version(conn): Read schema_meta.schema_version\n‚úÖ apply_migrations(conn): Apply pending migrations from references/migrations/\n‚úÖ apply_schema(conn) refactored: Base schema ‚Üí migrations\n‚úÖ Base schema (indexing_schema.sql) initializes schema_version = &#039;0&#039;\n‚úÖ Tested: Fresh DB (v0), migration upgrade (v0‚Üív1), idempotent re-runs\n\nMigration Directory:\nreferences/migrations/\r\n  (empty - ready for 001_add_vcs_cache_tables.sql when TSK-0110 completes)\n\nFuture Work\nVersion Compatibility Check:\ndef check_schema_compatibility(conn):\n    &quot;&quot;&quot;Warn if DB schema is newer than skill version.&quot;&quot;&quot;\n    db_version = get_current_version(conn)\n    skill_max_version = 2  # Hard-coded or read from VERSION file\n    if db_version &gt; skill_max_version:\n        print(f&quot;Warning: DB schema v{db_version} newer than skill v{skill_max_version}.&quot;)\nMigration Naming Convention:\n{version:03d}_{description}.sql\r\n001_add_vcs_cache_tables.sql\r\n002_add_embeddings_fts.sql\r\n003_add_worklog_search_index.sql\n\nTransaction Safety:\r\nAll migrations wrapped in BEGIN TRANSACTION / COMMIT (SQLite default for executescript()). If migration fails, rollback prevents partial application.\nBreaking Changes\nNone. Existing DBs without migrations are version 0; migrations upgrade them gracefully.\nBackward Compatibility:\n\nOld skill (no migration runner) + new DB (v1+): Read queries work; writes may fail on new constraints\nNew skill (with migration runner) + old DB (v0): Auto-upgrades on rebuild\n\nAlternatives Considered\n1. Hard-Coded Migrations in Code\nCurrent approach (before ADR-0008):\nif &quot;target_uid&quot; not in cols:\n    conn.execute(&quot;ALTER TABLE item_links ADD COLUMN target_uid TEXT&quot;)\nRejected because:\n\nNot scalable (code bloat)\nNo version tracking (can‚Äôt detect schema state)\nError-prone (forgotten migrations leave inconsistent DBs)\n\n2. Alembic (Python-Based Migrations)\nApproach: Use Alembic for ORM-style migrations.\nRejected because:\n\nRequires SQLAlchemy ORM (we use raw SQL)\nPython-based migrations harder to audit (prefer declarative SQL)\nOverkill for local-first use case\n\n3. Schema Versioning Without Migrations\nApproach: Store version but require manual schema updates.\nRejected because:\n\nShifts migration burden to users (error-prone)\nNo automated upgrade path\n\n4. Delete-and-Rebuild Only\nApproach: Always rm backlog.sqlite3 &amp;&amp; rebuild on schema change.\nPartially Retained: Still supported as nuclear option.\nWhy Not Sufficient:\n\nLarge backlogs: Rebuild too slow (&gt;10s for 1000+ items)\nCache tables: VCS commits, embeddings would be lost (no incremental updates)\n\nReferences\n\nTask: KABSD-TSK-0111\nRelated ADR: ADR-0004 (SQLite Index Architecture)\nRelated ADR: ADR-0012 (Workset DB Schema - migrations apply to worksets too)\nFuture Work: KABSD-TSK-0110 (VCS Cache - first migration user)\nDependency: None (standalone framework)\n\nWorkset DB Migration\nPer ADR-0012, workset DBs MUST reuse the canonical schema and apply the same migrations.\nWorkset Migration Strategy:\n\nWhen building a workset, detect canonical index schema version\nApply same migrations to workset DB in order\nStore canonical_index_version in workset_manifest table\nIf canonical schema is upgraded, worksets MUST be rebuilt or auto-migrated\n\nConstraint: Workset schema_version MUST NOT exceed canonical schema_version.\nAppendix: Migration File Template\n-- Migration 001: Add VCS cache tables (2026-01-07)\n-- Context: Support derived VCS commit data caching (TSK-0110)\n \nCREATE TABLE vcs_commits (\n  item_uid TEXT NOT NULL,\n  commit_hash TEXT NOT NULL,\n  author TEXT NOT NULL,\n  date TEXT NOT NULL,\n  message TEXT NOT NULL,\n  cached_at TEXT NOT NULL,\n  PRIMARY KEY(item_uid, commit_hash)\n);\n \nCREATE INDEX idx_vcs_commits_cached_at ON vcs_commits(cached_at);\n \nCREATE TABLE vcs_cache_metadata (\n  item_uid TEXT PRIMARY KEY,\n  last_query_at TEXT NOT NULL,\n  vcs_type TEXT NOT NULL  -- &#039;git&#039;, &#039;perforce&#039;, &#039;svn&#039;\n);\nNaming: {version:03d}_{description}.sql\nTesting:\n# Fresh DB (should apply migration)\nrm -f _kano/backlog/_index/backlog.sqlite3\npython skills/kano-agent-backlog-skill/scripts/indexing/build_sqlite_index.py \\\n  --agent copilot --mode rebuild\n \n# Re-run (should skip migration, idempotent)\npython skills/kano-agent-backlog-skill/scripts/indexing/build_sqlite_index.py \\\n  --agent copilot --mode rebuild"},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0009_local-first-embedding-search-architecture":{"title":"Local-First Embedding Search Strategic Evaluation","links":["items/userstory/0000/KABSD-USR-0015_generate-embeddings-for-backlog-items-derivative-index","demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0004_file-first-architecture-with-sqlite-index","demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0011_graph-assisted-retrieval-and-context-graph","items/feature/0000/KABSD-FTR-0023_graph-assisted-rag-planning-and-minimal-implementation"],"tags":[],"content":"Local-First Embedding Search Strategic Evaluation\nContext and Problem Statement\nAs the backlog grows with hundreds of items, agents need a way to perform semantic search to find relevant context (e.g., ‚ÄúWhy did we decide to use ULID?‚Äù or ‚ÄúFind similar tasks for refactoring the indexer‚Äù).\nOur architecture is ‚ÄúLocal-First‚Äù:\n\nCanonical Store: Markdown files.\nDerived Store: SQLite index (rebuildable).\n\nWe need an embedding search solution that:\n\nIntegrates well with the existing local-first workflow.\nMinimizes complex binary dependencies for cross-platform support.\nFollows the ‚Äúderived data‚Äù philosophy (indices can be thrown away and rebuilt).\n\nDecision Drivers\n\nDeployment Simplicity: Zero-install or easy-install on developer machines.\nConsistency: The vector index must stay in sync with the Markdown/SQLite data.\nPhilosophical Alignment: Keep the source of truth in files; everything else is a performance optimization.\n\nConsidered Options\nRoute A: SQLite + Vector Extension (e.g., sqlite-vec)\nUse a SQLite extension to handle vector storage and ANN (Approximate Nearest Neighbor) search directly in the database.\n\nGood, because: Single database item; relational + vector joins in one query.\nBad, because: Loading binary extensions in Python (sqlite3.load_extension) is notoriously finicky across platforms (Windows vs Linux vs macOS). Packaging these binaries into the skill makes it ‚Äúheavy‚Äù.\n\nRoute B: SQLite (Metadata) + Sidecar ANN Index (e.g., FAISS / HNSWlib)\nKeep the metadata (ID, title, state) in the existing SQLite index. Store the high-dimensional vectors in a separate, dedicated index file (sidecar).\n\nGood, because:\n\nHighly decoupled: We can swap FAISS for HNSWlib or even a plain NumPy file without touching the SQLite schema.\nFits the ‚ÄúKano Philosophy‚Äù: The vector index is just another derived artifact.\nPerformance: Sidecar indices like HNSWlib are extremely fast for mmap-based local search.\n\n\nBad, because: Requires a ‚Äútwo-step‚Äù lookup (Search Sidecar ‚Üí Map IDs ‚Üí Fetch SQLite) and a dual-sync process during ingestion.\n\nRoute C: Postgres + pgvector (Shared Derived Store)\nMove the derived index to a remote Postgres instance with the pgvector extension.\n\nGood, because: Perfect for multi-agent/multi-remote collaboration where a shared ‚Äúclaim‚Äù or ‚Äúlock‚Äù system is needed anyway.\nBad, because: Requires a server. Not ‚Äúlocal-first‚Äù in the spirit of the project. High latency for simple local tasks.\n\nDecision Outcome\nChosen option: Route B (Sidecar ANN Index) for local-first environments, with an optional path to Route C for shared/remote usage.\nImplementation Strategy\n\n\nUnified Ingestion:\n\nDocTypes: Cover WorkItem, ADR, Worklog, Workset (local cache), and Skill Docs.\nMetadata Store: SQLite documents table tracks uid, doctype, product, path, and content_hash.\nChunking: Document-aware chunking (e.g., ADR sections, Worklog per-day). Stores in chunks table with parent_doc and section metadata.\nFTS5: Index chunk text in SQLite FTS5 for sub-millisecond keyword search and BM25 ranking.\n\n\n\nEmbedding &amp; Vector Sidecar:\n\nSidecar: HNSWlib or FAISS index file (index_&lt;product&gt;.bin).\nIncremental Sync: Only compute embeddings for chunks where text_hash has changed.\nMapping: SQLite stores chunk_id -&gt; vector_id to bridge the sidecar back to metadata.\n\n\n\nHybrid Search &amp; Ranking:\n\nQuery Path:\n\nStructural: SQLite B-Tree (product/type/status).\nKeyword: SQLite FTS5 (BM25).\nSemantic: Sidecar ANN (Cosine similarity).\n\n\nFusion: Combine scores using weighted logic:\n\nw_exact: High weight for ID matches.\nw_type: Priority for ADR Decisions and WorkItem Titles.\nw_recency: Decay score for older content.\nw_visibility: Distinguish between canonical and local_cache.\n\n\n\n\n\nPros and Cons of the Consequences\nGood\n\nPortable: The sidecar can be shared or ignored by git easily.\nFast: Local search is sub-millisecond.\nRobust: If the sidecar breaks, we just delete it and rerun the indexing script.\nComprehensive: Covers ‚Äúeverything‚Äù in the repo while preserving visibility boundaries (Local Workset vs Canonical ADRs).\n\nBad\n\nSync Logic: Need to handle incremental updates (delete old vectors if file is deleted/moved).\nTooling: Requires an ANN library in the dependencies (e.g., hnswlib or sentence-transformers/faiss-cpu).\n\nReferences\n\nKABSD-USR-0015: Generate embeddings for backlog items\nADR-0004: File-first architecture with SQLite index\n\nGraph-assisted retrieval (Context Graph)\nIn addition to keyword/semantic retrieval, we can improve precision and traceability by expanding the seed set\r\nvia a derived Context Graph (parent chain, ADR refs, dependency links).\nMinimal strategy:\n\nRetrieve seeds via FTS/ANN\nExpand k-hop over allowlisted edges\nRe-rank and pack context (seed + neighbors)\n\nSee:\n\nADR-0011 Graph-assisted retrieval with a derived Context Graph\nKABSD-FTR-0023 Graph-assisted RAG planning\n"},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0010_project-babylon-global-scale-collaboration-vision":{"title":"Kano-Babylon Project","links":[],"tags":["vision","scaling","agents","babylon"],"content":"Kano-Babylon Project\nThe project has established a solid ‚ÄúLocal-First‚Äù foundation with the Kano Commit Convention (KCC) and the Backlog Skill. However, the ultimate goal transcends a single repo or a single user. We envision ‚ÄúProject Babylon‚Äù‚Äîa vast, distributed project execution system where thousands of humans and agents collaborate on a scale previously impossible.\nVision: The Babylon Tower\nThe metaphor of the Tower of Babel represents a project so grand it reaches the heavens. Unlike the biblical story, our ‚ÄúBabylon‚Äù uses technology to ensure that a multitude of voices and languages (human and machine) can work in perfect harmony.\nThe Workforce (Agents)\nAgents are the ‚Äúmasons‚Äù of the tower. They:\n\nExecute high-velocity code, doc, and test changes.\nMaintain the backlog discipline autonomously.\nCommunicate via structured data (JSON/MD) and auditable worklogs.\nOperate locally, minimizing latency and avoiding central bottlenecks.\n\nThe Overseers (Humans)\nHumans are the ‚Äúarchitects‚Äù and ‚Äúsupervisors‚Äù. They:\n\nProvide high-level context and intent.\nReview critical ADRs and release candidates.\nResolve high-level priority conflicts.\nDefine the ‚ÄúTemporary Clauses‚Äù and guardrails for the agent workforce.\n\nArchitectural Pillars for Babylon\n\nVCS-Agostic Distribution: Git/Perforce/Subversion act as the transport layer. The ‚Äústate‚Äù of the project is a forest of local-first backlogs.\nEventually Consistent Coordination: Moving away from central ‚Äúlocking‚Äù towards a ‚Äúclaim/lease‚Äù protocol where agents can claim segments of work and sync changes asynchronously.\nCanonical File-First Truth: The ‚ÄúSource of Truth‚Äù remains readable files (.md, .json). DBs (SQLite/Vector) are only ever derived caches for performance.\nAgent Semantic Indexing: Global search across thousands of products using decentralized vector embeddings and cross-repo referencing.\nUniversal Linter/Compliance: Every ‚Äúbrick‚Äù added to the tower must pass the KCC and Backlog Quality gates (STCC), ensuring the tower never crumbles from internal inconsistency.\n\nOvercoming the ‚ÄúBabylon Curse‚Äù (Counter-measures)\nThe historical Babylon fell because of linguistic fragmentation and loss of common purpose. Our architecture is designed to proactively avoid this ‚Äúcurse‚Äù:\n\nSTCC as a Universal Language: By strictly enforcing the Standardized Technical Communication Convention (STCC), we ensure that an agent in one part of the project produces output that is perfectly understood by an agent (or human) in another, regardless of their internal processing ‚Äúdialect‚Äù.\nLocal-First Resilience: If central coordination (cloud/server) fails, the ‚Äúbuilders‚Äù (local nodes) don‚Äôt stop. They continue working based on local truth and re-sync whenever possible, preventing total project paralysis.\nAuditable Reconstruction: The append-only Worklog and immutable Git history act as a permanent record. If coordination is temporarily lost, the project can be ‚Äúre-aligned‚Äù by traversing the decision trail.\nThe Human Context Anchor: Humans serve as the source of ‚ÄúGrand Intent‚Äù, preventing the workforce from diverging into irrelevant or conflicting optimizations.\n\nRationale\nBy documenting this now, we ensure that every local-first decision we make (ID strategy, path resolution, i18n) is a ‚Äúpre-fit‚Äù for a global-scale architecture. We are building the scaffold to support the weight of the heavens.\nStatus\nProposed/Visionary. This ADR serves as the north star for all future development. It justifies the strictness of our current local-first hardening while preparing the logic for the ‚ÄúGreat Sync‚Äù.\n2026-01-08 18:55 [agent=antigravity] Created based on user‚Äôs vision of reaching the divine through collaborative scale."},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0011_graph-assisted-retrieval-and-context-graph":{"title":"Graph-assisted retrieval with a derived Context Graph (weak graph first)","links":["demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0004_file-first-architecture-with-sqlite-index","demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0009_local-first-embedding-search-architecture","items/feature/0000/KABSD-FTR-0007_optional-db-index-and-embedding-rag-pipeline","items/feature/0000/KABSD-FTR-0023_graph-assisted-rag-planning-and-minimal-implementation"],"tags":[],"content":"Decision\nAdopt Graph-assisted retrieval as a minimal, local-first improvement to context quality:\n\nUse FTS/embeddings to retrieve seed nodes\nUse a derived Context Graph to expand to load-bearing neighbors (k-hop traversal)\nKeep everything derived/rebuildable from canonical Markdown (file-first)\n\nThis ADR explicitly chooses weak graph first: only structured relationships (no LLM entity extraction).\nContext\nWe already have a file-first backlog with optional derived indexes (SQLite / FTS / embeddings).\r\nVector-only retrieval often returns text-similar chunks but misses the structural context (parents, ADR decisions, dependency chains).\nWe want a deterministic, auditable way to expand context that is:\n\nlocal-first\nderived/rebuildable\nincrementally maintainable\nsafe (bounded expansion to avoid prompt bloat)\n\nDefinitions\n\nContext Graph: a derived, typed graph of artifact relationships (items, ADRs, dependencies, etc.).\nSeed set: top-N nodes from FTS/embedding retrieval.\nGraph expansion: k-hop traversal from seeds over allowlisted edges with limits.\n\nGraph model (v1)\nNodes\nMinimum node types:\n\nwork_item (Epic/Feature/UserStory/Task/Bug)\nadr\n\nOptional (for embedding/fts pipelines):\n\nchunk (document chunk tied to a parent doc)\n\nEdges\nMinimum edge types:\n\nparent (child ‚Üí parent)\ndecision_ref (work_item ‚Üí adr)\nrelates (work_item ‚Üí work_item)\nblocks / blocked_by\n\nStorage (derived)\nThe Context Graph is derived data. Implementations may:\n\n\nMaterialize into SQLite\n\nreuse items as the node registry\nstore edges in a links-style table (source_uid, target_uid, type, optional weight, source_path)\n\n\n\nSidecar graph artifacts\n\n&lt;backlog-root&gt;/_index/graph_nodes.jsonl\n&lt;backlog-root&gt;/_index/graph_edges.jsonl\n\n\n\nBoth must be safe to delete and rebuild.\nRetrieval strategy (Graph-assisted RAG)\n\nSeed retrieval\n\nFTS and/or embeddings return top-N seed nodes/chunks\n\n\nExpand\n\ntraverse k-hop (default k=1)\nedge allowlist and fanout caps\n\n\nRe-rank\n\nweights by doctype and section (ADR decision &gt; item title/acceptance &gt; worklog)\noptionally prioritize Ready/InProgress items\n\n\nContext packing\n\nemit a context pack describing:\n\nseeds (why selected)\nneighbors (which edge pulled them in)\nminimal excerpts/anchors (title/ids/links)\n\n\n\n\n\nConfig surface (indicative)\n\nretrieval.graph.enabled\nretrieval.graph.k_hop\nretrieval.graph.edge_allowlist\nretrieval.graph.max_neighbors_per_seed\nretrieval.weights.* (doctype/section/state weights)\n\nConsequences\n\nGraph-assisted retrieval becomes the preferred way to preserve traceability (seed + neighbors).\nTooling must keep expansion bounded to avoid context explosions.\nThe design stays compatible with file-scan fallback and optional SQLite/embedding acceleration.\n\nNon-goals\n\nLLM/NLP entity extraction and automatic relation mining\nserver/MCP mode or cross-repo graphs\n\nReferences\n\nADR-0004 File-first + SQLite index\nADR-0009 Local-first embedding search\nRAG pipeline\nKABSD-FTR-0023 Graph-assisted RAG planning\n"},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0011_workset-graphrag-context-graph-separation-of-responsibilities":{"title":"Workset vs GraphRAG / Context Graph ‚Äî Separation of Responsibilities","links":["items/feature/0000/KABSD-FTR-0013_add-derived-index-cache-layer-and-peragent-workset-cache-ttl","items/feature/0000/KABSD-FTR-0015_execution-layer-workset-cache-promote","demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0004_file-first-architecture-with-sqlite-index","demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0009_local-first-embedding-search-architecture","artifacts/workset_evaluation_report","items/task/0200/KABSD-TSK-0217_clarify-spec-workset-vs-graphrag-context-graph-responsibilities-no-conflict"],"tags":[],"content":"Decision\nWe adopt a clear separation of responsibilities between:\n\nWorkset: Per-agent/per-task materialized cache bundle (local, ephemeral, task-scoped)\nGraphRAG / Metadata Graph: Repo-level derived navigation/retrieval structure (nodes + edges, shared, rebuildable)\nContext Graph: Either knowledge graph (same as GraphRAG) or agent workflow/planning graph (different layer, no conflict)\n\nCore Principle: Workset and Graph are BOTH derived data. Neither is the source of truth. The canonical backlog/ADR files remain the single system of record.\nContext and Problem Statement\nAs Kano evolves to support:\n\nMulti-agent collaboration with context management (KABSD-FTR-0013, KABSD-FTR-0015)\nGraph-based retrieval and semantic search (ADR-0009)\nWorkset-based execution memory (workset_evaluation_report.md)\n\nWe face a critical architectural risk: role confusion between components.\nThe Risk: Role Confusion\nWithout clear boundaries, future implementations might:\n\nTreat per-agent worksets as the ‚Äútruth‚Äù instead of rebuildable cache\nStore the authoritative graph structure ONLY inside worksets (leading to divergence across agents)\nMix retrieval logic (graph expansion) with cache storage (workset)\nCreate worksets that cannot be rebuilt from canonical data\n\nThis ADR prevents these failure modes by establishing hard constraints and data flow patterns.\nDefinitions\n1. Workset (Working Set)\nWhat it is:\n\nA materialized bundle (typically a SQLite file + optional filesystem cache) containing a selected subset of items, chunks, and summaries.\nUsually per-agent or per-task, scoped to a specific time window or work session.\nStored in _kano/backlog/.cache/worksets/&lt;item-id&gt;/ (one directory per backlog item; agent recorded in manifest) and NOT tracked in Git.\n\nPurpose:\n\nMaximize context relevance and reduce repeated retrieval cost during task execution.\nProvide stable, fast access to ‚Äúcurrent working context‚Äù without re-querying repo-level indices.\nSupport execution-layer memory patterns (plan.md, notes.md, deliverable.md) as described in workset_evaluation_report.md.\n\nKey Properties:\n\nDerived: Built from canonical files + repo-level derived index\nRebuildable: Can be deleted and reconstructed at any time\nEphemeral: May have TTL (time-to-live) and automatic cleanup\nLocal: Not the source of truth; promotes back to canonical on important updates\n\nWhat it is NOT:\n\nNOT the system of record (canonical files are)\nNOT the authoritative graph store (repo-level graph index is)\nNOT shared across agents (each agent/task has its own)\nNOT version-controlled in Git\n\n2. GraphRAG / Metadata Graph\nWhat it is:\n\nA derived navigation/index structure with nodes and edges:\n\nNodes: workitems, ADRs (optionally commits, worklog entries, skill docs later)\nEdges: parent_of, references, depends_on, blocked_by, relates_to\n\n\nUsed for retrieval expansion and context assembly.\nStored at repo level (e.g., in SQLite links table, or separate graph DB file).\n\nPurpose:\n\nEnable graph-based queries: ‚ÄúFind all tasks blocking feature X‚Äù\nSupport k-hop expansion: ‚ÄúGiven seed items, expand to related context‚Äù\nProvide structured navigation for RAG (Retrieval-Augmented Generation)\n\nKey Properties:\n\nShared: One graph per product/repo (not per-agent)\nDerived: Built from canonical file frontmatter (parent, links.relates, etc.)\nRebuildable: Can be rebuilt from files + frontmatter\nQueryable: Supports graph queries, traversal, expansion\n\nWhat it is NOT:\n\nNOT stored only inside worksets (worksets may include subgraph slices, but the authoritative graph is repo-level)\nNOT ‚Äústrong KG‚Äù with LLM-extracted entities (that‚Äôs a future enhancement, not the base metadata graph)\nNOT the source of truth (canonical files are)\n\n3. Context Graph (Dual Meaning)\nThe term ‚ÄúContext Graph‚Äù can mean two different things, both valid and non-conflicting:\n3a. Context Graph = Knowledge Graph (Same as GraphRAG)\nIn RAG/retrieval contexts, ‚Äúcontext graph‚Äù often means the knowledge graph used for retrieval.\n\nSame as: GraphRAG / Metadata Graph (defined above)\nPurpose: Navigate and expand context for LLM queries\n\n3b. Context Graph = Agent Workflow / Planning Graph\nIn agent orchestration contexts, ‚Äúcontext graph‚Äù can mean the DAG (Directed Acyclic Graph) of agent tasks/steps.\n\nDifferent layer: This is about agent execution flow, not backlog item relationships\nPurpose: Plan and coordinate multi-step agent workflows\nNo conflict with Workset or GraphRAG: This is a workflow orchestration concept, not a data indexing concept\n\nClarification: Both meanings are valid. They address different layers and do not conflict with the Workset/GraphRAG separation.\nHard Constraints (Enforceable in Future Tickets)\n\n\nSource of Truth = Canonical Backlog/ADR Files\n\nAll writes MUST go to Markdown files in _kano/backlog/products/&lt;product&gt;/items/ or decisions/\nNeither Workset nor Graph can become the primary write target\n\n\n\nGraph and Workset are Derived and Must be Rebuildable\n\nBoth can be deleted and reconstructed from canonical files\nNo essential data lives ONLY in cache or index\n\n\n\nWorkset Must Not Become the Only Place Where Graph Truth Lives\n\nRepo-level graph index (shared derived) is the primary graph\nWorkset may include only a subgraph slice or expansion results\nWorkset does NOT store the authoritative full graph\n\n\n\nRetrieval Strategy (Workset-First with Fallback)\n\nQuery workset first (fast, stable context)\nFallback to repo-level derived index (vector/FTS/graph) when insufficient\nOptionally ‚Äúincrementally enrich‚Äù the workset after fallback\nNever skip repo-level index and rely solely on workset\n\n\n\nNon-Goals\nThis specification explicitly does NOT include:\n\nServer/MCP implementation: This is a local-first spec (per AGENTS.md temporary clause)\nStrong graph / LLM-based KG: Entity extraction, relationship mining via LLM (future enhancement)\nWorkset as global indexing authority: Worksets are local/ephemeral, not authoritative\nReal-time sync between worksets: Each agent/task workset is independent\nGraph database engine choice: This spec is agnostic to implementation (SQLite, Neo4j, plain files)\n\nData Flow Architecture\n1. Build/Maintain Repo-Level Derived Index\nCanonical Files (Markdown + frontmatter)\r\n    ‚Üì\r\n  Parse &amp; Extract\r\n    ‚Üì\r\nRepo-Level Derived Index (SQLite + sidecar ANN)\r\n‚îú‚îÄ‚îÄ items table (metadata)\r\n‚îú‚îÄ‚îÄ links table (graph edges)  ‚Üê PRIMARY GRAPH\r\n‚îú‚îÄ‚îÄ chunks table (text chunks)\r\n‚îú‚îÄ‚îÄ FTS5 index (keyword search)\r\n‚îî‚îÄ‚îÄ Sidecar ANN (vector embeddings) ‚Üê per ADR-0009\n\nGraph Tables (in SQLite or separate graph DB):\n\nlinks(source_uid, target_uid, type) stores all edges\nRebuilt from frontmatter: parent, links.relates, links.blocks, links.blocked_by\n\n2. Build Workset Using Profile Recipe\nWorkset Build Process:\r\n1. Select seeds (e.g., active/in-progress/claimed/recent items)\r\n2. Expand via graph k-hop closure:\r\n   - Follow parent chain upward\r\n   - Follow references (links.relates)\r\n   - Follow dependencies (links.depends_on, links.blocks)\r\n3. Materialize into SQLite workset:\r\n   - Copy relevant items/chunks from repo index\r\n   - Include subgraph slice (only edges relevant to this workset)\r\n   - Add workset manifest (seeds, expansion params, timestamp)\n\nWorkset Structure (SQLite file):\n-- Workset metadata\nCREATE TABLE workset_manifest (\n  workset_id TEXT PRIMARY KEY,\n  agent TEXT,\n  task_id TEXT,\n  created_at TEXT,\n  ttl_hours INTEGER,\n  seed_items TEXT -- JSON array of seed UIDs\n);\n \n-- Cached items (subset from repo index)\nCREATE TABLE cached_items (\n  uid TEXT PRIMARY KEY,\n  -- ... copy of repo index item fields\n);\n \n-- Subgraph slice (only edges relevant to this workset)\nCREATE TABLE cached_links (\n  source_uid TEXT,\n  target_uid TEXT,\n  type TEXT,\n  PRIMARY KEY (source_uid, target_uid, type)\n);\n \n-- Cached chunks (for semantic search within workset)\nCREATE TABLE cached_chunks (\n  chunk_id TEXT PRIMARY KEY,\n  parent_uid TEXT,\n  content TEXT,\n  -- ... copy of repo index chunk fields\n);\n \n-- Optional: execution memory (plan, notes, deliverable)\n-- per workset_evaluation_report.md\nWorkset Filesystem Layout (Decision 2026-01-10)\n\nBase Path: _kano/backlog/.cache/worksets/&lt;item-id&gt;/\nContents:\n\nworkset.db ‚Äî SQLite cache that reuses the canonical schema (ADR-0012)\nplan.md ‚Äî Execution checklist (three-file pattern)\nnotes.md ‚Äî Research notes / scratchpad\ndeliverable.md ‚Äî Draft output waiting for promotion\n\n\nAgent Attribution: workset_manifest.agent records who initialized the workset; directory naming stays per item to keep TTL cleanup simple.\nRationale: Local-first workflows typically have a single active agent per backlog item. Owner locking (KABSD-TSK-0036) prevents concurrent edits; adding agent IDs to the filesystem path would duplicate manifest data and complicate cleanup.\nFuture Extension: If multiple agents must share a task concurrently, we can add optional &lt;agent_id&gt; suffixes, but the default is per-item directories for deterministic paths.\n\n3. Query Path\nAgent Query\r\n    ‚Üì\r\n1. Search Workset (local SQLite)\r\n   ‚îú‚îÄ‚îÄ Fast: all relevant context already materialized\r\n   ‚îî‚îÄ‚îÄ If sufficient ‚Üí Return results\r\n    ‚Üì\r\n2. Fallback to Repo-Level Index (if workset insufficient)\r\n   ‚îú‚îÄ‚îÄ Query repo-level SQLite (items, links, chunks, FTS5)\r\n   ‚îú‚îÄ‚îÄ Query sidecar ANN (vector search)\r\n   ‚îî‚îÄ‚îÄ Expand via repo-level graph (k-hop from new seeds)\r\n    ‚Üì\r\n3. Optionally Update Workset (incremental enrichment)\r\n   ‚îú‚îÄ‚îÄ Add newly discovered items/chunks to workset\r\n   ‚îî‚îÄ‚îÄ Extend subgraph slice with new edges\r\n    ‚Üì\r\nReturn results to agent\n\nResponsibilities (Unambiguous)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComponentResponsibilityWhat It IsWhat It Is NOTCanonical FilesSource of truthMarkdown + frontmatter in GitNOT queryable at scaleRepo-Level GraphPrimary graph structureShared, rebuildable index of all edgesNOT per-agent cacheWorksetPer-task cache bundleLocal, ephemeral, task-scoped materialized contextNOT source of truth, NOT authoritative graphSidecar ANNVector similarity searchFast semantic search (per ADR-0009)NOT metadata storeSQLite IndexFast relational queriesDerived metadata + FTS (per ADR-0004)NOT source of truth\nRetrieval Strategy (Detailed)\nWorkset-First Strategy\nWhen to use Workset-first:\n\nDuring active task execution (agent has claimed a task)\nWhen workset is fresh (within TTL window)\nWhen working context is stable (no major scope changes)\n\nBenefits:\n\nFast: No re-querying repo-level index\nStable: Context doesn‚Äôt change mid-task\nOffline-friendly: Workset can be pre-built and used offline\n\nRepo-Index Fallback\nWhen to fallback to repo-level index:\n\nWorkset expired or missing\nQuery requires cross-cutting view (e.g., ‚Äúall items blocking any active task‚Äù)\nNew information needed that wasn‚Äôt in initial workset seeds\n\nFallback process:\n\nQuery repo-level SQLite (items, links, chunks, FTS5)\nQuery sidecar ANN if semantic search needed\nExpand via graph if relationship traversal needed\nCache results in workset for future queries (optional incremental enrichment)\n\nIncremental Enrichment (Optional)\nAfter fallback, agent MAY update workset:\n\nAdd newly discovered items/chunks\nExtend subgraph slice with new edges\nUpdate workset manifest (enrichment timestamp)\n\nGuardrails:\n\nWorkset size limits (prevent unbounded growth)\nEnrichment policy (e.g., only add items within 2-hop distance)\nTTL still applies (workset expires regardless of enrichment)\n\nTrade-offs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrade-offDescriptionWorkset StalenessWorkset may become stale if canonical files change during task execution. Mitigation: TTL + periodic rebuild.Dual MaintenanceNeed to maintain both repo-level index and workset build logic. Mitigation: Shared indexing code, clear derivation rules.Subgraph Slice ComplexityDeciding which edges to include in workset subgraph is non-trivial. Mitigation: Start with simple k-hop expansion, iterate.Storage OverheadWorksets duplicate data from repo index. Mitigation: Worksets are ephemeral, cleaned up by TTL.\nConsequences\nPositive\n\nClear Boundaries: No ambiguity about which component owns what\nRebuildable: All derived data can be deleted and reconstructed\nScalable: Worksets enable efficient multi-agent collaboration without index contention\nComposable: Graph, vector search, and worksets work together without conflict\n\nNegative\n\nComplexity: More components to understand and maintain\nSync Logic: Need careful handling of cache invalidation and TTL\nLearning Curve: Developers must understand the distinction between repo-level and workset-level data\n\nMitigations\n\nDocumentation: This ADR + inline code comments\nTooling: Scripts to rebuild indices, inspect worksets, validate consistency\nDefaults: Worksets are optional; can disable for simple single-agent scenarios\n\nReferences\n\ncache layer and per‚ÄëAgent workset cache (TTL)\nKABSD-FTR-0015: Execution Layer: Workset Cache + Promote\nADR-0004: File-First Architecture with SQLite Index\nADR-0009: Local-First Embedding Search Strategic Evaluation\nWorkset Evaluation Report\nKABSD-TSK-0217: Task tracking this specification\n\nFuture Work\nThis ADR establishes the foundation. Future enhancements may include:\n\nStrong Graph / Entity Extraction: LLM-based relationship mining beyond frontmatter\nMulti-Agent Workset Coordination: Shared worksets for pair programming scenarios\nWorkset Templates: Pre-configured recipes for common task types\nGraph Visualization: Tools to visualize repo-level graph and workset subgraphs\nPerformance Benchmarks: Measure workset-first vs repo-index-first query performance\n\nDecision Rationale\nWhy separate Workset and Graph?\n\nDifferent lifecycles: Graph is long-lived and shared; Workset is ephemeral and local\nDifferent query patterns: Graph is for exploration/expansion; Workset is for stable task context\nDifferent consistency models: Graph must stay in sync with canonical files; Workset can be stale within TTL\n\nWhy NOT merge them?\n\nMerging would force either (a) graph to be per-agent (duplication, inconsistency) or (b) workset to be shared (defeats the purpose of local cache)\nClear separation enables independent evolution and optimization of each component\n\nWhy repo-level graph is primary?\n\nGraph relationships are project-wide knowledge (e.g., ‚Äúwhat blocks what‚Äù)\nPer-agent graphs would diverge and create confusion\nWorksets can include subgraph slices for fast local queries, but authoritative graph must be shared\n\nStatus\nProposed (2026-01-09)\nThis ADR is proposed for review. Once accepted, it becomes the architectural constraint for all future Workset and GraphRAG implementation work.\n\nThis ADR was created as part of KABSD-TSK-0217 to prevent role confusion between Workset, GraphRAG, and Context Graph."},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0012_workset-db-canonical-schema-reuse":{"title":"Workset DB Uses Canonical Schema (No Parallel Schema)","links":["_meta/canonical_schema.sql","_meta/canonical_schema.json","artifacts/workset_schema_verification_examples","demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0003_identifier-strategy-for-local-first-backlog","demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0004_file-first-architecture-with-sqlite-index","demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0008_sqlite-schema-migration-framework","demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0011_workset-graphrag-context-graph-separation-of-responsibilities","items/task/0000/KABSD-TSK-0046_define-db-index-schema-items-links-worklog-decisions","items/feature/0000/KABSD-FTR-0013_add-derived-index-cache-layer-and-peragent-workset-cache-ttl","items/feature/0000/KABSD-FTR-0015_execution-layer-workset-cache-promote"],"tags":[],"content":"Decision\nWorkset DB must reuse the same system schema and semantics as the source-of-truth model, rather than creating a separate ‚Äúworkset-only schema‚Äù.\nWorkset DB is a materialized subset view of the canonical model, not a different model.\nContext and Problem Statement\nWe maintain canonical backlog data as local-first files (source of truth). We also plan to generate worksets (per-task/per-agent context bundles) as SQLite DBs for fast retrieval and stable context.\nWe want worksets to be derived data, rebuildable at any time, and we already rely on a globally unique UID for identity (ADR-0003).\nThe Question: Should workset DB have its own schema design, or should it reuse the canonical schema defined for the repo-level derived index?\nThe Risk: If workset has its own schema, it will inevitably diverge from the canonical data model, creating long-term maintenance cost, bugs, and integration friction.\nRationale\nWhy This Is Important\n1. Avoid Schema Drift\n\nIf workset has its own schema, it will diverge from the canonical data model over time\nEvery schema evolution would require parallel changes in two places\nDifferent schemas lead to subtle semantic mismatches and data loss during translation\n\n2. Portable Context with Zero Translation\n\nAgents/tools that understand the canonical schema can read a workset DB without custom mapping logic\nThis reduces integration friction across tools (CLI, future server fa√ßade, GUI)\nA workset can be directly queried using the same queries used for the repo-level index\n\n3. Deterministic Rebuild\n\nWorkset is derived: it must be regeneratable from source-of-truth + derived indexes\nReusing schema makes regeneration straightforward and verifiable\nSchema migrations (ADR-0008) apply uniformly to both repo index and worksets\n\n4. Consistent Identity &amp; References\n\nWorkitems/ADRs keep the same UID and same link semantics across canonical and workset DB\nEdges (parent/ref/depends) remain consistent\nNo need for ID translation or mapping tables\n\n5. Future-Proofing for Graph-Assisted Retrieval\n\nGraph expansion can materialize a subgraph into workset without inventing new edge formats\nWorkset becomes a ‚Äúview slice‚Äù of the full graph, not a separate graph model\n\nCanonical Schema (Reused by Workset DB)\nThe canonical schema is defined in ADR-0004 and KABSD-TSK-0046. It represents:\nCore Entities\nitems Table\nCore metadata for all work items (Epic/Feature/Story/Task/Bug) and ADRs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumnTypeDescriptionuidTEXT (PK)UUIDv7 (globally unique, from frontmatter)idTEXTDisplay ID (e.g., KABSD-TSK-0049)typeTEXTWork item type (Epic, Feature, UserStory, Task, Bug, ADR)stateTEXTCurrent state (Proposed, Ready, InProgress, Done, etc.)titleTEXTItem titlepathTEXTRelative path to canonical filemtimeREALFile modification timestampcontent_hashTEXTHash of content (for change detection)frontmatterJSONFull frontmatter blob (flexibility)createdTEXTCreation date (ISO 8601)updatedTEXTLast updated date (ISO 8601)priorityTEXTPriority (P1, P2, P3, etc.)parent_uidTEXTUID of parent item (null if root)ownerTEXTCurrent owner/assigneeareaTEXTFunctional areaiterationTEXTIteration/sprint identifiertagsJSONArray of tags\nlinks Table\nTracks typed relationships for graph queries.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumnTypeDescriptionsource_uidTEXTLink source (referencing item)target_uidTEXTLink target (referenced item)typeTEXTLink type: ‚Äúparent‚Äù, ‚Äúrelates_to‚Äù, ‚Äúblocks‚Äù, ‚Äúblocked_by‚Äù, ‚Äúdecision_ref‚ÄùPRIMARY KEY(source_uid, target_uid, type)\nworklog Table (Optional but Canonical)\nStores append-only worklog entries.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumnTypeDescriptionuidTEXT (PK)Unique worklog entry IDitem_uidTEXTUID of parent itemtimestampTEXTISO 8601 timestampagentTEXTAgent/user who created entrycontentTEXTWorklog entry text\nchunks Table (For Embedding/FTS)\nStores content chunks for semantic search.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumnTypeDescriptionchunk_idTEXT (PK)Unique chunk identifierparent_uidTEXTUID of parent itemchunk_indexINTSequence number within parentcontentTEXTChunk text contentsectionTEXTSection type (Context, Goal, Approach, etc.)embeddingBLOBFloat32 vector array (optional)\nSchema Metadata\nPer ADR-0008, track schema version for migrations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumnTypeDescriptionkeyTEXT (PK)Metadata key (e.g., ‚Äúschema_version‚Äù)valueTEXTMetadata value\nWorkset as a Subset\nA workset DB contains:\n\nIncluded nodes: A filtered subset of items (selected by workset recipe)\nIncluded edges: links restricted to included nodes (or optionally include boundary edges)\nIncluded chunks: Content chunks for included items\nIncluded worklog: Worklog entries for included items (optional)\n\nNOT included in workset:\n\nItems outside the selected scope\nEdges between excluded nodes\n\nWorkset-Specific Metadata (Additive Only)\nWorkset DB MAY add workset-specific metadata tables, but MUST NOT change core entity schemas.\nAllowed: workset_manifest Table\nCREATE TABLE workset_manifest (\n  workset_id TEXT PRIMARY KEY,\n  agent TEXT NOT NULL,\n  task_id TEXT,\n  created_at TEXT NOT NULL,\n  ttl_hours INTEGER,\n  seed_items TEXT,  -- JSON array of seed UIDs\n  expansion_params TEXT,  -- JSON: {k_hop: 2, edge_types: [...]}\n  source_commit_hash TEXT,  -- Git commit of canonical files\n  canonical_index_version TEXT NOT NULL CHECK (canonical_index_version &lt;&gt; &#039;&#039;)  -- Schema version of source index\n);\nAllowed: workset_provenance Table\nCREATE TABLE workset_provenance (\n  item_uid TEXT PRIMARY KEY,\n  selection_reason TEXT NOT NULL,  -- &quot;seed&quot;, &quot;parent_expansion&quot;, &quot;dependency_expansion&quot;, &quot;manual&quot;\n  distance_from_seed INTEGER,  -- Hop count from nearest seed (0 for seeds)\n  included_at TEXT NOT NULL,  -- ISO 8601 timestamp when item was added\n  FOREIGN KEY (item_uid) REFERENCES items(uid) ON DELETE CASCADE\n);\nThese tables are additive ‚Äî they extend the canonical schema without changing core table definitions.\nContent Storage Strategy\nWorkset DB supports multiple content strategies (choose based on use case):\nOption 1: Full Content (Portable)\n\nStore complete item content in workset DB (in items.frontmatter JSON or separate content column)\nPros: Workset is fully portable, can be used offline\nCons: Larger DB size, duplication of content\n\nOption 2: Pointer-Based (Smaller)\n\nStore only uid, path, and content_hash in workset\nRequire access to canonical files for full content retrieval\nPros: Smaller workset DB, no content duplication\nCons: Not portable, requires canonical file access\n\nOption 3: Hybrid (Recommended)\n\nStore summaries/excerpts in workset (title, first N words of sections)\nStore pointers to canonical files + hashes for verification\nOptionally include full content for ‚Äúhot‚Äù items (recently accessed)\nPros: Balanced size vs portability\nCons: More complex logic\n\nDecision: Support all three strategies via configuration. Default to Hybrid for best balance.\nSchema Evolution and Migrations\nPer ADR-0008, schema migrations apply uniformly:\n\n\nRepo-level index migration:\n\nApply migration 001_add_vcs_cache_tables.sql\nUpdate schema_meta.schema_version = &#039;1&#039;\n\n\n\nWorkset DB migration (when rebuilding workset):\n\nDetect source index schema version from canonical_index_version in manifest\nApply same migrations to workset DB\nEnsure workset schema version matches canonical schema version\n\n\n\nConstraint: Workset DB schema version MUST NOT exceed canonical schema version.\nRebuild Rule: If canonical schema is upgraded, all worksets MUST be rebuilt or auto-migrated.\nGuidelines for Maintaining Schema Compatibility\nDO: Add Workset-Specific Tables\n‚úÖ Add workset_manifest, workset_provenance, or similar metadata tables\r\n‚úÖ These tables MUST be prefixed with workset_ to avoid naming conflicts\r\n‚úÖ Document all workset-specific tables in this ADR or code comments\nDO NOT: Modify Core Table Schemas\n‚ùå Do NOT change items, links, chunks, worklog, or schema_meta table definitions\r\n‚ùå Do NOT add columns to core tables specific to worksets\r\n‚ùå Do NOT rename or remove columns from canonical schema\nDO: Subset Core Tables\n‚úÖ Workset items table contains fewer rows than canonical items (filtering is allowed)\r\n‚úÖ Workset links table only includes edges relevant to included nodes\nDO: Preserve Field Semantics\n‚úÖ uid means the same thing in workset and canonical DB (globally unique identifier)\r\n‚úÖ state values match canonical state vocabulary (Proposed, Ready, InProgress, Done, etc.)\r\n‚úÖ type values match canonical type vocabulary (Epic, Feature, UserStory, Task, Bug, ADR)\nDO: Version Compatibility Checks\n‚úÖ When loading a workset, verify canonical_index_version matches expected schema\r\n‚úÖ If version mismatch, warn or auto-rebuild workset\nAcceptance Criteria\n\n Canonical schema is defined (ADR-0004, this ADR)\n Workset DB reuses canonical items, links, chunks, worklog tables\n Workset-specific metadata is additive only (workset_manifest, workset_provenance)\n Content storage strategy is documented (full/pointer/hybrid)\n Schema migration compatibility is specified (workset follows canonical migrations)\n Guidelines for adding workset metadata are documented (DO/DO NOT rules)\n SQL schema definition created (canonical_schema.sql)\n JSON schema definition created (canonical_schema.json)\n Verification examples documented (workset_schema_verification_examples.md)\n Implementation validates schema compatibility at workset build time (future work)\n Tools that read canonical schema can read workset DB without special-case mapping (verified via examples)\n\nNon-Goals\n\nWorkset DB is NOT a new source-of-truth\nDo NOT implement a separate ‚Äúworkset schema v2‚Äù\nDo NOT require workset DB to contain all canonical data (it‚Äôs a subset by definition)\nDo NOT implement server runtime (per AGENTS.md temporary clause)\n\nConsequences\nPositive\n\nNo Schema Drift: Single schema definition for all derived DBs\nPortable Context: Worksets can be shared, inspected, queried with standard tools\nSimplified Maintenance: Schema migrations apply uniformly\nConsistent Identity: UIDs and link semantics preserved across canonical and workset\n\nNegative\n\nWorkset Constraints: Workset DB cannot optimize schema for workset-specific use cases\nMigration Coupling: Workset rebuild required when canonical schema changes\n\nMitigations\n\nExtensibility: Workset-specific tables allowed (additive only)\nRebuild Automation: Make workset rebuild fast and deterministic\nVersion Checks: Detect and handle schema version mismatches gracefully\n\nAlternatives Considered\n1. Separate Workset Schema\nApproach: Design a custom schema optimized for workset use cases.\nRejected because:\n\nSchema drift inevitable (maintenance burden)\nTranslation layer required (complexity, bugs)\nBreaks portable context (tools need dual schema support)\n\n2. Denormalized Workset Schema\nApproach: Flatten canonical schema into a denormalized ‚Äúworkset view‚Äù (e.g., single table with all fields).\nRejected because:\n\nLoses relational structure (graph queries become difficult)\nContent duplication (same item appears multiple times with different join results)\nStill requires mapping/translation logic\n\n3. Schema-Free (JSON Blobs Only)\nApproach: Store items as raw JSON blobs in workset DB.\nRejected because:\n\nNo relational query support (defeats purpose of SQL index)\nNo FTS or graph traversal without parsing JSON\nStill need consistent JSON schema (same drift problem)\n\nReferences\n\nADR-0003: Identifier Strategy (UID) ‚Äî Global UID ensures identity consistency\nADR-0004: File-First Architecture with SQLite Index ‚Äî Canonical schema definition\nADR-0008: SQLite Schema Migration Framework ‚Äî Migration strategy\nADR-0011: Workset vs GraphRAG Separation ‚Äî Workset role definition\nKABSD-TSK-0046: Define DB Index Schema ‚Äî Original schema definition task\nKABSD-FTR-0013: Workset Cache ‚Äî Workset feature\nKABSD-FTR-0015: Workset Promote ‚Äî Workset execution layer\n\nFuture Work\n\nDefine JSON schema for canonical frontmatter (complementary to SQL schema)\nBenchmark workset query performance vs canonical index\nImplement schema version compatibility checker for workset loading\nAdd workset content strategy configuration (full/pointer/hybrid)\nCreate workset rebuild automation on schema migration\n\nVerification\nSee Workset Schema Verification Examples for test cases demonstrating:\n\nSchema compatibility checks\nSchema version tracking\nCore table consistency\nWorkset-specific table validation\nSubset semantics verification\nDeterministic rebuild testing\n\nStatus\nProposed (2026-01-09)\nThis ADR is proposed for review. Once accepted, it becomes the architectural constraint for all future Workset DB implementation work.\n\nThis ADR ensures that workset DB remains a true materialized view of the canonical schema, preventing schema drift and maintaining portable, rebuildable context bundles."},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0013_codebase-architecture-and-module-boundaries":{"title":"Codebase Architecture and Module Boundaries","links":["demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0015_skill-scoped-cli-namespace-convention","demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0037_inspector-pattern-and-query-surface-architecture","items/feature/0000/KABSD-FTR-0028_refactor-kano-agent-backlog-skill-scripts-into-a-single-cli-entry-library-modules","KABSD-FTR-0025_unified-cli-for-backlog-operations","KABSD-FTR-0019_refactor-kano-backlog-core-cli-server-gui-facades","demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0004_file-first-architecture-with-sqlite-index"],"tags":[],"content":"Decision\nEstablish strict separation between executable entrypoints (scripts/) and library modules (src/). All agent-callable operations must go through a single CLI entrypoint (scripts/kano), which delegates to library use-cases.\nHard Rules\n\nscripts/ is executable-only: No reusable module code in scripts/. Scripts must not be imported as libraries.\nSingle CLI entrypoint: Agents call only scripts/kano &lt;subcommand&gt;. All operations are exposed through this interface.\nsrc/ is import-only: Core logic lives in src/kano_backlog_* packages. These are imported by the CLI (and future facades), never executed directly.\nConsistent gating: All write operations run prereqs + initialization checks via a single gate layer in the CLI.\nDeterministic output: Same input state produces stable, reproducible output for views and queries.\n\nContext\nCurrent State (Problems)\nThe scripts/ directory contains 40+ standalone Python scripts with overlapping responsibilities:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCategoryScriptsIssuesbacklog/35+ scriptsMixed executable + library code in same folderbootstrap/1 scriptOKindexing/8 scriptsSome logic should be library codefs/5 scriptsFile operations, OK as thin wrappersvcs/4 adaptersAlready library-style but in scripts/common/4 modulesShared code incorrectly placed in scripts/logging/3 modulesShared code incorrectly placed in scripts/\nProblems:\n\nCommon logic mixed into scripts makes it hard to enforce consistent gating (prereqs/initialized/dry-run).\nCoding agents don‚Äôt have a clear architecture reference; new code gets placed inconsistently.\nDifferent scripts may bypass checks or diverge behavior over time.\nNo single entry point exists; agents must know which script to call.\n\nExisting Foundation\nWe already have:\n\nsrc/kano_backlog_core/: Core models, config, errors, refs, state (good foundation)\nsrc/kano_cli/: CLI skeleton with Typer, ~4 commands implemented\nscripts/backlog/lib/: Some shared code (should move to src/)\nscripts/backlog/cli/: Thin wrappers (good pattern, needs expansion)\n\nArchitecture\nLayered Architecture\nflowchart TB\r\n  subgraph Agent[&quot;Coding Agent / Human&quot;]\r\n    A[&quot;calls scripts/kano (CLI)&quot;]\r\n  end\r\n\r\n  subgraph Scripts[&quot;scripts/ (executable-only)&quot;]\r\n    CLI[&quot;scripts/kano\\n‚îú‚îÄ parse args\\n‚îú‚îÄ run gates (prereqs/init)\\n‚îî‚îÄ call lib use-cases&quot;]\r\n  end\r\n\r\n  subgraph Src[&quot;src/ (import-only)&quot;]\r\n    Core[&quot;kano_backlog_core\\n(config/models/ids/errors/refs/state)&quot;]\r\n    Ops[&quot;kano_backlog_ops\\n(use-cases: init/create/update/\\nindex/workset/view)&quot;]\r\n    Adapters[&quot;kano_backlog_adapters\\n(sqlite/fts/faiss/vcs/fs)&quot;]\r\n    CLI_Pkg[&quot;kano_cli\\n(Typer app, commands)&quot;]\r\n    Hooks[&quot;kano_backlog_hooks (future)\\n(pre/post hooks interface)&quot;]\r\n  end\r\n\r\n  subgraph Data[&quot;Data Layer&quot;]\r\n    SoT[&quot;Source of Truth\\n(_kano/backlog/*.md)&quot;]\r\n    Cache[&quot;Derived Cache\\n(_kano/backlog/_index/*.sqlite3)&quot;]\r\n  end\r\n\r\n  A --&gt; CLI\r\n  CLI --&gt; CLI_Pkg\r\n  CLI_Pkg --&gt; Ops\r\n  Ops --&gt; Core\r\n  Ops --&gt; Adapters\r\n  Adapters --&gt; SoT\r\n  Adapters --&gt; Cache\r\n  Ops -. optional .-&gt; Hooks\n\nTarget Folder Structure\nflowchart LR\r\n  R[&quot;skills/kano-agent-backlog-skill/&quot;] --&gt; S[&quot;scripts/&quot;]\r\n  S --&gt; K[&quot;kano (only entrypoint)&quot;]\r\n  S --&gt; B[&quot;backlog/ (deprecated wrappers)&quot;]\r\n  S --&gt; I[&quot;bootstrap/, fs/ (thin utilities)&quot;]\r\n\r\n  R --&gt; SRC[&quot;src/&quot;]\r\n  SRC --&gt; CORE[&quot;kano_backlog_core/\\n(models, ids, config, errors)&quot;]\r\n  SRC --&gt; OPS[&quot;kano_backlog_ops/\\n(use-cases)&quot;]\r\n  SRC --&gt; ADP[&quot;kano_backlog_adapters/\\n(backends)&quot;]\r\n  SRC --&gt; CLIPKG[&quot;kano_cli/\\n(Typer commands)&quot;]\r\n\r\n  R --&gt; REF[&quot;references/\\n(schemas, docs)&quot;]\r\n  R --&gt; TPL[&quot;templates/\\n(markdown templates)&quot;]\r\n  R --&gt; DEC[&quot;decisions/ (this ADR)&quot;]\n\nPackage Responsibilities\nkano_backlog_core (existing, expand)\n\nmodels.py: Pydantic models for work items, ADRs\nconfig.py: Configuration loading, defaults\nids.py: ID parsing, generation, validation\nerrors.py: Custom exceptions\nrefs.py: Reference resolution logic\nstate.py: State machine definitions\naudit.py: Audit logging primitives\n\nkano_backlog_ops (new)\nUse-case functions that orchestrate operations:\n\ninit.py: Initialize backlog structure\nworkitem.py: Create, update, validate work items\nadr.py: Create, list ADRs\nworkset.py: Workset management (init/refresh/promote)\nview.py: Generate views, dashboards\nindex.py: Build/refresh SQLite index\n\nkano_backlog_adapters (new)\nPluggable backends:\n\nfs.py: File system operations (read/write markdown)\nsqlite.py: SQLite index adapter\nfts.py: Full-text search adapter\nembedding.py: Vector embedding adapter (optional)\nvcs/: VCS adapters (git, svn, perforce)\n\nkano_cli (existing, expand)\nTyper-based CLI application:\n\ncli.py: Main app, callback for gating\ncommands/: Subcommand modules (item, worklog, view, adr, index, workset)\nutil.py: CLI utilities (output formatting, path resolution)\n\nCLI Command Structure (Implemented)\nkano\r\n‚îú‚îÄ‚îÄ doctor              # Check prereqs + initialization\r\n‚îú‚îÄ‚îÄ backlog             # Backlog administration group\r\n‚îÇ   ‚îú‚îÄ‚îÄ init            # Initialize backlog structure\r\n‚îÇ   ‚îú‚îÄ‚îÄ index\r\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ build       # Build SQLite index\r\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ refresh     # Refresh index (MVP: full rebuild)\r\n‚îÇ   ‚îú‚îÄ‚îÄ demo\r\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ seed        # Seed demo data for testing\r\n‚îÇ   ‚îú‚îÄ‚îÄ persona\r\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ summary     # Generate persona activity summary\r\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ report      # Generate persona state report\r\n‚îÇ   ‚îî‚îÄ‚îÄ sandbox\r\n‚îÇ       ‚îî‚îÄ‚îÄ init        # Scaffold isolated sandbox environment\r\n‚îú‚îÄ‚îÄ item\r\n‚îÇ   ‚îú‚îÄ‚îÄ create          # Create work item\r\n‚îÇ   ‚îú‚îÄ‚îÄ read            # Read item details\r\n‚îÇ   ‚îú‚îÄ‚îÄ update-state    # Transition state + worklog append\r\n‚îÇ   ‚îú‚îÄ‚îÄ validate        # Check Ready gate\r\n‚îÇ   ‚îî‚îÄ‚îÄ create-v2       # Alias for create (compatibility)\r\n‚îú‚îÄ‚îÄ state\r\n‚îÇ   ‚îî‚îÄ‚îÄ transition      # Declarative state transitions\r\n‚îú‚îÄ‚îÄ worklog\r\n‚îÇ   ‚îî‚îÄ‚îÄ append          # Append worklog entry\r\n‚îú‚îÄ‚îÄ view\r\n‚îÇ   ‚îî‚îÄ‚îÄ refresh         # Refresh all dashboards\r\n‚îî‚îÄ‚îÄ init (legacy)       # Alias for `backlog init` (deprecated)\n\nMigration Strategy\nPhase 0: ADR + SKILL Gate (This ADR)\n\n Create this ADR with architecture diagrams\n Update SKILL.md: skill developers must read ADR-0013 before coding\n\nPhase 1: CLI Skeleton ‚úÖ COMPLETE\n\n Expanded src/kano_cli/commands/ to cover all high-frequency operations\n Add kano doctor for prereqs/init checks\n Implemented: item, state, worklog, view commands\n\nPhase 2: Library Migration ‚úÖ COMPLETE\n\n Created src/kano_backlog_ops/ with use-case functions (init, workitem, adr, view, index, demo, persona, sandbox)\n Created src/kano_backlog_adapters/ for backend abstraction (partially)\n Moved logic from scripts/backlog/*.py into library packages\n Added backlog subcommand group with nested commands (index, demo, persona, sandbox)\n\nPhase 3: Deprecation ‚úÖ COMPLETE\n\n Deleted 70+ legacy scripts from scripts/ directory\n Updated all documentation to recommend kano CLI\n Legacy kano init backlog aliased to kano backlog init with deprecation warning\n\nPhase 4: Future Extensions (Deferred)\n\nPlugin/hook system for external integrations\nNative engine option (C++/Rust via pybind11) for performance-critical paths\nHTTP/MCP server facade (reuses same kano_backlog_ops use-cases)\n\nTrade-offs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrade-offDescriptionMigration effortSignificant refactoring of existing scripts. Mitigated by phased approach.Backward compatibilityOld script paths break for agents. Mitigated by keeping thin wrappers.Initial complexityMore packages to maintain. Pays off with clearer boundaries and reusability.\nConsequences\n\nFor skill developers: Must read this ADR before adding code. New logic goes in src/, not scripts/.\nFor agents: Call only scripts/kano-backlog. Direct script calls are deprecated. See ADR-0015_skill-scoped-cli-namespace-convention for skill-scoped CLI naming convention.\nFor future facades: HTTP/MCP/GUI can import kano_backlog_ops directly, no CLI dependency.\nFor testing: Use-case functions in src/ are easier to unit test than CLI scripts.\nNaming convention: This skill follows skill-scoped naming (kano-backlog, kano_backlog_*). The bare kano namespace is reserved for a future umbrella CLI. See ADR-0015_skill-scoped-cli-namespace-convention for full rationale.\nInspector Pattern: External agents (health, review, security) consume query surface APIs from kano_backlog_ops, never write to canonical SoT directly. See ADR-0037_inspector-pattern-and-query-surface-architecture for full architecture.\n\nRelated\n\nKABSD-FTR-0028_refactor-kano-agent-backlog-skill-scripts-into-a-single-cli-entry-library-modules.md: Parent feature for this refactoring\nKABSD-FTR-0025_unified-cli-for-backlog-operations: Unified CLI (subset of this work)\nKABSD-FTR-0019_refactor-kano-backlog-core-cli-server-gui-facades: Core/CLI/Server/GUI facades separation\nADR-0004_file-first-architecture-with-sqlite-index: File-first architecture (complements this ADR)\nADR-0015_skill-scoped-cli-namespace-convention: Skill-scoped CLI namespace convention (naming strategy)\nADR-0037_inspector-pattern-and-query-surface-architecture: Inspector Pattern and Query Surface Architecture (extends module boundaries with external agent integration)\n"},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0014_plugin-and-hook-system-architecture":{"title":"Plugin and Hook System Architecture (Phase 4 - Deferred)","links":["demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0013_codebase-architecture-and-module-boundaries"],"tags":["architecture","extensibility","phase4","deferred"],"content":"Plugin and Hook System Architecture (Phase 4 - Deferred)\nStatus\nProposed - Design documented for future implementation. Phase 4 is deferred until concrete integration needs emerge.\nContext\nPer ADR-0013 Phase 4, we anticipate future needs for:\n\n\nPre/post hooks: External projects may want to inject custom logic before/after backlog operations (e.g., notify external systems, run custom validations, sync with remote databases).\n\n\nCustom engines: Performance-critical operations (workset retrieval, embedding search) may benefit from native (C++/Rust) implementations or external service integration.\n\n\nExternal tool integration: Projects may want to bridge kano backlog operations with external systems (JIRA sync, Slack notifications, custom dashboards).\n\n\nProblem: Hardcoding integrations into the core codebase violates separation of concerns and makes the skill harder to maintain.\nCurrent state: As of Phase 1-3 completion, kano_backlog_ops provides a clean use-case layer, but no hook/plugin mechanism exists.\nDecision\nDesign Principles\n\n\nInterface over Implementation: Define stable contracts (Python protocols/abstract base classes) that plugins must implement.\n\n\nRegistration-based Discovery: Plugins register themselves via entry points or configuration files; no hardcoded imports.\n\n\nFail-safe Defaults: If a plugin fails to load or execute, the system continues with default behavior (log warning but don‚Äôt crash).\n\n\nOptional Dependencies: Core skill must function without any plugins installed.\n\n\nProposed Architecture\nflowchart TB\r\n  subgraph CLI[&quot;kano CLI&quot;]\r\n    cmd[kano item create]\r\n  end\r\n  \r\n  subgraph Ops[&quot;kano_backlog_ops (Use-Cases)&quot;]\r\n    uc[create_item]\r\n  end\r\n  \r\n  subgraph Hooks[&quot;kano_backlog_hooks (Optional)&quot;]\r\n    mgr[HookManager]\r\n    pre[PreCreateHook protocol]\r\n    post[PostCreateHook protocol]\r\n  end\r\n  \r\n  subgraph Plugins[&quot;External Plugins (Optional)&quot;]\r\n    jira[jira-sync-plugin]\r\n    slack[slack-notify-plugin]\r\n    custom[custom-validation]\r\n  end\r\n  \r\n  subgraph Core[&quot;kano_backlog_core&quot;]\r\n    models[BacklogItem models]\r\n  end\r\n  \r\n  cmd --&gt; uc\r\n  uc --&gt; mgr\r\n  mgr --&gt; pre\r\n  pre -.optional.-&gt; jira\r\n  pre -.optional.-&gt; custom\r\n  mgr --&gt; post\r\n  post -.optional.-&gt; slack\r\n  uc --&gt; models\n\nHook Types\n\n\nOperation Hooks (Pre/Post):\n\nPreCreateHook: Validate/modify item before creation\nPostCreateHook: Notify external systems after creation\nPreUpdateStateHook: Block invalid state transitions\nPostUpdateStateHook: Trigger workflows on state change\n\n\n\nEngine Replacements:\n\nWorksetEngine: Interface for workset retrieval (default: Python, optional: Rust/C++)\nEmbeddingSearchEngine: Interface for ANN search (default: FAISS Python, optional: Qdrant/Milvus)\nIndexBuilder: Interface for index construction (default: SQLite Python, optional: DuckDB)\n\n\n\nPlugin Discovery\nOption A: Entry Points (Preferred for Python ecosystem)\n# pyproject.toml of external plugin\n[project.entry-points.&quot;kano_backlog.hooks&quot;]\njira-sync = &quot;jira_sync_plugin:JiraSyncHook&quot;\nOption B: Configuration File\n// _kano/backlog/.kano/plugins.json\n{\n  &quot;hooks&quot;: {\n    &quot;post_create&quot;: [&quot;jira_sync_plugin.JiraSyncHook&quot;, &quot;slack.NotifyHook&quot;]\n  },\n  &quot;engines&quot;: {\n    &quot;workset&quot;: &quot;workset_native.RustEngine&quot;\n  }\n}\nHook Protocol Example\nfrom typing import Protocol\nfrom kano_backlog_core.models import BacklogItem\n \nclass PreCreateHook(Protocol):\n    &quot;&quot;&quot;Protocol for pre-creation hooks.&quot;&quot;&quot;\n    \n    def execute(self, item: BacklogItem) -&gt; BacklogItem:\n        &quot;&quot;&quot;\n        Called before item creation.\n        \n        Args:\n            item: Item about to be created (may be modified)\n        \n        Returns:\n            Modified item (or original if no changes)\n        \n        Raises:\n            HookVetoError: If hook rejects the operation\n        &quot;&quot;&quot;\n        ...\n \nclass PostCreateHook(Protocol):\n    &quot;&quot;&quot;Protocol for post-creation hooks.&quot;&quot;&quot;\n    \n    def execute(self, item: BacklogItem) -&gt; None:\n        &quot;&quot;&quot;\n        Called after item creation.\n        \n        Args:\n            item: Newly created item (read-only)\n        \n        Note:\n            This should not raise exceptions; log errors internally\n        &quot;&quot;&quot;\n        ...\nConsequences\nPositive\n\nExtensibility: Projects can integrate custom logic without modifying skill core\nMaintainability: Core skill remains simple; complexity lives in plugins\nFlexibility: Users choose which plugins to install/enable\nEcosystem: Encourages community-contributed integrations\n\nNegative\n\nComplexity: Hook system adds architectural overhead (discovery, error handling, versioning)\nTesting burden: Must test core with/without plugins, handle plugin failures gracefully\nDocumentation: Need clear plugin development guide and hook lifecycle docs\nVersioning: Plugin API must be stable; breaking changes require migration path\n\nRisks\n\nPerformance overhead: Hook execution adds latency to every operation\n\nMitigation: Make hooks optional, measure overhead, provide async execution\n\n\nSecurity: Malicious plugins could corrupt backlog or leak data\n\nMitigation: Sandboxing (future), plugin allowlist/blocklist, audit logging\n\n\nDependency hell: Plugin A requires version X, plugin B requires version Y\n\nMitigation: Clear versioning policy, compatibility matrix\n\n\n\nImplementation Plan (When Needed)\nPhase 4a: Hook Interface (2-3 days)\n\nCreate src/kano_backlog_hooks/ package with protocol definitions\nAdd HookManager class for discovery and execution\nUpdate kano_backlog_ops to call hooks (with feature flag)\nWrite hook development guide\n\nPhase 4b: Engine Abstraction (3-5 days)\n\nDefine WorksetEngine, EmbeddingSearchEngine protocols\nRefactor existing code to use default implementations\nAdd engine registry and discovery\nDocument engine replacement guide\n\nPhase 4c: Plugin Ecosystem (Ongoing)\n\nCreate example plugins (jira-sync, slack-notify)\nPublish plugin template repository\nCurate community plugins list\n\nAlternatives Considered\n\n\nNo plugin system: Rely on users forking the skill\n\nRejected: Makes upgrades difficult, fragments ecosystem\n\n\n\nHardcoded integrations: Add JIRA/Slack support directly\n\nRejected: Violates separation of concerns, bloats codebase\n\n\n\nHTTP webhooks only: Use HTTP POST for all hooks\n\nRejected: Requires running server, adds network latency, no pre-hooks\n\n\n\nMonkey-patching: Let users patch functions at runtime\n\nRejected: Fragile, hard to test, no contract enforcement\n\n\n\nReferences\n\nADR-0013: Establishes use-case layer that hooks will integrate with\nPython Entry Points: packaging.python.org/en/latest/specifications/entry-points/\nProtocol classes (PEP 544): peps.python.org/pep-0544/\n\nRevision History\n\n2026-01-11: Initial proposal (copilot) - documented design for future Phase 4 implementation\n"},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0015_decision-for-kabsd-tsk-0056":{"title":"Decision for KABSD-TSK-0056","links":[],"tags":[],"content":"Status\nProposed\nContext\nThis decision was extracted from workset notes for task/feature KABSD-TSK-0056.\nDecision\nUse fine-grained chunking with semantic section boundaries instead of fixed-size windows. This preserves logical document structure for better retrieval accuracy.\nRationale\nSee workset notes in _kano\\backlog\\sandboxes.cache\\019b8f52-9fc8-7c94-aa2d-806cacdd9086 for full context.\nConsequences\n\nPositive:\n\n‚Ä¶\n\n\nNegative:\n\n‚Ä¶\n\n\nNeutral:\n\n‚Ä¶\n\n\n\nAlternatives Considered\n\nAlternative A: ‚Ä¶\nAlternative B: ‚Ä¶\n\nReferences\n\nExtracted from workset for KABSD-TSK-0056 by copilot\n"},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0015_skill-scoped-cli-namespace-convention":{"title":"Skill-Scoped CLI Namespace Convention","links":[],"tags":[],"content":"Decision\nEach skill repository MUST use skill-scoped naming for its CLI entrypoints and Python packages:\n\nCLI script: scripts/&lt;skill-name&gt; (e.g., scripts/kano-backlog)\nPython CLI package: &lt;skill_name&gt;_cli (e.g., kano_backlog_cli)\nConsole script entrypoint (in pyproject.toml): &lt;skill-name&gt; (e.g., kano-backlog)\n\nThe global name kano is reserved for a future umbrella CLI that will aggregate multiple skill CLIs. Individual skills MUST NOT claim the kano name in their own codebase.\nContext\nThe original implementation used universe-level names (kano, kano_cli) inside kano-agent-backlog-skill, assuming this was the only skill. As we add more skills (e.g., kano-commit-convention-skill), these names will collide.\nProblems with the current approach:\n\nkano is too generic for a single skill‚Äôs CLI\nMultiple skills can‚Äôt coexist if they all claim kano\nMigration confusion when the umbrella CLI is introduced later\n\nGoals:\n\nEnable multiple skill repos to coexist (each with its own scoped CLI)\nReserve kano as a future umbrella command aggregator\nMaintain consistency across all kano-ecosystem skills\n\nOptions Considered\n\n\nKeep kano as-is and ignore future skills ‚ùå\n\nRejected: causes immediate collision when adding second skill.\n\n\n\nUse skill-scoped naming (kano-backlog, kano_backlog_cli) ‚úÖ (chosen)\n\nEach skill is self-contained and independent.\nkano umbrella CLI can be added later without breaking existing skills.\n\n\n\nUse a monorepo with namespace packages\n\nRejected: conflicts with self-contained skill deployment model; skills are designed to be used as git submodules or standalone repos.\n\n\n\nPros / Cons\nPros:\n\nClear ownership: each skill owns its namespace\nNo collision when multiple skills are used together\nFuture-proof: umbrella CLI can be introduced as a separate repo\nAligns with ADR-0013 (module boundaries)\n\nCons:\n\nRequires renaming existing code (kano ‚Üí kano-backlog, kano_cli ‚Üí kano_backlog_cli)\nCommand tree changes (kano item ‚Üí kano-backlog workitem)\nMigration for existing users (mitigated by deprecation wrapper)\n\nConsequences\nImmediate actions (EPIC-0009):\n\nRename scripts/kano ‚Üí scripts/kano-backlog\nRename src/kano_cli ‚Üí src/kano_backlog_cli\nUpdate pyproject.toml entrypoint: kano-backlog instead of kano\nRestructure command groups (item ‚Üí workitem, backlog ‚Üí admin)\nUpdate all documentation (SKILL.md, README.md, REFERENCE.md)\nProvide deprecated kano wrapper script with migration warning\n\nLong-term (out of scope for this repo):\n\nFuture kano umbrella CLI repo can implement command delegation (e.g., kano backlog &lt;cmd&gt; ‚Üí kano-backlog &lt;cmd&gt;)\nEach skill continues to work standalone with its scoped CLI\n\nFollow-ups\n\n Implement renaming per KABSD-FTR-0034 (rename packages/scripts)\n Implement command tree restructuring per KABSD-FTR-0035\n Document reservation and future umbrella CLI design per KABSD-FTR-0036\n Update ADR-0013 to reference this naming convention\n"},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0016_per-product-isolated-index-architecture":{"title":"Per-Product Isolated Index Architecture","links":["items/feature/0000/KABSD-FTR-0010_monorepo-platform-migration","items/task/0000/KABSD-TSK-0084_update-indexer-and-resolver-for-product-isolation"],"tags":[],"content":"Context\nIn a multi-product monorepo environment, we needed to decide between two architectural approaches for the SQLite index:\n\nPlatform-level shared index: All products write to a single _kano/backlog/_index/backlog.sqlite3\nPer-product isolated index: Each product owns its own index at products/&lt;name&gt;/_index/backlog.sqlite3\n\nDecision Criteria\nWe evaluated 7 dimensions:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDimensionPer-ProductPlatformWinnerIsolationComplete separationShared namespacePer-ProductConcurrencyNo locking contentionPotential locksPer-ProductPerformancePredictable, isolatedCan degrade with loadPer-ProductScalabilityLinear per productShared bottleneckPer-ProductComplexitySimple, independentComplex coordinationPer-ProductMaintenanceEasy (per-product)Harder (shared state)Per-ProductFuture ExtensibilitySupports embedding DBDifficult to addPer-Product\nOverall Score: Per-Product = 91%, Platform = 43%\nDecision\nImplement per-product isolated SQLite indexes.\nEach product will:\n\nMaintain its own SQLite database at products/&lt;product-name&gt;/_index/backlog.sqlite3\nHave independent schema with product column for self-documentation\nSupport autonomous indexing/rebuilding without affecting other products\nEnable future cross-product aggregation via product-aware queries\n\nImplementation Details\nSchema Design\nAll tables include product column:\nCREATE TABLE items (\n  product TEXT NOT NULL,\n  id TEXT NOT NULL,\n  PRIMARY KEY (product, id),\n  ...\n);\n \nCREATE TABLE item_tags (\n  product TEXT NOT NULL,\n  item_id TEXT NOT NULL,\n  tag TEXT NOT NULL,\n  PRIMARY KEY (product, item_id, tag),\n  FOREIGN KEY (product, item_id) REFERENCES items(product, id)\n);\nIndex Rebuild Process\n\nscripts/indexing/build_sqlite_index.py --product &lt;name&gt;\nScans products/&lt;name&gt;/items/ directory\nExtracts product from file paths\nUpserts into product-specific SQLite database\nMaintains composite key integrity\n\nResolver Behavior\n\nresolve_ref(ref, index, product=None) accepts optional product filter\nIf product not specified, searches across all products\nProduct column enables cross-product queries when needed\n\nRationale\n\nIsolation ensures safety: No possibility of data leakage between products\nPredictable performance: Each product‚Äôs index grows independently\nSupports autonomy: Teams can rebuild their product‚Äôs index without coordination\nFuture-proof: Enables embedding databases per product or shared aggregation\nSimpler mental model: Products are truly independent\nBackward compatible: Product column facilitates future migrations\n\nAlternatives Considered\nPlatform-level shared index\n\nPros: Unified search across all products\nCons: Complex coordination, shared bottleneck, potential data leakage\nRejected: Architecture does not support team autonomy\n\nHybrid approach (shared metadata + per-product data)\n\nPros: Balanced complexity\nCons: Still requires coordination layer, adds complexity\nRejected: Per-product isolation is simpler and cleaner\n\nFuture Work\nWhen cross-product features are needed:\n\nCreate aggregation layer on top of per-product indexes\nOr implement global embedding database that reads from per-product sources\nProduct column in schema already prepared for this extensibility\n\nReferences\n\nKABSD-FTR-0010_monorepo-platform-migration.md: Monorepo Platform Migration feature\nKABSD-TSK-0084_update-indexer-and-resolver-for-product-isolation.md: Indexer and resolver product isolation\n"},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0017_product-column-retention-rationale":{"title":"Product Column Retention in Per-Product Indexes","links":["items/feature/0000/KABSD-FTR-0010_monorepo-platform-migration","demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0016_per-product-isolated-index-architecture"],"tags":[],"content":"Context\nWhen implementing per-product isolated indexes, a question arose: if each product owns its own SQLite database, why include a product column in the schema?\nArguments for removing it:\n\nRedundant (the file path already indicates product)\nAdds storage overhead\nColumn values are always the same for a given database\n\nArguments for keeping it:\n\nSelf-documentation (data independence from file paths)\nConsistency with composite key patterns\nFuture extensibility (global embedding DB, cross-product queries)\nError detection capability\n\nDecision\nRetain the product column in all schema tables.\nEach product‚Äôs schema will include a product column despite the apparent redundancy.\nRationale\n1. Data Self-Documentation\nThe product column makes data self-describing. If a database dump or export is created, the product context is explicit in the data itself, not dependent on file path or metadata.\nSELECT * FROM items WHERE product=&#039;kano-agent-backlog-skill&#039;;\n-- vs.\nSELECT * FROM items;  -- How do you know which product this is from?\n2. Consistency with Composite Key Strategy\nUsing (product, id) as composite primary keys throughout the schema ensures:\n\nAll foreign keys are uniform: FOREIGN KEY (product, item_id)\nConsistent pattern across all tables (items, item_tags, item_links, worklog_entries)\nEasier code generation and migrations\n\n3. Uniform Schema Across Products\nAll products use identical schema structure. A tool that works with KABSD‚Äôs index works with any product‚Äôs index without modification. This is valuable for:\n\nShared tool development\nSchema migration scripts\nIndex validation and repair tools\n\n4. Future Extensibility: Global Embedding Database\nThe primary long-term value is supporting a future global embedding database:\n-- Global embedding store (future)\nCREATE TABLE embeddings (\n  product TEXT NOT NULL,\n  item_id TEXT NOT NULL,\n  embedding BLOB NOT NULL,\n  PRIMARY KEY (product, item_id),\n  FOREIGN KEY (product, item_id) REFERENCES all_items(product, id)\n);\nThis would aggregate embeddings from all products while maintaining product context. The product column enables this without schema rework.\n5. Error Detection\nIncluding product column in per-product indexes enables validation queries:\n-- Verify database integrity\nSELECT DISTINCT product FROM items;\n-- Should always return single row: kano-agent-backlog-skill\n \n-- Detect misplaced files\nSELECT product FROM items WHERE product != &#039;kano-agent-backlog-skill&#039;;\n-- Should return empty set\n6. Cross-Product Queries (Future)\nWhen analytics or reporting tools are built, they may aggregate across products:\n-- Future: Query across products via union or aggregation\nSELECT product, COUNT(*) as item_count FROM all_items GROUP BY product;\nThe product column is essential for this without painful migrations.\nImplementation\nAll tables maintain the pattern:\nCREATE TABLE items (\n  product TEXT NOT NULL,\n  id TEXT NOT NULL,\n  uid UUID,\n  type TEXT,\n  -- ... other columns\n  PRIMARY KEY (product, id)\n);\n \nCREATE TABLE item_tags (\n  product TEXT NOT NULL,\n  item_id TEXT NOT NULL,\n  tag TEXT NOT NULL,\n  PRIMARY KEY (product, item_id, tag),\n  FOREIGN KEY (product, item_id) REFERENCES items(product, id)\n);\nAlternatives Considered\nRemove product column entirely\n\nPros: Minimal storage, no apparent redundancy\nCons: Breaks future embedding DB design, harder to validate integrity, poor data self-documentation\nRejected: Future extensibility cost is too high\n\nMake product optional/nullable\n\nPros: Allows querying ‚Äúwhich product‚Äù implicitly\nCons: Makes schema ambiguous, difficult to enforce consistency\nRejected: Product should always be explicit and required\n\nFuture Work\nWhen global embedding database is implemented:\n\nProduct column in schema already prepared\nNo schema migration required\nTools can immediately work with cross-product data\n\nReferences\n\nKABSD-FTR-0010_monorepo-platform-migration.md: Monorepo Platform Migration\nADR-0016_per-product-isolated-index-architecture: Per-Product Isolated Index Architecture\n"},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0018_decision-for-kabsd-tsk-0056":{"title":"Decision for KABSD-TSK-0056","links":[],"tags":[],"content":"Status\nProposed\nContext\nThis decision was extracted from workset notes for task/feature KABSD-TSK-0056.\nDecision\nUse fine-grained chunking with semantic section boundaries instead of fixed-size windows. This preserves logical document structure for better retrieval accuracy.\nRationale\nSee workset notes in _kano\\backlog\\sandboxes.cache\\019b8f52-9fc8-7c94-aa2d-806cacdd9086 for full context.\nConsequences\n\nPositive:\n\n‚Ä¶\n\n\nNegative:\n\n‚Ä¶\n\n\nNeutral:\n\n‚Ä¶\n\n\n\nAlternatives Considered\n\nAlternative A: ‚Ä¶\nAlternative B: ‚Ä¶\n\nReferences\n\nExtracted from workset for KABSD-TSK-0056 by copilot\n"},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0019_decision-for-kabsd-tsk-0056":{"title":"Decision for KABSD-TSK-0056","links":[],"tags":[],"content":"Status\nProposed\nContext\nThis decision was extracted from workset notes for task/feature KABSD-TSK-0056.\nDecision\nUse fine-grained chunking with semantic section boundaries instead of fixed-size windows. This preserves logical document structure for better retrieval accuracy.\nRationale\nSee workset notes in _kano\\backlog\\sandboxes.cache\\019b8f52-9fc8-7c94-aa2d-806cacdd9086 for full context.\nConsequences\n\nPositive:\n\n‚Ä¶\n\n\nNegative:\n\n‚Ä¶\n\n\nNeutral:\n\n‚Ä¶\n\n\n\nAlternatives Considered\n\nAlternative A: ‚Ä¶\nAlternative B: ‚Ä¶\n\nReferences\n\nExtracted from workset for KABSD-TSK-0056 by copilot\n"},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0020_decision-for-kabsd-tsk-0056":{"title":"Decision for KABSD-TSK-0056","links":[],"tags":[],"content":"Status\nProposed\nContext\nThis decision was extracted from workset notes for task/feature KABSD-TSK-0056.\nDecision\nUse fine-grained chunking with semantic section boundaries instead of fixed-size windows. This preserves logical document structure for better retrieval accuracy.\nRationale\nSee workset notes in _kano\\backlog\\sandboxes.cache\\019b8f52-9fc8-7c94-aa2d-806cacdd9086 for full context.\nConsequences\n\nPositive:\n\n‚Ä¶\n\n\nNegative:\n\n‚Ä¶\n\n\nNeutral:\n\n‚Ä¶\n\n\n\nAlternatives Considered\n\nAlternative A: ‚Ä¶\nAlternative B: ‚Ä¶\n\nReferences\n\nExtracted from workset for KABSD-TSK-0056 by copilot\n"},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0021_per-product-isolated-index-architecture":{"title":"Per-Product Isolated Index Architecture","links":["items/feature/0000/KABSD-FTR-0010_monorepo-platform-migration","items/task/0000/KABSD-TSK-0084_update-indexer-and-resolver-for-product-isolation"],"tags":[],"content":"Context\nIn a multi-product monorepo environment, we needed to decide between two architectural approaches for the SQLite index:\n\nPlatform-level shared index: All products write to a single _kano/backlog/_index/backlog.sqlite3\nPer-product isolated index: Each product owns its own index at products/&lt;name&gt;/_index/backlog.sqlite3\n\nDecision Criteria\nWe evaluated 7 dimensions:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDimensionPer-ProductPlatformWinnerIsolationComplete separationShared namespacePer-ProductConcurrencyNo locking contentionPotential locksPer-ProductPerformancePredictable, isolatedCan degrade with loadPer-ProductScalabilityLinear per productShared bottleneckPer-ProductComplexitySimple, independentComplex coordinationPer-ProductMaintenanceEasy (per-product)Harder (shared state)Per-ProductFuture ExtensibilitySupports embedding DBDifficult to addPer-Product\nOverall Score: Per-Product = 91%, Platform = 43%\nDecision\nImplement per-product isolated SQLite indexes.\nEach product will:\n\nMaintain its own SQLite database at products/&lt;product-name&gt;/_index/backlog.sqlite3\nHave independent schema with product column for self-documentation\nSupport autonomous indexing/rebuilding without affecting other products\nEnable future cross-product aggregation via product-aware queries\n\nImplementation Details\nSchema Design\nAll tables include product column:\nCREATE TABLE items (\n  product TEXT NOT NULL,\n  id TEXT NOT NULL,\n  PRIMARY KEY (product, id),\n  ...\n);\n \nCREATE TABLE item_tags (\n  product TEXT NOT NULL,\n  item_id TEXT NOT NULL,\n  tag TEXT NOT NULL,\n  PRIMARY KEY (product, item_id, tag),\n  FOREIGN KEY (product, item_id) REFERENCES items(product, id)\n);\nIndex Rebuild Process\n\nscripts/indexing/build_sqlite_index.py --product &lt;name&gt;\nScans products/&lt;name&gt;/items/ directory\nExtracts product from file paths\nUpserts into product-specific SQLite database\nMaintains composite key integrity\n\nResolver Behavior\n\nresolve_ref(ref, index, product=None) accepts optional product filter\nIf product not specified, searches across all products\nProduct column enables cross-product queries when needed\n\nRationale\n\nIsolation ensures safety: No possibility of data leakage between products\nPredictable performance: Each product‚Äôs index grows independently\nSupports autonomy: Teams can rebuild their product‚Äôs index without coordination\nFuture-proof: Enables embedding databases per product or shared aggregation\nSimpler mental model: Products are truly independent\nBackward compatible: Product column facilitates future migrations\n\nAlternatives Considered\nPlatform-level shared index\n\nPros: Unified search across all products\nCons: Complex coordination, shared bottleneck, potential data leakage\nRejected: Architecture does not support team autonomy\n\nHybrid approach (shared metadata + per-product data)\n\nPros: Balanced complexity\nCons: Still requires coordination layer, adds complexity\nRejected: Per-product isolation is simpler and cleaner\n\nFuture Work\nWhen cross-product features are needed:\n\nCreate aggregation layer on top of per-product indexes\nOr implement global embedding database that reads from per-product sources\nProduct column in schema already prepared for this extensibility\n\nReferences\n\nKABSD-FTR-0010_monorepo-platform-migration.md: Monorepo Platform Migration feature\nKABSD-TSK-0084_update-indexer-and-resolver-for-product-isolation.md: Indexer and resolver product isolation\n"},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0022_product-column-retention-rationale":{"title":"Product Column Retention in Per-Product Indexes","links":["items/feature/0000/KABSD-FTR-0010_monorepo-platform-migration","demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0004_file-first-architecture-with-sqlite-index"],"tags":[],"content":"Context\nWhen implementing per-product isolated indexes, a question arose: if each product owns its own SQLite database, why include a product column in the schema?\nArguments for removing it:\n\nRedundant (the file path already indicates product)\nAdds storage overhead\nColumn values are always the same for a given database\n\nArguments for keeping it:\n\nSelf-documentation (data independence from file paths)\nConsistency with composite key patterns\nFuture extensibility (global embedding DB, cross-product queries)\nError detection capability\n\nDecision\nRetain the product column in all schema tables.\nEach product‚Äôs schema will include a product column despite the apparent redundancy.\nRationale\n1. Data Self-Documentation\nThe product column makes data self-describing. If a database dump or export is created, the product context is explicit in the data itself, not dependent on file path or metadata.\nSELECT * FROM items WHERE product=&#039;kano-agent-backlog-skill&#039;;\n-- vs.\nSELECT * FROM items;  -- How do you know which product this is from?\n2. Consistency with Composite Key Strategy\nUsing (product, id) as composite primary keys throughout the schema ensures:\n\nAll foreign keys are uniform: FOREIGN KEY (product, item_id)\nConsistent pattern across all tables (items, item_tags, item_links, worklog_entries)\nEasier code generation and migrations\n\n3. Uniform Schema Across Products\nAll products use identical schema structure. A tool that works with KABSD‚Äôs index works with any product‚Äôs index without modification. This is valuable for:\n\nShared tool development\nSchema migration scripts\nIndex validation and repair tools\n\n4. Future Extensibility: Global Embedding Database\nThe primary long-term value is supporting a future global embedding database:\n-- Global embedding store (future)\nCREATE TABLE embeddings (\n  product TEXT NOT NULL,\n  item_id TEXT NOT NULL,\n  embedding BLOB NOT NULL,\n  PRIMARY KEY (product, item_id),\n  FOREIGN KEY (product, item_id) REFERENCES all_items(product, id)\n);\nThis would aggregate embeddings from all products while maintaining product context. The product column enables this without schema rework.\n5. Error Detection\nIncluding product column in per-product indexes enables validation queries:\n-- Verify database integrity\nSELECT DISTINCT product FROM items;\n-- Should always return single row: kano-agent-backlog-skill\n \n-- Detect misplaced files\nSELECT product FROM items WHERE product != &#039;kano-agent-backlog-skill&#039;;\n-- Should return empty set\n6. Cross-Product Queries (Future)\nWhen analytics or reporting tools are built, they may aggregate across products:\n-- Future: Query across products via union or aggregation\nSELECT product, COUNT(*) as item_count FROM all_items GROUP BY product;\nThe product column is essential for this without painful migrations.\nImplementation\nAll tables maintain the pattern:\nCREATE TABLE items (\n  product TEXT NOT NULL,\n  id TEXT NOT NULL,\n  uid UUID,\n  type TEXT,\n  -- ... other columns\n  PRIMARY KEY (product, id)\n);\n \nCREATE TABLE item_tags (\n  product TEXT NOT NULL,\n  item_id TEXT NOT NULL,\n  tag TEXT NOT NULL,\n  PRIMARY KEY (product, item_id, tag),\n  FOREIGN KEY (product, item_id) REFERENCES items(product, id)\n);\nAlternatives Considered\nRemove product column entirely\n\nPros: Minimal storage, no apparent redundancy\nCons: Breaks future embedding DB design, harder to validate integrity, poor data self-documentation\nRejected: Future extensibility cost is too high\n\nMake product optional/nullable\n\nPros: Allows querying ‚Äúwhich product‚Äù implicitly\nCons: Makes schema ambiguous, difficult to enforce consistency\nRejected: Product should always be explicit and required\n\nFuture Work\nWhen global embedding database is implemented:\n\nProduct column in schema already prepared\nNo schema migration required\nTools can immediately work with cross-product data\n\nReferences\n\nKABSD-FTR-0010_monorepo-platform-migration.md: Monorepo Platform Migration\nADR-0004_file-first-architecture-with-sqlite-index: Per-Product Isolated Index Architecture\n"},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0023_graph-assisted-retrieval-and-context-graph":{"title":"Graph-assisted retrieval with a derived Context Graph (weak graph first)","links":["demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0004_file-first-architecture-with-sqlite-index","demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0009_local-first-embedding-search-architecture","items/feature/0000/KABSD-FTR-0007_optional-db-index-and-embedding-rag-pipeline","items/feature/0000/KABSD-FTR-0023_graph-assisted-rag-planning-and-minimal-implementation"],"tags":[],"content":"Decision\nAdopt Graph-assisted retrieval as a minimal, local-first improvement to context quality:\n\nUse FTS/embeddings to retrieve seed nodes\nUse a derived Context Graph to expand to load-bearing neighbors (k-hop traversal)\nKeep everything derived/rebuildable from canonical Markdown (file-first)\n\nThis ADR explicitly chooses weak graph first: only structured relationships (no LLM entity extraction).\nContext\nWe already have a file-first backlog with optional derived indexes (SQLite / FTS / embeddings).\r\nVector-only retrieval often returns text-similar chunks but misses the structural context (parents, ADR decisions, dependency chains).\nWe want a deterministic, auditable way to expand context that is:\n\nlocal-first\nderived/rebuildable\nincrementally maintainable\nsafe (bounded expansion to avoid prompt bloat)\n\nDefinitions\n\nContext Graph: a derived, typed graph of artifact relationships (items, ADRs, dependencies, etc.).\nSeed set: top-N nodes from FTS/embedding retrieval.\nGraph expansion: k-hop traversal from seeds over allowlisted edges with limits.\n\nGraph model (v1)\nNodes\nMinimum node types:\n\nwork_item (Epic/Feature/UserStory/Task/Bug)\nadr\n\nOptional (for embedding/fts pipelines):\n\nchunk (document chunk tied to a parent doc)\n\nEdges\nMinimum edge types:\n\nparent (child ‚Üí parent)\ndecision_ref (work_item ‚Üí adr)\nrelates (work_item ‚Üí work_item)\nblocks / blocked_by\n\nStorage (derived)\nThe Context Graph is derived data. Implementations may:\n\n\nMaterialize into SQLite\n\nreuse items as the node registry\nstore edges in a links-style table (source_uid, target_uid, type, optional weight, source_path)\n\n\n\nSidecar graph artifacts\n\n&lt;backlog-root&gt;/_index/graph_nodes.jsonl\n&lt;backlog-root&gt;/_index/graph_edges.jsonl\n\n\n\nBoth must be safe to delete and rebuild.\nRetrieval strategy (Graph-assisted RAG)\n\nSeed retrieval\n\nFTS and/or embeddings return top-N seed nodes/chunks\n\n\nExpand\n\ntraverse k-hop (default k=1)\nedge allowlist and fanout caps\n\n\nRe-rank\n\nweights by doctype and section (ADR decision &gt; item title/acceptance &gt; worklog)\noptionally prioritize Ready/InProgress items\n\n\nContext packing\n\nemit a context pack describing:\n\nseeds (why selected)\nneighbors (which edge pulled them in)\nminimal excerpts/anchors (title/ids/links)\n\n\n\n\n\nConfig surface (indicative)\n\nretrieval.graph.enabled\nretrieval.graph.k_hop\nretrieval.graph.edge_allowlist\nretrieval.graph.max_neighbors_per_seed\nretrieval.weights.* (doctype/section/state weights)\n\nConsequences\n\nGraph-assisted retrieval becomes the preferred way to preserve traceability (seed + neighbors).\nTooling must keep expansion bounded to avoid context explosions.\nThe design stays compatible with file-scan fallback and optional SQLite/embedding acceleration.\n\nNon-goals\n\nLLM/NLP entity extraction and automatic relation mining\nserver/MCP mode or cross-repo graphs\n\nReferences\n\nADR-0004 File-first + SQLite index\nADR-0009 Local-first embedding search\nRAG pipeline\nKABSD-FTR-0023 Graph-assisted RAG planning\n"},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0024_decision-for-kabsd-tsk-0056":{"title":"Decision for KABSD-TSK-0056","links":[],"tags":[],"content":"Status\nProposed\nContext\nThis decision was extracted from workset notes for task/feature KABSD-TSK-0056.\nDecision\nUse fine-grained chunking with semantic section boundaries instead of fixed-size windows. This preserves logical document structure for better retrieval accuracy.\nRationale\nSee workset notes in _kano\\backlog\\sandboxes.cache\\019b8f52-9fc8-7c94-aa2d-806cacdd9086 for full context.\nConsequences\n\nPositive:\n\n‚Ä¶\n\n\nNegative:\n\n‚Ä¶\n\n\nNeutral:\n\n‚Ä¶\n\n\n\nAlternatives Considered\n\nAlternative A: ‚Ä¶\nAlternative B: ‚Ä¶\n\nReferences\n\nExtracted from workset for KABSD-TSK-0056 by copilot\n"},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0025_decision-for-kabsd-tsk-0056":{"title":"Decision for KABSD-TSK-0056","links":[],"tags":[],"content":"Status\nProposed\nContext\nThis decision was extracted from workset notes for task/feature KABSD-TSK-0056.\nDecision\nUse fine-grained chunking with semantic section boundaries instead of fixed-size windows. This preserves logical document structure for better retrieval accuracy.\nRationale\nSee workset notes in _kano\\backlog\\sandboxes.cache\\019b8f52-9fc8-7c94-aa2d-806cacdd9086 for full context.\nConsequences\n\nPositive:\n\n‚Ä¶\n\n\nNegative:\n\n‚Ä¶\n\n\nNeutral:\n\n‚Ä¶\n\n\n\nAlternatives Considered\n\nAlternative A: ‚Ä¶\nAlternative B: ‚Ä¶\n\nReferences\n\nExtracted from workset for KABSD-TSK-0056 by copilot\n"},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0026_decision-for-kabsd-tsk-0056":{"title":"Decision for KABSD-TSK-0056","links":[],"tags":[],"content":"Status\nProposed\nContext\nThis decision was extracted from workset notes for task/feature KABSD-TSK-0056.\nDecision\nUse fine-grained chunking with semantic section boundaries instead of fixed-size windows. This preserves logical document structure for better retrieval accuracy.\nRationale\nSee workset notes in _kano\\backlog\\sandboxes.cache\\019b8f52-9fc8-7c94-aa2d-806cacdd9086 for full context.\nConsequences\n\nPositive:\n\n‚Ä¶\n\n\nNegative:\n\n‚Ä¶\n\n\nNeutral:\n\n‚Ä¶\n\n\n\nAlternatives Considered\n\nAlternative A: ‚Ä¶\nAlternative B: ‚Ä¶\n\nReferences\n\nExtracted from workset for KABSD-TSK-0056 by copilot\n"},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0027_decision-for-kabsd-tsk-0056":{"title":"Decision for KABSD-TSK-0056","links":[],"tags":[],"content":"Status\nProposed\nContext\nThis decision was extracted from workset notes for task/feature KABSD-TSK-0056.\nDecision\nUse fine-grained chunking with semantic section boundaries instead of fixed-size windows. This preserves logical document structure for better retrieval accuracy.\nRationale\nSee workset notes in _kano\\backlog\\sandboxes.cache\\019b8f52-9fc8-7c94-aa2d-806cacdd9086 for full context.\nConsequences\n\nPositive:\n\n‚Ä¶\n\n\nNegative:\n\n‚Ä¶\n\n\nNeutral:\n\n‚Ä¶\n\n\n\nAlternatives Considered\n\nAlternative A: ‚Ä¶\nAlternative B: ‚Ä¶\n\nReferences\n\nExtracted from workset for KABSD-TSK-0056 by copilot\n"},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0028_per-product-isolated-index-architecture":{"title":"Per-Product Isolated Index Architecture","links":["items/feature/0000/KABSD-FTR-0010_monorepo-platform-migration","items/task/0000/KABSD-TSK-0084_update-indexer-and-resolver-for-product-isolation"],"tags":[],"content":"Context\nIn a multi-product monorepo environment, we needed to decide between two architectural approaches for the SQLite index:\n\nPlatform-level shared index: All products write to a single _kano/backlog/_index/backlog.sqlite3\nPer-product isolated index: Each product owns its own index at products/&lt;name&gt;/_index/backlog.sqlite3\n\nDecision Criteria\nWe evaluated 7 dimensions:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDimensionPer-ProductPlatformWinnerIsolationComplete separationShared namespacePer-ProductConcurrencyNo locking contentionPotential locksPer-ProductPerformancePredictable, isolatedCan degrade with loadPer-ProductScalabilityLinear per productShared bottleneckPer-ProductComplexitySimple, independentComplex coordinationPer-ProductMaintenanceEasy (per-product)Harder (shared state)Per-ProductFuture ExtensibilitySupports embedding DBDifficult to addPer-Product\nOverall Score: Per-Product = 91%, Platform = 43%\nDecision\nImplement per-product isolated SQLite indexes.\nEach product will:\n\nMaintain its own SQLite database at products/&lt;product-name&gt;/_index/backlog.sqlite3\nHave independent schema with product column for self-documentation\nSupport autonomous indexing/rebuilding without affecting other products\nEnable future cross-product aggregation via product-aware queries\n\nImplementation Details\nSchema Design\nAll tables include product column:\nCREATE TABLE items (\n  product TEXT NOT NULL,\n  id TEXT NOT NULL,\n  PRIMARY KEY (product, id),\n  ...\n);\n \nCREATE TABLE item_tags (\n  product TEXT NOT NULL,\n  item_id TEXT NOT NULL,\n  tag TEXT NOT NULL,\n  PRIMARY KEY (product, item_id, tag),\n  FOREIGN KEY (product, item_id) REFERENCES items(product, id)\n);\nIndex Rebuild Process\n\nscripts/indexing/build_sqlite_index.py --product &lt;name&gt;\nScans products/&lt;name&gt;/items/ directory\nExtracts product from file paths\nUpserts into product-specific SQLite database\nMaintains composite key integrity\n\nResolver Behavior\n\nresolve_ref(ref, index, product=None) accepts optional product filter\nIf product not specified, searches across all products\nProduct column enables cross-product queries when needed\n\nRationale\n\nIsolation ensures safety: No possibility of data leakage between products\nPredictable performance: Each product‚Äôs index grows independently\nSupports autonomy: Teams can rebuild their product‚Äôs index without coordination\nFuture-proof: Enables embedding databases per product or shared aggregation\nSimpler mental model: Products are truly independent\nBackward compatible: Product column facilitates future migrations\n\nAlternatives Considered\nPlatform-level shared index\n\nPros: Unified search across all products\nCons: Complex coordination, shared bottleneck, potential data leakage\nRejected: Architecture does not support team autonomy\n\nHybrid approach (shared metadata + per-product data)\n\nPros: Balanced complexity\nCons: Still requires coordination layer, adds complexity\nRejected: Per-product isolation is simpler and cleaner\n\nFuture Work\nWhen cross-product features are needed:\n\nCreate aggregation layer on top of per-product indexes\nOr implement global embedding database that reads from per-product sources\nProduct column in schema already prepared for this extensibility\n\nReferences\n\nKABSD-FTR-0010_monorepo-platform-migration.md: Monorepo Platform Migration feature\nKABSD-TSK-0084_update-indexer-and-resolver-for-product-isolation.md: Indexer and resolver product isolation\n"},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0029_product-column-retention-rationale":{"title":"Product Column Retention in Per-Product Indexes","links":["items/feature/0000/KABSD-FTR-0010_monorepo-platform-migration","demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0004_file-first-architecture-with-sqlite-index"],"tags":[],"content":"Context\nWhen implementing per-product isolated indexes, a question arose: if each product owns its own SQLite database, why include a product column in the schema?\nArguments for removing it:\n\nRedundant (the file path already indicates product)\nAdds storage overhead\nColumn values are always the same for a given database\n\nArguments for keeping it:\n\nSelf-documentation (data independence from file paths)\nConsistency with composite key patterns\nFuture extensibility (global embedding DB, cross-product queries)\nError detection capability\n\nDecision\nRetain the product column in all schema tables.\nEach product‚Äôs schema will include a product column despite the apparent redundancy.\nRationale\n1. Data Self-Documentation\nThe product column makes data self-describing. If a database dump or export is created, the product context is explicit in the data itself, not dependent on file path or metadata.\nSELECT * FROM items WHERE product=&#039;kano-agent-backlog-skill&#039;;\n-- vs.\nSELECT * FROM items;  -- How do you know which product this is from?\n2. Consistency with Composite Key Strategy\nUsing (product, id) as composite primary keys throughout the schema ensures:\n\nAll foreign keys are uniform: FOREIGN KEY (product, item_id)\nConsistent pattern across all tables (items, item_tags, item_links, worklog_entries)\nEasier code generation and migrations\n\n3. Uniform Schema Across Products\nAll products use identical schema structure. A tool that works with KABSD‚Äôs index works with any product‚Äôs index without modification. This is valuable for:\n\nShared tool development\nSchema migration scripts\nIndex validation and repair tools\n\n4. Future Extensibility: Global Embedding Database\nThe primary long-term value is supporting a future global embedding database:\n-- Global embedding store (future)\nCREATE TABLE embeddings (\n  product TEXT NOT NULL,\n  item_id TEXT NOT NULL,\n  embedding BLOB NOT NULL,\n  PRIMARY KEY (product, item_id),\n  FOREIGN KEY (product, item_id) REFERENCES all_items(product, id)\n);\nThis would aggregate embeddings from all products while maintaining product context. The product column enables this without schema rework.\n5. Error Detection\nIncluding product column in per-product indexes enables validation queries:\n-- Verify database integrity\nSELECT DISTINCT product FROM items;\n-- Should always return single row: kano-agent-backlog-skill\n \n-- Detect misplaced files\nSELECT product FROM items WHERE product != &#039;kano-agent-backlog-skill&#039;;\n-- Should return empty set\n6. Cross-Product Queries (Future)\nWhen analytics or reporting tools are built, they may aggregate across products:\n-- Future: Query across products via union or aggregation\nSELECT product, COUNT(*) as item_count FROM all_items GROUP BY product;\nThe product column is essential for this without painful migrations.\nImplementation\nAll tables maintain the pattern:\nCREATE TABLE items (\n  product TEXT NOT NULL,\n  id TEXT NOT NULL,\n  uid UUID,\n  type TEXT,\n  -- ... other columns\n  PRIMARY KEY (product, id)\n);\n \nCREATE TABLE item_tags (\n  product TEXT NOT NULL,\n  item_id TEXT NOT NULL,\n  tag TEXT NOT NULL,\n  PRIMARY KEY (product, item_id, tag),\n  FOREIGN KEY (product, item_id) REFERENCES items(product, id)\n);\nAlternatives Considered\nRemove product column entirely\n\nPros: Minimal storage, no apparent redundancy\nCons: Breaks future embedding DB design, harder to validate integrity, poor data self-documentation\nRejected: Future extensibility cost is too high\n\nMake product optional/nullable\n\nPros: Allows querying ‚Äúwhich product‚Äù implicitly\nCons: Makes schema ambiguous, difficult to enforce consistency\nRejected: Product should always be explicit and required\n\nFuture Work\nWhen global embedding database is implemented:\n\nProduct column in schema already prepared\nNo schema migration required\nTools can immediately work with cross-product data\n\nReferences\n\nKABSD-FTR-0010_monorepo-platform-migration.md: Monorepo Platform Migration\nADR-0004_file-first-architecture-with-sqlite-index: Per-Product Isolated Index Architecture\n"},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0030_graph-assisted-retrieval-and-context-graph":{"title":"Graph-assisted retrieval with a derived Context Graph (weak graph first)","links":["demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0004_file-first-architecture-with-sqlite-index","demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0009_local-first-embedding-search-architecture","items/feature/0000/KABSD-FTR-0007_optional-db-index-and-embedding-rag-pipeline","items/feature/0000/KABSD-FTR-0023_graph-assisted-rag-planning-and-minimal-implementation"],"tags":[],"content":"Decision\nAdopt Graph-assisted retrieval as a minimal, local-first improvement to context quality:\n\nUse FTS/embeddings to retrieve seed nodes\nUse a derived Context Graph to expand to load-bearing neighbors (k-hop traversal)\nKeep everything derived/rebuildable from canonical Markdown (file-first)\n\nThis ADR explicitly chooses weak graph first: only structured relationships (no LLM entity extraction).\nContext\nWe already have a file-first backlog with optional derived indexes (SQLite / FTS / embeddings).\r\nVector-only retrieval often returns text-similar chunks but misses the structural context (parents, ADR decisions, dependency chains).\nWe want a deterministic, auditable way to expand context that is:\n\nlocal-first\nderived/rebuildable\nincrementally maintainable\nsafe (bounded expansion to avoid prompt bloat)\n\nDefinitions\n\nContext Graph: a derived, typed graph of artifact relationships (items, ADRs, dependencies, etc.).\nSeed set: top-N nodes from FTS/embedding retrieval.\nGraph expansion: k-hop traversal from seeds over allowlisted edges with limits.\n\nGraph model (v1)\nNodes\nMinimum node types:\n\nwork_item (Epic/Feature/UserStory/Task/Bug)\nadr\n\nOptional (for embedding/fts pipelines):\n\nchunk (document chunk tied to a parent doc)\n\nEdges\nMinimum edge types:\n\nparent (child ‚Üí parent)\ndecision_ref (work_item ‚Üí adr)\nrelates (work_item ‚Üí work_item)\nblocks / blocked_by\n\nStorage (derived)\nThe Context Graph is derived data. Implementations may:\n\n\nMaterialize into SQLite\n\nreuse items as the node registry\nstore edges in a links-style table (source_uid, target_uid, type, optional weight, source_path)\n\n\n\nSidecar graph artifacts\n\n&lt;backlog-root&gt;/_index/graph_nodes.jsonl\n&lt;backlog-root&gt;/_index/graph_edges.jsonl\n\n\n\nBoth must be safe to delete and rebuild.\nRetrieval strategy (Graph-assisted RAG)\n\nSeed retrieval\n\nFTS and/or embeddings return top-N seed nodes/chunks\n\n\nExpand\n\ntraverse k-hop (default k=1)\nedge allowlist and fanout caps\n\n\nRe-rank\n\nweights by doctype and section (ADR decision &gt; item title/acceptance &gt; worklog)\noptionally prioritize Ready/InProgress items\n\n\nContext packing\n\nemit a context pack describing:\n\nseeds (why selected)\nneighbors (which edge pulled them in)\nminimal excerpts/anchors (title/ids/links)\n\n\n\n\n\nConfig surface (indicative)\n\nretrieval.graph.enabled\nretrieval.graph.k_hop\nretrieval.graph.edge_allowlist\nretrieval.graph.max_neighbors_per_seed\nretrieval.weights.* (doctype/section/state weights)\n\nConsequences\n\nGraph-assisted retrieval becomes the preferred way to preserve traceability (seed + neighbors).\nTooling must keep expansion bounded to avoid context explosions.\nThe design stays compatible with file-scan fallback and optional SQLite/embedding acceleration.\n\nNon-goals\n\nLLM/NLP entity extraction and automatic relation mining\nserver/MCP mode or cross-repo graphs\n\nReferences\n\nADR-0004 File-first + SQLite index\nADR-0009 Local-first embedding search\nRAG pipeline\nKABSD-FTR-0023 Graph-assisted RAG planning\n"},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0031_per-product-isolated-index-architecture":{"title":"Per-Product Isolated Index Architecture","links":["items/feature/0000/KABSD-FTR-0010_monorepo-platform-migration","items/task/0000/KABSD-TSK-0084_update-indexer-and-resolver-for-product-isolation"],"tags":[],"content":"Context\nIn a multi-product monorepo environment, we needed to decide between two architectural approaches for the SQLite index:\n\nPlatform-level shared index: All products write to a single _kano/backlog/_index/backlog.sqlite3\nPer-product isolated index: Each product owns its own index at products/&lt;name&gt;/_index/backlog.sqlite3\n\nDecision Criteria\nWe evaluated 7 dimensions:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDimensionPer-ProductPlatformWinnerIsolationComplete separationShared namespacePer-ProductConcurrencyNo locking contentionPotential locksPer-ProductPerformancePredictable, isolatedCan degrade with loadPer-ProductScalabilityLinear per productShared bottleneckPer-ProductComplexitySimple, independentComplex coordinationPer-ProductMaintenanceEasy (per-product)Harder (shared state)Per-ProductFuture ExtensibilitySupports embedding DBDifficult to addPer-Product\nOverall Score: Per-Product = 91%, Platform = 43%\nDecision\nImplement per-product isolated SQLite indexes.\nEach product will:\n\nMaintain its own SQLite database at products/&lt;product-name&gt;/_index/backlog.sqlite3\nHave independent schema with product column for self-documentation\nSupport autonomous indexing/rebuilding without affecting other products\nEnable future cross-product aggregation via product-aware queries\n\nImplementation Details\nSchema Design\nAll tables include product column:\nCREATE TABLE items (\n  product TEXT NOT NULL,\n  id TEXT NOT NULL,\n  PRIMARY KEY (product, id),\n  ...\n);\n \nCREATE TABLE item_tags (\n  product TEXT NOT NULL,\n  item_id TEXT NOT NULL,\n  tag TEXT NOT NULL,\n  PRIMARY KEY (product, item_id, tag),\n  FOREIGN KEY (product, item_id) REFERENCES items(product, id)\n);\nIndex Rebuild Process\n\nscripts/indexing/build_sqlite_index.py --product &lt;name&gt;\nScans products/&lt;name&gt;/items/ directory\nExtracts product from file paths\nUpserts into product-specific SQLite database\nMaintains composite key integrity\n\nResolver Behavior\n\nresolve_ref(ref, index, product=None) accepts optional product filter\nIf product not specified, searches across all products\nProduct column enables cross-product queries when needed\n\nRationale\n\nIsolation ensures safety: No possibility of data leakage between products\nPredictable performance: Each product‚Äôs index grows independently\nSupports autonomy: Teams can rebuild their product‚Äôs index without coordination\nFuture-proof: Enables embedding databases per product or shared aggregation\nSimpler mental model: Products are truly independent\nBackward compatible: Product column facilitates future migrations\n\nAlternatives Considered\nPlatform-level shared index\n\nPros: Unified search across all products\nCons: Complex coordination, shared bottleneck, potential data leakage\nRejected: Architecture does not support team autonomy\n\nHybrid approach (shared metadata + per-product data)\n\nPros: Balanced complexity\nCons: Still requires coordination layer, adds complexity\nRejected: Per-product isolation is simpler and cleaner\n\nFuture Work\nWhen cross-product features are needed:\n\nCreate aggregation layer on top of per-product indexes\nOr implement global embedding database that reads from per-product sources\nProduct column in schema already prepared for this extensibility\n\nReferences\n\nKABSD-FTR-0010_monorepo-platform-migration.md: Monorepo Platform Migration feature\nKABSD-TSK-0084_update-indexer-and-resolver-for-product-isolation.md: Indexer and resolver product isolation\n"},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0032_per-product-isolated-index-architecture":{"title":"Per-Product Isolated Index Architecture","links":["items/feature/0000/KABSD-FTR-0010_monorepo-platform-migration","items/task/0000/KABSD-TSK-0084_update-indexer-and-resolver-for-product-isolation"],"tags":[],"content":"Context\nIn a multi-product monorepo environment, we needed to decide between two architectural approaches for the SQLite index:\n\nPlatform-level shared index: All products write to a single _kano/backlog/_index/backlog.sqlite3\nPer-product isolated index: Each product owns its own index at products/&lt;name&gt;/_index/backlog.sqlite3\n\nDecision Criteria\nWe evaluated 7 dimensions:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDimensionPer-ProductPlatformWinnerIsolationComplete separationShared namespacePer-ProductConcurrencyNo locking contentionPotential locksPer-ProductPerformancePredictable, isolatedCan degrade with loadPer-ProductScalabilityLinear per productShared bottleneckPer-ProductComplexitySimple, independentComplex coordinationPer-ProductMaintenanceEasy (per-product)Harder (shared state)Per-ProductFuture ExtensibilitySupports embedding DBDifficult to addPer-Product\nOverall Score: Per-Product = 91%, Platform = 43%\nDecision\nImplement per-product isolated SQLite indexes.\nEach product will:\n\nMaintain its own SQLite database at products/&lt;product-name&gt;/_index/backlog.sqlite3\nHave independent schema with product column for self-documentation\nSupport autonomous indexing/rebuilding without affecting other products\nEnable future cross-product aggregation via product-aware queries\n\nImplementation Details\nSchema Design\nAll tables include product column:\nCREATE TABLE items (\n  product TEXT NOT NULL,\n  id TEXT NOT NULL,\n  PRIMARY KEY (product, id),\n  ...\n);\n \nCREATE TABLE item_tags (\n  product TEXT NOT NULL,\n  item_id TEXT NOT NULL,\n  tag TEXT NOT NULL,\n  PRIMARY KEY (product, item_id, tag),\n  FOREIGN KEY (product, item_id) REFERENCES items(product, id)\n);\nIndex Rebuild Process\n\nscripts/indexing/build_sqlite_index.py --product &lt;name&gt;\nScans products/&lt;name&gt;/items/ directory\nExtracts product from file paths\nUpserts into product-specific SQLite database\nMaintains composite key integrity\n\nResolver Behavior\n\nresolve_ref(ref, index, product=None) accepts optional product filter\nIf product not specified, searches across all products\nProduct column enables cross-product queries when needed\n\nRationale\n\nIsolation ensures safety: No possibility of data leakage between products\nPredictable performance: Each product‚Äôs index grows independently\nSupports autonomy: Teams can rebuild their product‚Äôs index without coordination\nFuture-proof: Enables embedding databases per product or shared aggregation\nSimpler mental model: Products are truly independent\nBackward compatible: Product column facilitates future migrations\n\nAlternatives Considered\nPlatform-level shared index\n\nPros: Unified search across all products\nCons: Complex coordination, shared bottleneck, potential data leakage\nRejected: Architecture does not support team autonomy\n\nHybrid approach (shared metadata + per-product data)\n\nPros: Balanced complexity\nCons: Still requires coordination layer, adds complexity\nRejected: Per-product isolation is simpler and cleaner\n\nFuture Work\nWhen cross-product features are needed:\n\nCreate aggregation layer on top of per-product indexes\nOr implement global embedding database that reads from per-product sources\nProduct column in schema already prepared for this extensibility\n\nReferences\n\nKABSD-FTR-0010_monorepo-platform-migration.md: Monorepo Platform Migration feature\nKABSD-TSK-0084_update-indexer-and-resolver-for-product-isolation.md: Indexer and resolver product isolation\n"},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0033_decision-for-kabsd-tsk-0056":{"title":"Decision for KABSD-TSK-0056","links":[],"tags":[],"content":"Status\nProposed\nContext\nThis decision was extracted from workset notes for task/feature KABSD-TSK-0056.\nDecision\nUse fine-grained chunking with semantic section boundaries instead of fixed-size windows. This preserves logical document structure for better retrieval accuracy.\nRationale\nSee workset notes in _kano\\backlog\\sandboxes.cache\\019b8f52-9fc8-7c94-aa2d-806cacdd9086 for full context.\nConsequences\n\nPositive:\n\n‚Ä¶\n\n\nNegative:\n\n‚Ä¶\n\n\nNeutral:\n\n‚Ä¶\n\n\n\nAlternatives Considered\n\nAlternative A: ‚Ä¶\nAlternative B: ‚Ä¶\n\nReferences\n\nExtracted from workset for KABSD-TSK-0056 by copilot\n"},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0034_conflict-handling-policy-for-duplicate-ids-and-uids":{"title":"Conflict handling policy for duplicate IDs and UIDs","links":[],"tags":[],"content":"Decision\nAdopt a configurable conflict policy for duplicate IDs and UIDs, with defaults:\n\nconflict_policy.id_conflict = &quot;rename&quot;: when duplicate display IDs are detected, rename duplicates to the next available ID.\nconflict_policy.uid_conflict = &quot;trash_shorter&quot;: when the same UID appears with differing content, keep the longer file and move the shorter file to _trash/&lt;YYYYMMDD&gt;/....\n\nTie-breaker for equal length: keep the lexicographically earliest path and trash the other(s).\nContext\nWe need deterministic, low-friction behavior for duplicate IDs/UIDs across agents. Recent link integrity repairs showed how ambiguity slows down remediation and risks cross-agent divergence.\nOptions Considered\n\nAlways report and require human intervention.\nAuto-rename duplicate IDs and auto-trash UID conflicts.\nAuto-rename for both ID and UID conflicts.\n\nPros / Cons\n\nAuto-rename reduces manual cleanup for benign ID collisions.\nTrashing UID conflicts is safer than deletion and preserves a recovery path.\nAuto-remediation can hide mistakes if applied without review; defaults should be documented and configurable.\n\nConsequences\n\nadmin links normalize-ids will apply these defaults unless overridden in config.\nUID conflict resolution will create _trash/ entries; audits should treat them as recoverable artifacts.\n\nFollow-ups\n\nDocument the policy in the skill and config defaults.\n"},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0035_cross-lingual-retrieval-requirement-and-default-embedding-policy":{"title":"Cross-lingual retrieval requirement and default embedding policy","links":[],"tags":[],"content":"Decision\nCross-lingual retrieval is a requirement.\nDefault embedding policy must be multilingual-capable. Provider/model choices must be evaluated against a small multilingual benchmark corpus that includes CJK and mixed-language queries.\nContext\nThis repo‚Äôs backlog contains mixed-language content (English + CJK). The semantic retrieval system is local-first and derived from canonical Markdown (see ADR-0009).\nIf we choose an embedder that is not multilingual-capable, cross-lingual queries will fail or regress unpredictably, and benchmark results will not reflect real usage.\nOptions Considered\nOption A: English-only retrieval\nTreat cross-lingual retrieval as out-of-scope. Only optimize for English content.\nOption B: Cross-lingual retrieval required (multilingual embedder policy) [chosen]\nRequire cross-lingual retrieval and evaluate embedders using multilingual cases.\nPros / Cons\nOption A: English-only retrieval\nPros:\n\nPotentially smaller/faster embedding models.\n\nCons:\n\nFails for mixed-language backlog content.\nForces users to translate queries manually.\nMakes retrieval quality brittle as content language mix evolves.\n\nOption B: Cross-lingual retrieval required\nPros:\n\nMatches repository reality (mixed-language artifacts).\nMakes evaluation criteria explicit and repeatable.\n\nCons:\n\nMay increase model footprint/cost.\nRequires benchmark coverage for multilingual/cross-lingual cases.\n\nConsequences\n\nBenchmarks MUST include cross-lingual cases.\n\n\nThe benchmark harness (USR-0034) must include multilingual docs and cross-lingual queries.\n\n\nTelemetry MUST capture tokenizer behavior and truncation.\n\n\nToken inflation for CJK can reduce effective context windows.\nTelemetry must distinguish exact vs heuristic token counts (see tokenizer TokenCount).\n\n\nDefault embedder configuration is allowed to be ‚Äúnoop‚Äù for local tests.\n\n\nReal embedders remain optional dependencies.\nDecisions about a real default model should be benchmark-driven.\n\nFollow-ups\n\nKeep ADR-0036 aligned: per-model indexes are required for safe experimentation and rollback.\nUse kano-backlog benchmark run outputs under _kano/backlog/products/&lt;product&gt;/artifacts/KABSD-TSK-0261/runs/ as the evidence trail for model selection.\n"},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0036_index-strategy-shared-index-now-per-model-indexes-later-via-config":{"title":"Index strategy: shared index now, per-model indexes later via config","links":[],"tags":[],"content":"Decision\nWe will use per-model indexes (per embedding space) as the default.\n\nA single ‚Äúshared index‚Äù across different embedding models is not supported because vectors\nfrom different models are not generally comparable (dimension and semantic space differ).\n\nWe will keep an explicit configuration mechanism to switch index selection and to allow\ncontrolled ‚Äúaliasing‚Äù (sharing) only when two model identifiers are proven to be the same\nembedding space (strict allowlist; no automatic inference).\nContext\nWe need a local-first semantic retrieval capability for backlog artifacts (items, ADRs, etc.).\r\nCross-lingual retrieval is required (see ADR-0035), which increases the likelihood that we\r\nwill evaluate and potentially switch between embedding models over time.\nThe index must remain:\n\nlocal-first (no server runtime required)\nrebuildable (derived from canonical Markdown)\ndeterministic enough for incremental rebuilds (chunk IDs + content hashes)\nconfigurable (so we can swap providers/models/backends without rewriting code)\n\nOptions Considered\nOption A: Single shared index across multiple embedding models\nStore all vectors in one ANN index, regardless of which model produced them.\nOption B: Per-model indexes (per embedding space) [chosen]\nMaintain separate ANN indexes keyed by an explicit ‚Äúembedding space‚Äù identity.\nOption C: Shared index with automatic compatibility detection (future research)\nAttempt to infer which models are ‚Äúcompatible enough‚Äù to share one index by running\r\nstatistical tests (e.g., neighborhood agreement on a validation corpus).\nPros / Cons\nOption A: Single shared index across multiple embedding models\nPros:\n\nMinimal operational complexity (one index file)\n\nCons:\n\nGenerally invalid: vectors from different embedding models are not comparable\r\n(dimensions can differ; even with same dimension the spaces are different).\nProduces misleading similarity scores and unstable retrieval quality.\nMakes troubleshooting and benchmarking ambiguous.\n\nOption B: Per-model indexes (per embedding space)\nPros:\n\nCorrectness: avoids mixing incompatible vector spaces.\nOperational clarity: retrieval quality changes map to a single model+version.\nEnables safe experimentation: add a new index for a new model without corrupting the old one.\nSupports gradual rollout: switch default index via config, keep rollback path.\n\nCons:\n\nMore disk usage and rebuild time when multiple models are evaluated.\nRequires a routing key (index selection) to be part of pipeline config and metadata.\n\nOption C: Shared index with automatic compatibility detection\nPros:\n\nCould reduce the number of indexes in some cases.\n\nCons:\n\nHigh risk of false positives (appears compatible on a small corpus but fails in practice).\nAdds complexity and ‚Äúmagic‚Äù behavior that is hard to reason about.\nNeeds an evaluation harness and careful governance anyway.\n\nConsequences\n1) Define a stable embedding space identity\nIntroduce an embedding_space_id used for:\n\nselecting the vector index (routing)\nstoring metadata alongside chunks/vectors\npreventing accidental mixing\n\nDefinition (conceptual):\n\nembedding_space_id = sha256(provider_id + model_name + model_revision + dims + preprocessing_version + vector_norm + prompt_style_id)\n\nNotes:\n\n‚Äúmodel_revision‚Äù should include an immutable identifier when possible (HF revision hash, provider version).\npreprocessing_version covers normalization/prefixing decisions that affect embeddings.\nprompt_style_id matters for instruct-style embedders (query vs document templates).\n\n2) Index storage layout\nPersist one index per embedding_space_id (or per ‚Äúmodel key‚Äù that maps to it), for example:\n\n.../vector_indexes/&lt;embedding_space_id&gt;/index.bin\n.../vector_indexes/&lt;embedding_space_id&gt;/mapping.sqlite (if needed)\n\n3) Config-driven routing and future ‚Äúsharing‚Äù via allowlist aliasing\nConfiguration must allow:\n\nselecting the default embedder (and therefore default index)\nselecting a specific index key explicitly (for debugging/rollback)\ndefining alias mappings only when two identifiers are truly the same embedding space\n\nPolicy:\n\nDo not auto-merge indexes.\nIf aliasing is used, it must be explicit and reviewed (allowlist).\n\n4) Benchmark implications\nBenchmarks must report results per embedding_space_id so comparisons are reproducible.\nFollow-ups\n\nUpdate KABSD-USR-0035 to reference ADR-0036 as the index strategy baseline.\nEnsure KABSD-USR-0031 (telemetry) includes embedding_space_id and model revision in results.\nEnsure KABSD-USR-0034 (benchmark harness) compares multiple embedding_space_id runs.\nOptional (future): research Option C as a non-default experiment, producing an ADR addendum\r\nif the evidence supports safe aliasing beyond strict equality.\n\nOptions Considered\nPros / Cons\nConsequences\nFollow-ups"},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0037_inspector-pattern-and-query-surface-architecture":{"title":"Inspector Pattern and Query Surface Architecture","links":["KABSD-EPIC-0011","KABSD-FTR-0055","KABSD-FTR-0056","ADR-0013","ADR-0004"],"tags":[],"content":"Decision\nThe kano-agent-backlog-skill will adopt an ‚ÄúInspector Pattern‚Äù architecture where:\n\nSkill Core = Query Surface: Provides deterministic, evidence-based data extraction APIs\nExternal Agents = Inspectors: Consume query surface, produce conclusions with evidence trails\nEvidence-First: Every conclusion must cite traceable sources (file paths, line ranges, IDs)\nRead-Only Contract: Inspectors query canonical SoT, never write to it directly\n\nThis means:\n\nAll ‚Äúexpert judgment‚Äù (health assessment, review, refactor suggestions) lives in external agents\nCore skill provides only deterministic data + derived artifacts (audit, snapshot, constellation, indexes)\nInspector agents are replaceable (any agent can implement the contract)\nAll inspector outputs must include evidence attachments (no unsourced claims)\n\nContext\nProblem Statement\nOrigin: GPT-5.2 feedback on backlog discipline and architecture (2026-01-22)\nCurrent risk: encoding ‚Äújudgment‚Äù (health assessment, review suggestions, refactor recommendations) into core skill logic creates:\n\nGoodhart‚Äôs Law risk: Metrics become targets, lose meaning when hardcoded\nTight coupling: Expert logic hardcoded into skill core, hard to extend/replace\nLimited extensibility: Can‚Äôt swap assessment strategies without modifying core\nInconsistent evidence: Conclusions without traceable sources\n\nKey Insight from GPT-5.2\n\n‚ÄúBacklog skill‚Äôs responsibility: provide all tools for agents to reliably acquire information, not hardcode judgment into core.‚Äù\n\nAnalogy:\n\nSkill = Database with query API\nInspector Agents = Analytics tools that consume the database\nEvidence = Structured citations (table/row/column refs)\n\nCurrent State\nWhat exists:\n\nAudit primitives (rule checking, gap detection)\nSnapshot generation (state summaries)\nConstellation (relationship graphs)\nSQLite index for fast queries\n\nWhat‚Äôs missing:\n\nFormal Inspector Pattern contract\nUnified query surface with JSON output\nEvidence attachment standards\nReference inspector implementation\n\nRelated Work\n\n3+3 questions (health/ideas assessment): Proposed feature, now reframed as inspector agent\nDecision audit/write-back: Implemented ad-hoc, needs formalization\nExternal agent integration: No contract defined\n\nArchitecture\nPrinciple A: Core = Data + Derived Artifacts (Deterministic)\nCore Responsibilities (all deterministic, repeatable):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComponentPurposeOutputaudit.run()Rules enforcement (Ready gate, schema validation)Findings with IDs, categories, severitysnapshot.build()Current state extraction (item counts, distributions)Timestamped state summaryconstellation.build()Relationship graph (parent/child, blocks, relates)Graph with nodes, edges, metadataindexSearch acceleration (FTS, embedding, graph)Query results with relevance scores\nKey: No ‚Äúexpert opinion‚Äù - only facts derived from canonical files.\nPrinciple B: External Agents = Inspectors (Replaceable)\nInspector Types (all external to core):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInspectorPurposeExample OutputHealth/Ideas3+3 questions, gap analysis, anti-patterns‚Äù5 items missing Context field (AF-001, AF-002‚Ä¶)‚ÄùReviewerCode review suggestions, best practices‚ÄùConsider extracting common logic (evidence: L45-67)‚ÄúArchitectRefactoring recommendations, design improvements‚ÄùDetected circular dependency (items: X, Y, Z)‚ÄúSecurityThreat model, vulnerability assessment‚ÄùExposed secrets in item TASK-042 (file: ‚Ä¶, L25)‚Äù\nKey: These are separate processes/agents, not core modules.\nPrinciple C: Evidence = First-Class Citizen\nCore Principle: Every inspector finding must cite traceable evidence. Without evidence, conclusions are rejected or downgraded.\nEvidence Quality: Five Axes (Critical Thinking Foundation)\nBased on critical thinking principles, evidence quality must be evaluated across five axes:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAxisDefinitionSystem ApplicationRelevanceEvidence must directly support the claim, not just appear relatedclaim_id links evidence to specific findingReliabilitySource is real, traceable, verifiable (not ‚Äújargon credentialism‚Äù)provenance field: hash, commit, URL, timestamp, authorSufficiencyOne example ‚â† universal rule (avoid survivor bias, no control group)coverage field: sample range, counter-examples, failure statsVerifiabilityOthers can check, repeat, measure the claimverification field: reproducible command, test, or check stepsIndependenceEvidence doesn‚Äôt collude with conclusion (avoid self-citation, astroturfing)independence field: source type, conflict-of-interest flags\nWhy This Matters:\n\nBayes‚Äô prior problem: Math looks objective, but where priors come from is the landmine; priors can be manipulated ‚Üí conclusions can be steered\nPeer review ideal vs reality: The idea is to shift credibility from individuals to community validation, but institutions develop interest chains, conservatism within paradigms\nTrust should be in continuous questioning + verifiable judgment, not fixed standards\n\nEvidence Schema (Extended with 5-Axes)\n@dataclass\nclass EvidenceRecord:\n    &quot;&quot;&quot;Full evidence record with quality metadata.&quot;&quot;&quot;\n    \n    # Core identity (existing)\n    type: str                 # &quot;item&quot;, &quot;adr&quot;, &quot;file&quot;, &quot;audit_finding&quot;, &quot;commit&quot;, &quot;log&quot;\n    id: str                   # Item/ADR ID or finding ID\n    file_path: str            # Relative path from backlog root\n    line_range: Optional[Tuple[int, int]] = None\n    excerpt: Optional[str] = None\n    timestamp: Optional[str] = None\n    \n    # Relevance (5-axis 1)\n    claim_id: Optional[str] = None       # Which claim/decision this supports\n    support_type: Optional[str] = None   # &quot;direct&quot;, &quot;indirect&quot;, &quot;counterpoint&quot;\n    \n    # Reliability (5-axis 2)\n    source_type: str = &quot;unknown&quot;         # &quot;repo_path&quot;, &quot;issue&quot;, &quot;pr&quot;, &quot;chat&quot;, &quot;doc&quot;, &quot;web&quot;, &quot;experiment&quot;\n    provenance: Optional[Dict] = None    # {&quot;hash&quot;: &quot;...&quot;, &quot;commit&quot;: &quot;...&quot;, &quot;url&quot;: &quot;...&quot;, &quot;author&quot;: &quot;...&quot;}\n    \n    # Sufficiency (5-axis 3)\n    coverage: Optional[Dict] = None      # {&quot;sample_size&quot;: N, &quot;has_counterexamples&quot;: bool, &quot;has_failure_stats&quot;: bool}\n    \n    # Verifiability (5-axis 4)\n    verification: Optional[Dict] = None  # {&quot;method&quot;: &quot;command|test|manual&quot;, &quot;steps&quot;: [...], &quot;reproducible&quot;: bool}\n    \n    # Independence (5-axis 5)\n    independence: Optional[Dict] = None  # {&quot;self_cited&quot;: bool, &quot;conflict_of_interest&quot;: bool, &quot;same_source_chain&quot;: bool}\n    \n    # Meta\n    confidence: Optional[float] = None   # 0.0-1.0, with explicit reasoning\n    confidence_reason: Optional[str] = None\nMinimum Evidence Requirements by Context\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContextRequired FieldsRecommended FieldsWorkset materialtype, id, file_path, source_typeprovenance, verificationInspector findingtype, id, file_path, line_range, claim_idcoverage, independenceADR decision supporttype, id, file_path, claim_id, verificationcoverage, independence, confidenceHealth reviewtype, id, source_type, independencecoverage, sufficiency analysis\nAnti-Patterns (Evidence Red Flags)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRed FlagDetectionRiskSingle source dependencyAll evidence from one file/authorLow reliability, no cross-validationSame-source masqueradeMultiple ‚Äúevidence‚Äù items from same originFake sufficiencyMissing counterexamplesNo failure cases, only success storiesSurvivor biasNon-reproducible‚ÄùTrust me‚Äù without verification stepsUnverifiable claimsSelf-citation loopAuthor cites own previous work exclusivelyIndependence violationJargon credentialismClaims backed by terminology, not dataReliability theater\nEvery inspector output MUST include evidence records:\n{\n  &quot;finding_id&quot;: &quot;F-001&quot;,\n  &quot;category&quot;: &quot;health&quot;,\n  &quot;assessment&quot;: &quot;Item missing required field&quot;,\n  &quot;evidence&quot;: [\n    {\n      &quot;type&quot;: &quot;item&quot;,\n      &quot;item_id&quot;: &quot;KABSD-TSK-0042&quot;,\n      &quot;file&quot;: &quot;_kano/backlog/items/task/0000/KABSD-TSK-0042.md&quot;,\n      &quot;line_range&quot;: [25, 30],\n      &quot;field&quot;: &quot;Context&quot;,\n      &quot;issue&quot;: &quot;Empty or missing&quot;\n    }\n  ],\n  &quot;timestamp&quot;: &quot;2026-01-22T08:51:00Z&quot;,\n  &quot;agent&quot;: &quot;health-inspector-v1&quot;\n}\nNo evidence = rejected or downgraded.\nInspector Agent Contract\nQuery Surface API (Existing + Planned)\nExisting APIs (already implemented):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOps FunctionCLI CommandJSON SupportNotessnapshot.generate_pack()kano-backlog snapshot‚úÖ pack.to_json()Returns EvidencePack with stubs, capabilities, healthworkitem.list_items()kano-backlog item list‚úÖ --format jsonFilters: type, state, parent, tagsworkitem.get_item()kano-backlog workitem read‚úÖ --format jsonSingle item with full metadataworkitem.validate_ready()kano-backlog workitem validate‚úÖ --format jsonReady gate validationtopic.decision_audit()kano-backlog topic decision-audit‚úÖ --format jsonDecision write-back audittopic.export_context()kano-backlog topic export-context‚úÖ --format jsonTopic context bundle\nPlanned APIs (to implement in KABSD-FTR-0055):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOps FunctionCLI CommandStatusNotesrelease_check.run_phase1/2()kano-backlog release check‚ö†Ô∏è Markdown onlyNeed --format jsonVarious health checkskano-backlog doctor‚ö†Ô∏è Plain textNeed --format jsonconstellation.build()kano-backlog constellation build‚ùå MissingRelationship graphdoc_resolve.resolve()kano-backlog workitem resolve‚ùå MissingStructured excerpts\nEvidence Schema (to standardize across all APIs):\nSee ‚ÄúPrinciple C: Evidence Quality Five Axes‚Äù above for the extended EvidenceRecord schema with 5-axis quality metadata.\nMinimal Evidence schema (for backward compatibility):\n@dataclass\nclass Evidence:\n    &quot;&quot;&quot;Minimal traceable source for inspector findings.&quot;&quot;&quot;\n    type: str           # &quot;item&quot;, &quot;adr&quot;, &quot;file&quot;, &quot;audit_finding&quot;\n    id: str             # Item/ADR ID or finding ID\n    file_path: str      # Relative path from backlog root\n    line_range: Optional[Tuple[int, int]] = None  # Start, end lines\n    excerpt: Optional[str] = None  # Text snippet\n    timestamp: Optional[str] = None\nAudit vs Health: Distinction\nBoth use the same query surface and evidence format, but differ in what they check:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAspectAuditHealth ReviewFocusConsistency / ConformanceCredibility / Risk / GapsQuestions‚ÄùDoes X conform to rule Y?&quot;&quot;Can we trust X? What are the risks?‚ÄùExamplesNaming conventions, directory structure, schema drift, deterministic pipeline completenessSingle-source dependency, survivor bias, unverifiable claims, conflicted evidenceOutputViolation list (pass/fail per rule)Risk and trust gap report (with severity)TriggerCI gate, pre-release check, schema migrationManual request, agent stuck, decision review\nAudit checks for ‚Äúrule violations‚Äù:\n\nMissing required fields\nWrong directory structure\nSchema mismatch\nDeterministic pipeline gaps\n\nHealth checks for ‚Äútrust gaps‚Äù:\n\nEvidence quality degradation (5-axis failures)\nSingle-narrative dependency\nMissing counterexamples\nUnstated assumptions (priors)\n\nInspector Output Contract\nStandard output schema:\n{\n  &quot;inspector&quot;: &quot;health-ideas-v1&quot;,\n  &quot;agent&quot;: &quot;antigravity&quot;,\n  &quot;timestamp&quot;: &quot;2026-01-22T08:51:00Z&quot;,\n  &quot;query_params&quot;: {\n    &quot;window&quot;: &quot;7d&quot;,\n    &quot;filters&quot;: {}\n  },\n  &quot;findings&quot;: [\n    {\n      &quot;finding_id&quot;: &quot;F-001&quot;,\n      &quot;category&quot;: &quot;health&quot;,\n      &quot;question&quot;: &quot;Are items well-formed?&quot;,\n      &quot;assessment&quot;: &quot;5 tasks missing Context field&quot;,\n      &quot;severity&quot;: &quot;warning&quot;,\n      &quot;evidence&quot;: [\n        {\n          &quot;type&quot;: &quot;audit_finding&quot;,\n          &quot;audit_finding_id&quot;: &quot;AF-001&quot;,\n          &quot;item_id&quot;: &quot;KABSD-TSK-0042&quot;,\n          &quot;file&quot;: &quot;_kano/backlog/items/task/0000/KABSD-TSK-0042.md&quot;,\n          &quot;line_range&quot;: [25, 30],\n          &quot;field&quot;: &quot;Context&quot;,\n          &quot;issue&quot;: &quot;Empty&quot;\n        }\n      ]\n    }\n  ],\n  &quot;summary&quot;: {\n    &quot;total_findings&quot;: 12,\n    &quot;by_severity&quot;: {&quot;error&quot;: 0, &quot;warning&quot;: 5, &quot;info&quot;: 7}\n  }\n}\nRequired fields:\n\ninspector: Inspector identity (name + version)\nagent: Which agent ran the inspector\ntimestamp: When inspection occurred\nfindings: Array of findings, each with evidence\nevidence: Array of traceable sources with file paths + line ranges\n\nIntegration Patterns\nPattern 1: Manual Invocation\n# Human asks agent to run inspector\nUser: &quot;Check backlog health and show me the report&quot;\n \n# Agent executes\n$ kano-backlog query snapshot --format json &gt; snapshot.json\n$ health-inspector --input snapshot.json --output report.json\n$ cat report.json\nPattern 2: CI Integration\n# .github/workflows/backlog-health.yml\n- name: Check backlog health\n  run: |\n    kano-backlog query snapshot --format json &gt; snapshot.json\n    health-inspector --input snapshot.json --output report.json\n    if grep -q &#039;&quot;severity&quot;: &quot;error&quot;&#039; report.json; then exit 1; fi\nPattern 3: Agent Self-Assessment\nAgent: &quot;I&#039;m stuck. Let me consult the inspector...&quot;\r\nAgent: &lt;runs inspector, gets findings&gt;\r\nAgent: &quot;Inspector found 3 items blocking my work (evidence: ...)&quot;\n\nWhere Inspectors Live\nOptions:\n\n\nSeparate skill: kano-inspector-health-ideas-skill/ (recommended)\n\nPro: Replaceable, versionable, independent lifecycle\nCon: Extra installation step\n\n\n\nReference implementation in core skill: examples/inspectors/health.py\n\nPro: Batteries-included for demos\nCon: Risk of coupling if not disciplined\n\n\n\nExternal repository: Community-maintained inspectors\n\nPro: Maximum flexibility\nCon: Discovery problem\n\n\n\nRecommendation: Start with (2) for reference, encourage (1) for production use.\nConsequences\nFor Skill Developers\n\n\nMust separate data from judgment:\n\n‚úÖ Good: audit.run() returns list of ‚Äúmissing Context field‚Äù findings\n‚ùå Bad: audit.run() returns ‚Äúbacklog health is poor‚Äù conclusion\n\n\n\nMust provide evidence attachments:\n\nAll query APIs return structured data with file paths, line ranges, IDs\nNo APIs that return ‚Äúsummary strings‚Äù without evidence\n\n\n\nMust document query surface:\n\nInspector contract is a public API, needs versioning and docs\n\n\n\nFor Inspector Agents\n\n\nMust consume query surface, not parse files directly:\n\n‚úÖ Good: Call workitem.query(filters={...}) API\n‚ùå Bad: Parse _kano/backlog/items/**/*.md directly\n\n\n\nMust attach evidence to all findings:\n\nEvery conclusion cites file path + line range (or stable anchor)\nNo ‚ÄúI think X‚Äù without ‚Äúbecause I saw Y at Z‚Äù\n\n\n\nAre replaceable:\n\nAny agent can implement inspector contract\nMultiple inspectors can coexist (health, review, security, etc)\n\n\n\nFor End Users (Humans)\n\n\nInspector frequency is use-case specific:\n\nNOT ‚Äúrun daily‚Äù (avoid Goodhart‚Äôs Law)\nRun when: manual request, CI gate, agent stuck, pre-release audit\n\n\n\nInspector outputs are recommendations, not commands:\n\nHuman decides which findings to act on\nAgent may propose fixes but doesn‚Äôt auto-apply\n\n\n\nEvidence trail enables trust:\n\nEvery finding cites sources\nHuman can verify inspector claims\n\n\n\nMigration Strategy\nPhase 1: Define Contract (ADR + Documentation)\n\n Create this ADR\n Document query surface API spec\n Document inspector output schema\n Update ADR-0013 (Module Boundaries) with inspector pattern section\n\nPhase 2: Implement Query Surface (KABSD-FTR-0055)\n\n Add JSON output to existing audit/snapshot/constellation\n Implement workitem.query API\n Implement doc.resolve API\n Implement export.bundle API\n Add CLI commands under kano-backlog query\n\nPhase 3: Reference Inspector (KABSD-FTR-0056)\n\n Build health/ideas inspector as reference implementation\n Validate inspector contract through real usage\n Document integration patterns\n\nPhase 4: Documentation &amp; Tooling\n\n Update AGENTS.md with inspector pattern guidance\n Add inspector examples to SKILL.md\n Create inspector scaffolding tool (optional)\n\nAlternatives Considered\nAlternative A: Keep Assessment Logic in Core\nApproach: Encode health checks, review logic, etc directly in skill core.\nRejected because:\n\nCreates tight coupling (hard to extend/replace)\nGoodhart‚Äôs Law risk (metrics become targets when hardcoded)\nNo separation of concerns (data vs judgment)\n\nAlternative B: Build Generic ‚ÄúLLM Judge‚Äù Framework\nApproach: Create abstract framework for LLM-based assessment.\nDeferred because:\n\nToo abstract for initial implementation\nStart concrete (inspector contract), generalize later if needed\nRisk of over-engineering\n\nAlternative C: Use External Tool (Jira, Linear, etc)\nApproach: Export to external PM tool, use their analytics/dashboards.\nRejected because:\n\nViolates local-first principle\nExternal tools can consume inspector outputs via adapters (future work)\nWe control the query surface, not the external tool\n\nRelated\n\nKABSD-EPIC-0011: Inspector Pattern: External Agent Query Surface (parent epic)\nKABSD-FTR-0055: Query Surface API Implementation\nKABSD-FTR-0056: Inspector Agent Reference Implementation\nADR-0013: Codebase Architecture and Module Boundaries (will add inspector pattern section)\nADR-0004: File-First Architecture with SQLite Index (complements this ADR)\n\nStatus\nAccepted (2026-01-22)\nThis ADR establishes the architectural direction. Implementation will occur in phases via linked feature work items."},"demo/_kano/backlog/products/kano-agent-backlog-skill/decisions/README":{"title":"README","links":[],"tags":[],"content":"_kano/backlog/decisions\nStore ADRs here. Link ADR IDs in work items via the decisions field and\r\nappend a Worklog entry when a decision affects scope or approach."},"demo/docs/index":{"title":"index","links":[],"tags":[],"content":"Welcome to Kano Agent Backlog Skill Demo\nThis repository demonstrates the capabilities of the Kano Agent Backlog Skill - a local-first, file-based backlog management system for AI agent collaboration.\nReleases\n\nLatest: docs/releases/0.0.2.md\nPrevious: docs/releases/0.0.1.md\n\nQuick Start\n1. Clone the repository\ngit clone github.com/dorgonman/kano-agent-backlog-skill-demo.git\ncd kano-agent-backlog-skill-demo\n2. Initialize submodules\ngit submodule update --init --recursive\n3. Install the skill\npython -m pip install -e skills/kano-agent-backlog-skill\n4. Verify installation\nkano --help\n5. Explore the demo backlog\n# List work items\nkano item list --product kano-agent-backlog-skill\nOfficial Documentation\nFor comprehensive documentation, including detailed guides and API references, please visit our Official Documentation Website."},"demo/docs/releases/0.0.1":{"title":"0.0.1","links":[],"tags":[],"content":"0.0.1 ‚Äî Initial local-first backlog workflow (MVP)\nThis is the first public cut of the Kano Agent Backlog demo workspace: a local-first, file-based backlog that supports agent collaboration with an auditable trail.\nNo server runtime is included (by design).\nHighlights\n\nFile-first backlog as the system of record under _kano/backlog/ (items + decisions + views)\nAgent-friendly CLI to create/update items, enforce Ready-gate discipline, and keep views in sync\nTopics + Worksets to manage multi-session context and per-item scratch space\nDeterministic views (dashboards/reports) refreshable from canonical data\n\nWhat‚Äôs included\n\nWork items\n\nCreate/read and validate Ready-gate fields\nUpdate state transitions with append-only Worklog entries\n\n\nTopics\n\nCreate topics and attach items/snippets/materials for shared exploration context\n\n\nWorksets\n\nInitialize per-item working directories for deliverables and notes\n\n\nViews\n\nRefresh dashboards/reports after changes to keep the backlog readable\n\n\nAuditability\n\nAppend-only Worklog patterns and CLI-driven changes for traceability\nWorklog entries support [agent=...] and [model=...] attribution (defaults to unknown when not provided)\n\n\n\nGetting started\n\nInstall the skill (editable)\n\npython -m pip install -e skills/kano-agent-backlog-skill\n\n\nCheck environment health\n\npython skills/kano-agent-backlog-skill/scripts/kano-backlog doctor --format plain\n\n\nCreate a work item\n\npython skills/kano-agent-backlog-skill/scripts/kano-backlog workitem create --type task --title &quot;...&quot; --agent &lt;agent&gt; --product &lt;product&gt;\n\n\nRefresh views\n\npython skills/kano-agent-backlog-skill/scripts/kano-backlog view refresh --agent &lt;agent&gt; --product &lt;product&gt;\n\n\n\nKnown limitations (MVP)\n\nNo server / MCP / HTTP runtime (local-first only)\nVector backends / embeddings provider integrations are not implemented in this release\nSome workflows are intentionally minimal to keep the system deterministic and auditable\n\nNotes for agents\n\nPrefer running backlog operations through the kano-backlog CLI to preserve the audit trail.\nIf the model name is unknown, record unknown (recommended over guessing).\n"},"demo/docs/releases/0.0.2":{"title":"0.0.2","links":[],"tags":[],"content":"0.0.2 ‚Äî Topics system hardening + embedding pipeline foundations\nThis release focuses on two areas:\n\nTopic system enhancements to make multi-session, multi-agent context management reliable.\nEmbedding preprocessing + vector backend research foundations to enable local-first semantic retrieval work in subsequent releases.\n\nNo server runtime is included (by design).\nHighlights\n\nTopics: templates/archetypes for consistent topic scaffolding\nTopics: cross-references (related_topics) with bidirectional linking\nTopics: snapshots (create/list/restore/cleanup) for safe checkpoints\nTopics: merge/split operations with history preservation and dry-run mode\nEmbeddings: clarified requirements and decisions (cross-lingual retrieval; per-model indexes)\nEmbeddings: deterministic chunking/token-budget contracts and vector backend adapter contract captured in backlog items\n\nRelease focus topics (backlog evidence)\n\n_kano/backlog/topics/topic-system-enhancements/brief.md\n_kano/backlog/topics/embedding-preprocessing-and-vector-backend-research/brief.md\n\nKey related decisions:\n\n_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0009_local-first-embedding-search-architecture.md\n_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0035_cross-lingual-retrieval-requirement-and-default-embedding-policy.md\n_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0036_index-strategy-shared-index-now-per-model-indexes-later-via-config.md\n\nKnown limitations\n\nNo server / MCP / HTTP runtime (local-first only)\nEmbedding providers and benchmarking are still evolving; expect iteration and config churn\nVector backend selection remains local-first and dependency-sensitive (Windows compatibility is a primary constraint)\n"},"demo/docs/releases/PROCESS":{"title":"PROCESS","links":[],"tags":[],"content":"Release Process (Demo Repo)\nThis repo treats releases as a documentation + backlog milestone exercise.\r\nKeep everything local-first and auditable (no server runtime).\nChecklist\n\n\nPick the release version (e.g., 0.0.2) and confirm the milestone Epic (e.g., KABSD-EPIC-0003).\n\n\nUpdate the human-facing version references\n\n\n\nREADME.md (top banner + ‚ÄúCurrent Status‚Äù section)\ndocs/releases/&lt;version&gt;.md\nskills/kano-agent-backlog-skill/docs/releases/&lt;version&gt;.md\n\n\nEnsure the release focus topics are in a reviewable state\n\n\nRun topic distill to refresh brief.generated.md (do not overwrite brief.md)\nClose finished topics (kano-backlog topic close ...) and prune snapshots if needed\n\n\nGenerate/merge changelog entries (optional but recommended)\n\n\nGenerate: python skills/kano-agent-backlog-skill/scripts/kano-backlog changelog generate --version &lt;version&gt; --product kano-agent-backlog-skill\nMerge into CHANGELOG.md: python skills/kano-agent-backlog-skill/scripts/kano-backlog changelog merge-unreleased --version &lt;version&gt;\n\n\nAttach release notes to the milestone Epic (recommended)\n\n\npython skills/kano-agent-backlog-skill/scripts/kano-backlog workitem attach-artifact &lt;EPIC_ID&gt; --path docs/releases/&lt;version&gt;.md --no-shared --product kano-agent-backlog-skill --backlog-root-override _kano/backlog --agent &lt;agent-id&gt; --note &quot;Attach release notes&quot;\n\n\nRefresh views (if dashboards are used)\n\n\npython skills/kano-agent-backlog-skill/scripts/kano-backlog view refresh --agent &lt;agent-id&gt; --product kano-agent-backlog-skill\n\nNotes\n\nbrief.generated.md is tool-owned and overwritten by topic distill.\nbrief.md is human-owned and should remain stable across iterations.\nKeep ‚Äúshared index across different embedding models‚Äù out of scope unless explicitly decided; default to per-model indexes keyed by an explicit embedding_space_id.\n"},"index":{"title":"Kano Agent Backlog Skill","links":["docs/workset","docs/topic"],"tags":[],"content":"kano-agent-backlog-skill\n\r\n\r\n\r\n\n\nAI Agent Skills for Spec-Driven Agentic Programming | Local-first backlog | Multi-agent collaboration | Durable decision trail\n\nLocal-first backlog + decision trail for agent collaboration.\nTurn chat-only context (trade-offs, decisions, why-not-that-option) into durable engineering assets, so your agent writes code only after capturing what to do, why, and how to verify.\n\nCode can be rewritten. Lost decisions can‚Äôt.\n\nAgent-first quickstart\nThis repo is meant to be used with an AI agent. The goal is not just to generate code, but to keep an auditable trail of intent, constraints, and decisions.\nCopy/paste: agent instructions\nPaste this into your agent‚Äôs system prompt / project instructions, ex: AGENTS.md:\nYou are an engineering agent working in this repository.\n \nRules of engagement:\n- Read and follow SKILL.md.\n- Before changing any code, ensure there is a Task/Bug item for the change and that it is Ready (Context, Goal, Approach, Acceptance Criteria, Risks / Dependencies are non-empty).\n- Use kano-backlog commands to create/update items and append Worklog entries; Worklog is append-only.\n- Use a workset while implementing: init -&gt; next -&gt; edit plan.md (check off completed steps) -&gt; next -&gt; repeat.\n- If a decision is load-bearing, create an ADR and link it from the item.\n- Prefer deterministic, repo-grounded outputs; do not invent file paths or backlog state.\nThe execution loop (minimal)\n# One-time setup in a repo\nkano-backlog backlog init --product &lt;my-product&gt; --agent &lt;agent-id&gt;\n \n# Start work (tickets-first)\nkano-backlog item create --type task --title &quot;...&quot; --agent &lt;agent-id&gt; --product &lt;my-product&gt;\nkano-backlog item set-ready &lt;ITEM_ID&gt; --product &lt;my-product&gt; \\\n  --context &quot;...&quot; --goal &quot;...&quot; --approach &quot;...&quot; --acceptance-criteria &quot;...&quot; --risks &quot;...&quot;\nkano-backlog item update-state &lt;ITEM_ID&gt; --state InProgress --agent &lt;agent-id&gt; --product &lt;my-product&gt;\n \n# Prevent drift while implementing\nkano-backlog workset init --item &lt;ITEM_ID&gt; --agent &lt;agent-id&gt;\nkano-backlog workset next --item &lt;ITEM_ID&gt;\n \n# (Edit _kano/backlog/.cache/worksets/items/&lt;ITEM_ID&gt;/plan.md to check off steps)\n# Then run `kano-backlog workset next --item &lt;ITEM_ID&gt;` again.\n \n# Write back anything worth keeping\nkano-backlog workset promote --item &lt;ITEM_ID&gt; --agent &lt;agent-id&gt;\nkano-backlog view refresh --product &lt;my-product&gt; --agent &lt;agent-id&gt;\nTip: most commands support --format json for structured agent/tool integration.\nWhat this is\nkano-agent-backlog-skill is an Agent Skill bundle (centered around SKILL.md) that guides/constrains an agent into a ‚Äútickets first‚Äù workflow:\n\nCreate/update a work item (Epic/Feature/UserStory/Task/Bug) before any code change\nCapture key decisions via append-only Worklog entries or ADRs, and link them together\nEnforce a Ready gate so each item has the minimum shippable context (Context/Goal/Approach/Acceptance/Risks)\nOptional Obsidian views (Dataview / Bases) so humans can inspect, intervene, and review\n\nThis skill is local-first: you can start without Jira / Azure Boards and still keep engineering discipline.\nWhy you might want it\nIf any of these sound familiar, this helps:\n\nYou made an architecture choice, but later forgot why you didn‚Äôt pick the other option\nThe agent output works, but maintenance feels like archaeology (missing rationale and constraints)\nRequirement changes force you back into chat history to understand impact\nYou want the agent as a teammate, but you end up acting as the ‚Äúhuman memory cache‚Äù\n\nGoal: convert ‚Äúevaporating context‚Äù into searchable, linkable, auditable files in your repo.\nThe Dual-Readability Principle\nTopics and Snapshots are designed to solve two problems at once:\n\nHuman Overload: Humans cannot keep entire repo states in their head. We need high-level summaries (brief.md, Reports) to make decisions.\nAgent Coordination: Agents cannot ‚Äúsee‚Äù the repo like we do. They need explicit lists of files, line numbers, and ‚Äústub inventories‚Äù to know what to do next.\n\nBy enforcing Dual-Readability (Markdown for humans, JSON/Structured data for Agents), we create a shared workspace where:\n\nHumans provide direction (via Briefs).\nAgents provide evidence (via Snapshots).\nBoth can understand the other‚Äôs output without translation.\n\nWhat you get (implemented)\n\nSKILL.md: the workflow and rules (planning-before-coding, Ready gate, worklog discipline)\nreferences/schema.md: item types, states, naming, minimal frontmatter\nreferences/templates.md: work item / ADR templates\nreferences/workflow.md: SOP (when to create items, when to record decisions, how to converge)\nreferences/views.md: Obsidian view patterns (Dataview + Bases)\nkano-backlog: Typer-based CLI entrypoint with subcommands:\n\nitem - Create and manage work items (Epic/Feature/UserStory/Task/Bug)\nitem update-state - Transition item states with worklog tracking\nworklog - Append worklog entries\nadr - Create and manage Architecture Decision Records\nworkset - Per-item execution cache (init/refresh/next/promote/cleanup/detect-adr)\ntopic - Context grouping and switching (create/add/pin/distill/decision-audit/switch/export-context/list)\nworkitem - Item utilities (including add-decision write-back)\nview - Generate dashboards and reports\nbacklog - Initialize and manage backlog structure\ndoctor - Validate backlog health\n\n\nsrc/kano_backlog_core: canonical models/storage helpers\nsrc/kano_backlog_ops: use-cases (create/update/view/workset/topic)\nsrc/kano_backlog_cli: CLI wiring (commands + utilities)\n\nNote: backlog administration commands are grouped under kano-backlog backlog ....\nOptionally, create _kano/backlog/ in your project repo to store items, ADRs, views, and helper scripts as the system of record.\nInstall and run\nFrom this repository root:\npython -m pip install -e .\nkano-backlog --help\nNo-install option (useful for agents/tools that run scripts directly):\npython scripts/kano-backlog --help\nQuick start (see value in ~5 minutes)\n\nRun kano-backlog backlog init --product &lt;my-product&gt; --agent &lt;id&gt; to scaffold _kano/backlog/products/&lt;my-product&gt;/\n(Optional) Open the repo in Obsidian and enable Dataview or Bases\nOpen _kano/backlog/products/&lt;my-product&gt;/views/ (or regenerate them with kano-backlog view refresh --agent &lt;id&gt; --product &lt;my-product&gt;)\nBefore any code change, create a Task/Bug and satisfy the Ready gate\nWhen a load-bearing decision happens, append a Worklog line; create an ADR when it‚Äôs truly architectural and link it\n\nWorkset and Topic Usage\nWorksets: Focused Execution\nWorksets prevent agent drift during task execution by providing a structured working context:\n# Initialize workset for a task\nkano-backlog workset init --item TASK-0042 --agent kiro\n \n# Get next action from plan\nkano-backlog workset next --item TASK-0042\n \n# Detect decisions that should become ADRs\nkano-backlog workset detect-adr --item TASK-0042\n \n# Promote deliverables to canonical artifacts\nkano-backlog workset promote --item TASK-0042 --agent kiro\n \n# Clean up expired worksets\nkano-backlog workset cleanup --ttl-hours 72\nSee workset.md for complete documentation.\nTopics: Context Switching\nTopics enable rapid context switching when focus areas change.\nUnlike worksets (which live entirely under _kano/backlog/.cache/worksets/items/&lt;item-id&gt;/), topics are stored under _kano/backlog/topics/&lt;topic&gt;/ so the deterministic brief.md can be shared/reviewed. Raw materials under materials/ are treated as cache.\n# Create a topic for related work\nkano-backlog topic create auth-refactor --agent kiro\n \n# Add items to the topic\nkano-backlog topic add auth-refactor --item TASK-0042\nkano-backlog topic add auth-refactor --item BUG-0012\n \n# Pin relevant documents\nkano-backlog topic pin auth-refactor --doc _kano/backlog/decisions/ADR-0015.md\n \n# Collect a code snippet reference (optional cached snapshot)\nkano-backlog topic add-snippet auth-refactor --file src/auth.py --start 10 --end 25 --agent kiro --snapshot\n \n# Distill deterministic brief.generated.md from collected materials\nkano-backlog topic distill auth-refactor\n \n# Generate a decision write-back audit report (writes to topic publish/)\nkano-backlog topic decision-audit auth-refactor --format plain\n \n# Switch active topic (per-agent pointer lives in cache)\nkano-backlog topic switch auth-refactor --agent kiro\n \n# Export context bundle\nkano-backlog topic export-context auth-refactor --format json\n \n# Close and later cleanup closed topics\nkano-backlog topic close auth-refactor --agent kiro\nkano-backlog topic cleanup --ttl-days 14\nkano-backlog topic cleanup --ttl-days 14 --apply\n \n# Write back a decision to a work item (appends to the item&#039;s Decisions section + Worklog)\nkano-backlog workitem add-decision KABSD-TSK-0001 \\\n  --decision &quot;Use X over Y because ...&quot; \\\n  --source &quot;_kano/backlog/topics/auth-refactor/synthesis/decision-notes.md&quot; \\\n  --agent kiro \\\n  --product &lt;my-product&gt;\nSee topic.md for complete documentation.\nSkill version\nShow the current skill version:\npython -c &quot;import pathlib; print((pathlib.Path(&#039;VERSION&#039;)).read_text().strip())&quot;\nExternal references\n\nAgent skills overview (Anthropic/Claude): platform.claude.com/docs/en/agents-and-tools/agent-skills/overview\nVersioning policy: VERSIONING.md\nRelease notes: CHANGELOG.md\n\nContributing\nPRs welcome, with one rule: don‚Äôt turn this into another Jira.\nThe point is to preserve decisions and acceptance, not to worship process.\nLicense\nMIT"},"skill-guide":{"title":"skill-guide","links":[],"tags":[],"content":"Kano Agent Backlog Skill (local-first)\nScope\nUse this skill to:\n\nPlan new work by creating backlog items before code changes.\nMaintain hierarchy and relationships via parent links, as defined by the active process profile.\nRecord decisions with ADRs and link them to items.\nKeep a durable, append-only worklog for project evolution.\n\nAgent compatibility: read the whole skill\n\nAlways load the entire SKILL.md before acting; some agent shells only fetch the first ~100 lines by default.\nIf your client truncates, fetch in chunks (e.g., lines 1-200, 200-400, ‚Ä¶) until you see the footer marker END_OF_SKILL_SENTINEL.\nIf you cannot confirm the footer marker, stop and ask for help; do not proceed with partial rules.\nWhen generating per-agent guides, preserve this read-all requirement so downstream agents stay in sync.\n\nNon-negotiables\n\nPlanning before coding: create/update items and meet the Ready gate before making code changes.\nWorklog is append-only; never rewrite history.\nUpdate Worklog whenever:\n\na discussion produces a clear decision or direction,\nan item state changes,\nscope/approach changes,\nor an ADR is created/linked.\n\n\nArchive by view: hide Done/Dropped items in views by default; do not move files unless explicitly requested.\nBacklog volume control:\n\nOnly create items for work that changes code or design decisions.\nAvoid new items for exploratory discussion; record in existing Worklog instead.\nKeep Tasks/Bugs sized for a single focused session.\nAvoid ADRs unless a real architectural trade-off is made.\n\n\nTicketing threshold (agent-decided):\n\nOpen a new Task/Bug when you will change code/docs/views/scripts.\nOpen an ADR (and link it) when a real trade-off or direction change is decided.\nOtherwise, record the discussion in an existing Worklog; ask if unsure.\n\n\nTicket type selection (keep it lightweight):\n\nEpic: multi-release or multi-team milestone spanning multiple Features.\nFeature: a new capability that delivers multiple UserStories.\nUserStory: a single user-facing outcome that requires multiple Tasks.\nTask: a single focused implementation or doc change (typically one session).\nExample: ‚ÄúEnd-to-end embedding pipeline‚Äù = Epic; ‚ÄúPluggable vector backend‚Äù = Feature; ‚ÄúMVP chunking pipeline‚Äù = UserStory; ‚ÄúImplement tokenizer adapter‚Äù = Task.\n\n\nBug vs Task triage (when fixing behavior):\n\nIf you are correcting a behavior that was previously marked Done and the behavior violates the original intent/acceptance (defect or regression), open a Bug and link it to the original item.\nIf the change is a new requirement/scope change beyond the original acceptance, open a Task/UserStory (or Feature) instead, and link it for traceability.\n\n\nBug origin tracing (when diagnosing a defect/regression):\n\nRecord when the issue started and the evidence path you used to determine it.\nPrefer VCS-backed evidence when available:\n\nlast-known-good revision (commit hash or tag)\nfirst-known-bad revision (commit hash or tag)\nsuspected introducing change(s) (commit hash) and why (e.g., git blame on specific lines)\n\n\nIf git history is unavailable (zip export, shallow clone, missing remote), explicitly record that limitation and what alternative evidence you used (e.g., release notes, timestamps, reproduction reports).\nKeep evidence lightweight: record commit hashes + 1‚Äì2 line summaries; avoid pasting large diffs into Worklog. Attach artifacts when needed.\nSuggested Worklog template:\n\nBug origin: last_good=&lt;sha|tag&gt;, first_bad=&lt;sha|tag&gt;, suspect=&lt;sha&gt; (reason: blame &lt;path&gt;:&lt;line&gt;), evidence=&lt;git log/blame/bisect|other&gt;\n\n\n\n\nState ownership: the agent decides when to move items to InProgress or Done; humans observe and can add context.\nState semantics: Proposed = needs discovery/confirmation; Planned = approved but not started; Ready gate applies before start.\nHierarchy is in frontmatter links, not folder nesting; avoid moving files to reflect scope changes.\nFilenames stay stable; use ASCII slugs.\nNever include secrets in backlog files or logs.\nLanguage: backlog and documentation content must be English-only (no CJK), to keep parsing and cross-agent collaboration deterministic.\nAgent Identity: In Worklog and audit logs, use your own identity (e.g., [agent=antigravity]), never copy [agent=codex] blindly.\nAlways provide an explicit --agent value for auditability (some commands currently default to cli, but do not rely on it).\nModel attribution (optional but preferred): provide --model &lt;name&gt; (or env KANO_AGENT_MODEL / KANO_MODEL) when it is known deterministically.\n\nDo not guess model names; if unknown, omit the [model=...] segment.\n\n\nAgent Identity Protocol: Supply --agent &lt;ID&gt; with your real product name (e.g., cursor, copilot, windsurf, antigravity).\n\nForbidden (Placeholders): auto, user, assistant, &lt;AGENT_NAME&gt;, $AGENT_NAME.\n\n\nFile operations for backlog/skill artifacts must go through the kano-backlog CLI\r\n(python skills/kano-agent-backlog-skill/scripts/kano-backlog &lt;command&gt;) so audit logs capture the action.\nSkill scripts only operate on paths under _kano/backlog/ or _kano/backlog_sandbox/;\r\nrefuse other paths.\nAfter modifying backlog items, refresh the plain Markdown views immediately using\r\npython skills/kano-agent-backlog-skill/scripts/kano-backlog view refresh --agent &lt;agent-id&gt; --backlog-root &lt;path&gt; so the dashboards stay current.\n\nPersona summaries/reports are available via python skills/kano-agent-backlog-skill/scripts/kano-backlog admin persona summary|report ....\n\n\npython skills/kano-agent-backlog-skill/scripts/kano-backlog workitem update-state ... auto-syncs parent states forward-only by default; use --no-sync-parent\r\nfor manual re-plans where parent state should stay put.\nAdd Obsidian [[wikilink]] references in the body (e.g., a ## Links section) so Graph/backlinks work; frontmatter alone does not create graph edges.\n\nAgent compatibility: read the whole skill\n\nAlways load the entire SKILL.md before acting; some agent shells only fetch the first ~100 lines by default.\nIf your client truncates, fetch in chunks (e.g., lines 1-200, 200-400, ‚Ä¶) until you see the footer marker END_OF_SKILL_SENTINEL.\nIf you cannot confirm the footer marker, stop and ask for help; do not proceed with partial rules.\nWhen generating per-agent guides, preserve this read-all requirement so downstream agents stay in sync.\n\nFirst-run bootstrap (prereqs + initialization)\nBefore using this skill in a repo, the agent must confirm:\n\nPython prerequisites are available (or install them), and\nthe backlog scaffold exists for the target product/root.\n\nIf the backlog structure is missing, propose the bootstrap commands and wait for user approval before writing files.\nDeveloper vs user mode (where to declare it)\n\nPreferred source of truth: product config in _kano/backlog/products/&lt;product&gt;/_config/config.json.\n\nmode.skill_developer: true when this repo actively develops the skill itself (this demo repo).\nmode.persona: optional string describing the primary human persona (e.g. developer, pm, qa), used only for human-facing summaries/views.\n\n\nSecondary: agent guide files (e.g., AGENTS.md / CLAUDE.md) can document expectations, but are agent-specific and not script-readable.\n\nSkill developer gate (architecture compliance)\nIf mode.skill_developer=true, before writing any skill code (in scripts/ or src/), you must:\n\nRead ADR-0013 (‚ÄúCodebase Architecture and Module Boundaries‚Äù) in the product decisions folder.\nFollow the folder rules defined in ADR-0013:\n\nscripts/ is executable-only: no reusable module code.\nsrc/ is import-only: core logic lives here, never executed directly.\nAll agent-callable operations go through scripts/kano-backlog CLI.\n\n\nPlace new code in the correct package:\n\nModels/config/errors ‚Üí src/kano_backlog_core/\nUse-cases (create/update/view) ‚Üí src/kano_backlog_ops/\nStorage backends ‚Üí src/kano_backlog_adapters/\nCLI commands ‚Üí src/kano_backlog_cli/commands/\n\n\n\nViolating these boundaries will be flagged in code review.\nPrerequisite install (Python)\nDetect:\n\nRun python skills/kano-agent-backlog-skill/scripts/kano-backlog doctor --format plain.\n\nIf packages are missing, install once (recommended):\n\nDefault: python -m pip install -e skills/kano-agent-backlog-skill\nSkill contributors: python -m pip install -e skills/kano-agent-backlog-skill[dev]\nOptional heavy dependencies (FAISS, sentence-transformers) should be installed manually per platform requirements before running the CLI against embedding features.\n\nBacklog initialization (file scaffold + config + dashboards)\nDetect (multi-product / platform layout):\n\nProduct initialized if _kano/backlog/products/&lt;product&gt;/_config/config.json exists (or confirm via python skills/kano-agent-backlog-skill/scripts/kano-backlog doctor --product &lt;product&gt;).\n\nBootstrap:\n\nRun python skills/kano-agent-backlog-skill/scripts/kano-backlog admin init --product &lt;product&gt; --agent &lt;agent-id&gt; [--backlog-root &lt;path&gt;] to scaffold _kano/backlog/products/&lt;product&gt;/ (items/, decisions/, views/, _config/, _meta/, _index/).\nThe init command derives a project prefix, writes _config/config.json, and refreshes dashboards so views exist immediately after initialization.\nManual fallback (only if automation is unavailable): follow _kano/backlog/README.md to copy the template scaffold, then refresh views via kano-backlog view refresh.\n\nOptional LLM analysis over deterministic reports\nThis skill can optionally append an LLM-generated analysis to a deterministic report.\r\nThe deterministic report is the SSOT; analysis is treated as a derived artifact.\n\nDeterministic report: views/Report_&lt;persona&gt;.md\nDerived LLM output: views/_analysis/Report_&lt;persona&gt;_LLM.md (gitignored by default)\nDeterministic prompt artifact: views/_analysis/Report_&lt;persona&gt;_analysis_prompt.md\n\nEnable by config (per product):\n\nanalysis.llm.enabled = true\n\nExecution:\n\nThe default workflow is: generate the deterministic report ‚Üí use it as SSOT ‚Üí fill in the analysis template.\n\nThe skill generates a deterministic prompt file to guide the analysis, and a derived markdown file with placeholder headings.\n\n\nOptional automation: when analysis.llm.enabled = true in config, view refresh generates views/snapshots/_analysis/Report_&lt;persona&gt;_analysis_prompt.md (deterministic prompt) and Report_&lt;persona&gt;_LLM.md (template or LLM output)\nNever pass API keys as CLI args; keep secrets in env vars to avoid leaking into audit logs.\n\nID prefix derivation\n\nSource of truth:\n\nProduct config: _kano/backlog/products/&lt;product&gt;/_config/config.toml (product.name, product.prefix), or\nRepo config (single-product): _kano/backlog/_config/config.toml (product.name, product.prefix).\n\n\nDerivation:\n\nSplit product.name on non-alphanumeric separators and camel-case boundaries.\nTake the first letter of each segment.\nIf only one letter, take the first letter plus the next consonant (A/E/I/O/U skipped).\nIf still short, use the first two letters.\nUppercase the result.\n\n\nExample: product.name=kano-agent-backlog-skill-demo ‚Üí KABSD.\n\nRecommended layout\nThis skill supports both single-product and multi-product layouts:\n\nSingle-product (repo-level): _kano/backlog/\nMulti-product (monorepo): _kano/backlog/products/&lt;product&gt;/\n\nWithin each backlog root:\n\n_meta/ (schema, conventions)\nitems/&lt;type&gt;/&lt;bucket&gt;/ (work items)\ndecisions/ (ADR files)\nviews/ (dashboards / generated Markdown)\n\nItem bucket folders (per 100)\n\nStore items under _kano/backlog/items/&lt;type&gt;/&lt;bucket&gt;/.\nBucket names use 4 digits for the lower bound of each 100 range.\n\nExample: 0000, 0100, 0200, 0300, ‚Ä¶\n\n\nExample path:\n\n_kano/backlog/items/task/0000/KABSD-TSK-0007_define-secret-provider-validation.md\n\n\n\nIndex/MOC files\n\nFor Epic, create an adjacent index file:\n\n&lt;ID&gt;_&lt;slug&gt;.index.md\n\n\nIndex files should render a tree using Dataview/DataviewJS and rely on parent links.\nTrack epic index files in _kano/backlog/_meta/indexes.md (type, item_id, index_file, updated, notes).\n\nReferences\n\nReference index: REFERENCE.md\nSchema and rules: references/schema.md\nTemplates: references/templates.md\nWorkflow SOP: references/workflow.md\nView patterns: references/views.md\nObsidian Bases (plugin-free): references/bases.md\nContext Graph + Graph-assisted retrieval: references/context_graph.md\n\nIf the backlog structure is missing, propose creation and wait for user approval before writing files.\nKano CLI entrypoints (current surface)\nscripts/ exposes a single executable: scripts/kano-backlog. The CLI is intentionally organized as nested command groups so agents can discover operations via --help on-demand (instead of hard-coding the full command surface into this skill).\nHelp-driven discovery (preferred)\nRun these in order, expanding only what you need:\n\npython skills/kano-agent-backlog-skill/scripts/kano-backlog --help\n\nShows top-level groups (e.g., backlog, item, state, worklog, view) and global options.\n\n\npython skills/kano-agent-backlog-skill/scripts/kano-backlog &lt;group&gt; --help\n\nShows subcommands for that group.\n\n\npython skills/kano-agent-backlog-skill/scripts/kano-backlog &lt;group&gt; &lt;command&gt; --help\n\nShows required args/options for that command.\n\n\n\nGuideline: do not paste large --help output into chat; inspect it locally and run the command.\nCanonical examples (keep these few memorized)\n\nBootstrap:\n\npython skills/kano-agent-backlog-skill/scripts/kano-backlog doctor --format plain\npython skills/kano-agent-backlog-skill/scripts/kano-backlog admin init --product &lt;name&gt; --agent &lt;id&gt;\n\n\nDaily workflow:\n\npython skills/kano-agent-backlog-skill/scripts/kano-backlog workitem create --type task --title &quot;...&quot; --agent &lt;id&gt; --product &lt;name&gt;\npython skills/kano-agent-backlog-skill/scripts/kano-backlog workitem set-ready &lt;item-id&gt; --context &quot;...&quot; --goal &quot;...&quot; --approach &quot;...&quot; --acceptance-criteria &quot;...&quot; --risks &quot;...&quot; --product &lt;name&gt;\npython skills/kano-agent-backlog-skill/scripts/kano-backlog workitem validate &lt;item-id&gt; --product &lt;name&gt;\npython skills/kano-agent-backlog-skill/scripts/kano-backlog workitem update-state &lt;item-ref&gt; --state InProgress --agent &lt;id&gt; --message &quot;...&quot; --product &lt;name&gt;\npython skills/kano-agent-backlog-skill/scripts/kano-backlog workitem attach-artifact &lt;item-id&gt; --path &lt;file&gt; --shared --agent &lt;id&gt; --product &lt;name&gt; [--note &quot;...&quot;]\npython skills/kano-agent-backlog-skill/scripts/kano-backlog view refresh --agent &lt;id&gt; --product &lt;name&gt;\n\n\nBacklog integrity checks:\n\npython skills/kano-agent-backlog-skill/scripts/kano-backlog admin validate uids --product &lt;name&gt;\n\n\n\nConflict handling policy (configurable)\nUse product config to control how duplicate IDs and UIDs are handled by maintenance commands\r\nsuch as admin links normalize-ids.\n\nConfig keys (product _config/config.toml):\n\nconflict_policy.id_conflict: default rename (rename duplicate IDs).\nconflict_policy.uid_conflict: default trash_shorter (move shorter duplicate content to _trash/).\n\n\ntrash_shorter uses _trash/&lt;YYYYMMDD&gt;/... under the product root; items get a Worklog entry.\n\nSandbox workflow (isolated experimentation)\nFor testing, prototyping, or demos without affecting production backlog:\n\nCreate: python skills/kano-agent-backlog-skill/scripts/kano-backlog admin sandbox init &lt;sandbox-name&gt; --product &lt;source-product&gt; --agent &lt;id&gt;\nUse: python skills/kano-agent-backlog-skill/scripts/kano-backlog workitem create --product &lt;sandbox-name&gt; ... (same CLI, different product)\nCleanup: rm -rf _kano/backlog_sandbox/&lt;sandbox-name&gt; (git will ignore this directory)\nRationale: Sandboxes mirror production structure but live in _kano/backlog_sandbox/, so changes never leak into _kano/backlog/.\n\nArtifacts policy (local-first)\n\nStorage locations:\n\nShared across products: _kano/backlog/_shared/artifacts/&lt;ITEM_ID&gt;/ (use --shared).\nProduct-local: _kano/backlog/products/&lt;product&gt;/artifacts/&lt;ITEM_ID&gt;/ (use --no-shared).\n\n\nUsage:\n\nAttach via workitem attach-artifact ‚Äî copies the file and appends a Worklog link.\nPrefer lightweight, text-first artifacts (Markdown, Mermaid, small images). Use Git LFS for large binaries if needed.\n\n\nGit policy:\n\nCommit human-readable artifacts that aid review. Avoid committing generated binaries unless justified.\nSandboxes under _kano/backlog_sandbox/ are gitignored; artifacts there are ephemeral.\nFor derived analysis, store under views/_analysis/ (gitignored by default), and keep deterministic reports in views/.\n\n\nLinking:\n\nThe CLI appends a Markdown link relative to the item file. Optionally add a ## Links section for richer context.\n\n\n\nState update helper\n\nUse python skills/kano-agent-backlog-skill/scripts/kano-backlog workitem update-state ... to update state + append Worklog.\nPrefer --action on kano-backlog state transition for the common transitions (start, ready, review, done, block, drop).\nUse python skills/kano-agent-backlog-skill/scripts/kano-backlog workitem validate &lt;item-id&gt; to check the Ready gate explicitly.\n\nTopic and Workset workflow (context management)\nWhen to use Topics\nTopics are shareable context buffers for multi-step work that spans multiple work items or requires exploratory research before creating formal backlog items.\nUse Topics when:\n\nExploring a complex problem that may result in multiple work items\nCollecting code snippets, logs, and materials across multiple sessions\nCollaborating across agents/sessions with a shared context\nRefactoring work that requires tracking multiple code locations\n\nTopic lifecycle:\n\nCreate: python skills/kano-agent-backlog-skill/scripts/kano-backlog topic create &lt;topic-name&gt; --agent &lt;id&gt;\n\nCreates _kano/backlog/topics/&lt;topic&gt;/ with manifest.json, brief.md, brief.generated.md, notes.md, and materials/ subdirectories\n\n\nCollect materials:\n\nAdd items: topic add &lt;topic-name&gt; --item &lt;ITEM_ID&gt;\nAdd code snippets: topic add-snippet &lt;topic-name&gt; --file &lt;path&gt; --start &lt;line&gt; --end &lt;line&gt; --agent &lt;id&gt;\nPin docs: topic pin &lt;topic-name&gt; --doc &lt;path&gt;\n\n\nDistill: python skills/kano-agent-backlog-skill/scripts/kano-backlog topic distill &lt;topic-name&gt;\n\n\nGenerates/overwrites deterministic brief.generated.md from collected materials\nbrief.md is a stable, human-maintained brief (do not overwrite it automatically)\n\n\nSwitch context: python skills/kano-agent-backlog-skill/scripts/kano-backlog topic switch &lt;topic-name&gt; --agent &lt;id&gt;\n\nSets active topic (affects config overlays and workset behavior)\n\n\nClose: python skills/kano-agent-backlog-skill/scripts/kano-backlog topic close &lt;topic-name&gt; --agent &lt;id&gt;\n\nMarks topic as closed; eligible for TTL cleanup\n\n\nCleanup: python skills/kano-agent-backlog-skill/scripts/kano-backlog topic cleanup --ttl-days &lt;N&gt; [--dry-run]\n\nRemoves raw materials from closed topics older than TTL\n\n\n\nTopic snapshots (retention policy):\n\nSnapshots are intended for milestone checkpoints (pre-merge/split/restore, risky bulk edits), not every small edit.\nTo prevent noise, keep only the latest snapshot per topic in this demo repo.\nAfter creating a snapshot (or periodically), prune all but the newest snapshot:\n\npython skills/kano-agent-backlog-skill/scripts/kano-backlog topic snapshot cleanup &lt;topic-name&gt; --ttl-days 0 --keep-latest 1 --apply\n\n\n\nTopic structure:\n_kano/backlog/topics/&lt;topic&gt;/\r\n  manifest.json          # refs to items/docs/snippets, status, timestamps\r\n  brief.md               # stable, human-maintained brief (do not overwrite automatically)\r\n  brief.generated.md     # deterministic distilled brief (generated/overwritten by `topic distill`)\r\n  notes.md               # freeform notes (backward compat)\r\n  materials/             # raw collection (gitignored by default)\r\n    clips/               # code snippet refs + cached text\r\n    links/               # urls / notes\r\n    extracts/            # extracted paragraphs\r\n    logs/                # build logs / command outputs\r\n  synthesis/             # intermediate drafts\r\n  publish/               # prepared write-backs (patches/ADRs)\r\n  config.toml            # optional topic-specific config overrides\n\nWhen to use Worksets\nWorksets are per-item working directories (cached, derived data) for a single backlog item.\nUse Worksets when:\n\nStarting work on a specific Task/Bug/UserStory\nNeed scratch space for deliverables (patches, test artifacts, etc.)\nWant item-specific config overrides (rare)\n\nWorkset lifecycle:\n\nInitialize: python skills/kano-agent-backlog-skill/scripts/kano-backlog workset init &lt;ITEM_ID&gt; --agent &lt;id&gt; [--ttl-hours &lt;N&gt;]\n\nCreates _kano/backlog/.cache/worksets/items/&lt;ITEM_ID&gt;/ with meta.json, plan.md, notes.md, deliverables/\n\n\nWork: Store scratch files in deliverables/ (patches, test outputs, etc.)\nRefresh: python skills/kano-agent-backlog-skill/scripts/kano-backlog workset refresh &lt;ITEM_ID&gt; --agent &lt;id&gt;\n\nUpdates refreshed_at timestamp\n\n\nCleanup: python skills/kano-agent-backlog-skill/scripts/kano-backlog workset cleanup --ttl-hours &lt;N&gt; [--dry-run]\n\nRemoves stale worksets older than TTL\n\n\n\nWorkset structure:\n_kano/backlog/.cache/worksets/items/&lt;ITEM_ID&gt;/\r\n  meta.json              # workset metadata (item_id, agent, timestamps, ttl)\r\n  plan.md                # execution plan template\r\n  notes.md               # work notes with Decision: marker guidance\r\n  deliverables/          # scratch outputs (patches, logs, test artifacts)\r\n  config.toml            # optional item-specific config overrides\n\nTopic vs Workset decision guide\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScenarioUse TopicUse WorksetExploring before creating items‚úÖ Yes‚ùå NoMulti-item refactor‚úÖ Yes‚ùå NoCollecting code snippets across files‚úÖ Yes‚ùå NoShared context for collaboration‚úÖ Yes‚ùå NoSingle item scratch space‚ùå No‚úÖ YesItem-specific deliverables‚ùå No‚úÖ YesVersion-controlled distillation‚úÖ Yes (brief.generated.md)‚ùå No\nBest practice: Start exploration in a Topic, create work items as scope clarifies, then use Worksets for individual item execution.\nActive topic and config overlays\n\nActive topic is per-agent: _kano/backlog/.cache/worksets/active_topic.&lt;agent&gt;.txt\nWhen an agent has an active topic, config resolution includes topic overrides:\n\nLayer order: defaults ‚Üí product ‚Üí topic ‚Üí workset ‚Üí runtime\nTopic config: _kano/backlog/topics/&lt;topic&gt;/config.toml\nUse for temporary overrides (e.g., switch default_product during exploration)\n\n\nGet active topic: python skills/kano-agent-backlog-skill/scripts/kano-backlog topic show --agent &lt;id&gt;\n\nMaterials buffer (Topic-specific)\n\nReference-first snippet collection: Avoid large copy-paste; store file+line+hash+optional snapshot\nSnippet refs include:\n\nfile: relative path from workspace root\nlines: [start, end] (1-based inclusive)\nhash: sha256:... of content for staleness check\ncached_text: optional snapshot (use --snapshot to include)\nrevision: git commit hash if available\n\n\n\nHuman decision materials vs. machine manifest\nDual-Readability Design: Every artifact checks against both human and agent readability:\n\nHuman-Readable: High-level summaries, clear checklists, ‚Äúmanager-friendly‚Äù reports for rapid decision-making\nAgent-Readable: Structural precision, file paths, line numbers, explicit markers for action without hallucination\n\nImplementation in Topics:\n\nTreat manifest.json as machine-oriented metadata:\n\nseed_items: UUID list for precise agent reference\nsnippet_refs: file+line+hash for deterministic loading\npinned_docs: absolute paths for unambiguous reference\n\n\nKeep brief.generated.md deterministic and tool-owned (generated/overwritten by topic distill):\n\nReadable item titles (e.g., ‚ÄúKABSD-TSK-0042: Implement tokenizer adapter‚Äù)\nIf available, include item path and keep UID in a hidden HTML comment for deterministic mapping\nMaterials index with items/docs/snippets sorted for repeatability\n\n\nKeep brief.md human-oriented and stable (do not overwrite automatically):\n\nContext summary and key decisions\nOptional: include a human-friendly materials list (do not duplicate raw snippet text)\n\n\nPut human-facing decision support in _kano/backlog/topics/&lt;topic&gt;/notes.md (and/or pinned docs), e.g.:\n\nDecision to make\nOptions + trade-offs\nEvidence (ADR links, snippet refs, benchmark/log artifacts)\nRecommendation + follow-ups\n\n\nStaleness detection: Compare current file hash with stored hash to detect if code changed\nDistillation: topic distill generates deterministic brief.generated.md with a repeatable materials index\n\n\nEND_OF_SKILL_SENTINEL"},"skill/CHANGELOG":{"title":"CHANGELOG","links":[],"tags":[],"content":"Changelog\nAll notable changes to kano-agent-backlog-skill will be documented in this file.\nThis project uses Git tags as releases: vX.Y.Z.\n[Unreleased]\n[0.0.2] - 2026-01-19\nAdded\n\nTopic templates/archetypes with variable substitution and CLI integration.\nTopic cross-references (related_topics) with bidirectional linking.\nTopic snapshots (create/list/restore/cleanup) for checkpointing.\nTopic merge/split operations with dry-run support and history preservation.\n\nChanged\n\nTopic distillation renders human-readable seed item listings (ID/title/type/state) while keeping UID mapping in HTML comments.\nArtifact attachment resolves items in product layout (_kano/backlog/products/&lt;product&gt;/items/...) when --backlog-root-override is used with --product.\n\nDocumentation\n\nRelease notes for GitHub Releases: skills/kano-agent-backlog-skill/docs/releases/0.0.2.md.\n\n[0.0.1] - 2026-01-15\nAdded\n\nOptional SQLite index layer (rebuildable) to accelerate reads and view generation.\nDBIndex vs NoDBIndex demo dashboards under _kano/backlog/views/_demo/.\nDemo tool for recent/iteration focus views (_kano/backlog/tools/generate_focus_view.py).\nFirst-run bootstrap (scripts/backlog/bootstrap_init_project.py) + templates to enable the backlog system in a repo.\nviews.auto_refresh config flag (default: true) to keep dashboards up to date automatically.\n\nDocumentation\n\nRelease notes for GitHub Releases: skills/kano-agent-backlog-skill/docs/releases/0.0.1.md.\n\nChanged\n\nUnified generated dashboards to prefer SQLite when enabled/available and fall back to file scan.\nKept scripts/backlog/view_generate_demo.py self-contained; demo repo tool is a thin wrapper.\nMutating scripts auto-refresh dashboards by default; scripts/fs/* now also require --agent for auditability.\n\nFixed\n\nquery_sqlite_index.py --sql validation (SELECT/WITH detection).\n\nAdded\n\nLocal-first backlog structure under _kano/backlog/ (items, decisions/ADRs, views).\nWork item scripts: create items, validate Ready gate, update state with append-only Worklog.\nAudit logging for tool invocations with redaction and rotation.\nPlain Markdown dashboards + Obsidian Dataview/Bases demo views.\nConfig system under _kano/backlog/_config/config.json.\n\nChanged\n\nEnforced explicit --agent for Worklog-writing scripts and auditability.\n\nSecurity\n\nSecret redaction and log rotation defaults for audit logs.\n"},"skill/README":{"title":"README","links":["docs/workset","docs/topic"],"tags":[],"content":"kano-agent-backlog-skill\n\r\n\r\n\r\n\n\nAI Agent Skills for Spec-Driven Agentic Programming | Local-first backlog | Multi-agent collaboration | Durable decision trail\n\nLocal-first backlog + decision trail for agent collaboration.\nTurn chat-only context (trade-offs, decisions, why-not-that-option) into durable engineering assets, so your agent writes code only after capturing what to do, why, and how to verify.\n\nCode can be rewritten. Lost decisions can‚Äôt.\n\nAgent-first quickstart\nThis repo is meant to be used with an AI agent. The goal is not just to generate code, but to keep an auditable trail of intent, constraints, and decisions.\nCopy/paste: agent instructions\nPaste this into your agent‚Äôs system prompt / project instructions, ex: AGENTS.md:\nYou are an engineering agent working in this repository.\n \nRules of engagement:\n- Read and follow SKILL.md.\n- Before changing any code, ensure there is a Task/Bug item for the change and that it is Ready (Context, Goal, Approach, Acceptance Criteria, Risks / Dependencies are non-empty).\n- Use kano-backlog commands to create/update items and append Worklog entries; Worklog is append-only.\n- Use a workset while implementing: init -&gt; next -&gt; edit plan.md (check off completed steps) -&gt; next -&gt; repeat.\n- If a decision is load-bearing, create an ADR and link it from the item.\n- Prefer deterministic, repo-grounded outputs; do not invent file paths or backlog state.\nThe execution loop (minimal)\n# One-time setup in a repo\nkano-backlog backlog init --product &lt;my-product&gt; --agent &lt;agent-id&gt;\n \n# Start work (tickets-first)\nkano-backlog item create --type task --title &quot;...&quot; --agent &lt;agent-id&gt; --product &lt;my-product&gt;\nkano-backlog item set-ready &lt;ITEM_ID&gt; --product &lt;my-product&gt; \\\n  --context &quot;...&quot; --goal &quot;...&quot; --approach &quot;...&quot; --acceptance-criteria &quot;...&quot; --risks &quot;...&quot;\nkano-backlog item update-state &lt;ITEM_ID&gt; --state InProgress --agent &lt;agent-id&gt; --product &lt;my-product&gt;\n \n# Prevent drift while implementing\nkano-backlog workset init --item &lt;ITEM_ID&gt; --agent &lt;agent-id&gt;\nkano-backlog workset next --item &lt;ITEM_ID&gt;\n \n# (Edit _kano/backlog/.cache/worksets/items/&lt;ITEM_ID&gt;/plan.md to check off steps)\n# Then run `kano-backlog workset next --item &lt;ITEM_ID&gt;` again.\n \n# Write back anything worth keeping\nkano-backlog workset promote --item &lt;ITEM_ID&gt; --agent &lt;agent-id&gt;\nkano-backlog view refresh --product &lt;my-product&gt; --agent &lt;agent-id&gt;\nTip: most commands support --format json for structured agent/tool integration.\nWhat this is\nkano-agent-backlog-skill is an Agent Skill bundle (centered around SKILL.md) that guides/constrains an agent into a ‚Äútickets first‚Äù workflow:\n\nCreate/update a work item (Epic/Feature/UserStory/Task/Bug) before any code change\nCapture key decisions via append-only Worklog entries or ADRs, and link them together\nEnforce a Ready gate so each item has the minimum shippable context (Context/Goal/Approach/Acceptance/Risks)\nOptional Obsidian views (Dataview / Bases) so humans can inspect, intervene, and review\n\nThis skill is local-first: you can start without Jira / Azure Boards and still keep engineering discipline.\nWhy you might want it\nIf any of these sound familiar, this helps:\n\nYou made an architecture choice, but later forgot why you didn‚Äôt pick the other option\nThe agent output works, but maintenance feels like archaeology (missing rationale and constraints)\nRequirement changes force you back into chat history to understand impact\nYou want the agent as a teammate, but you end up acting as the ‚Äúhuman memory cache‚Äù\n\nGoal: convert ‚Äúevaporating context‚Äù into searchable, linkable, auditable files in your repo.\nThe Dual-Readability Principle\nTopics and Snapshots are designed to solve two problems at once:\n\nHuman Overload: Humans cannot keep entire repo states in their head. We need high-level summaries (brief.md, Reports) to make decisions.\nAgent Coordination: Agents cannot ‚Äúsee‚Äù the repo like we do. They need explicit lists of files, line numbers, and ‚Äústub inventories‚Äù to know what to do next.\n\nBy enforcing Dual-Readability (Markdown for humans, JSON/Structured data for Agents), we create a shared workspace where:\n\nHumans provide direction (via Briefs).\nAgents provide evidence (via Snapshots).\nBoth can understand the other‚Äôs output without translation.\n\nWhat you get (implemented)\n\nSKILL.md: the workflow and rules (planning-before-coding, Ready gate, worklog discipline)\nreferences/schema.md: item types, states, naming, minimal frontmatter\nreferences/templates.md: work item / ADR templates\nreferences/workflow.md: SOP (when to create items, when to record decisions, how to converge)\nreferences/views.md: Obsidian view patterns (Dataview + Bases)\nkano-backlog: Typer-based CLI entrypoint with subcommands:\n\nitem - Create and manage work items (Epic/Feature/UserStory/Task/Bug)\nitem update-state - Transition item states with worklog tracking\nworklog - Append worklog entries\nadr - Create and manage Architecture Decision Records\nworkset - Per-item execution cache (init/refresh/next/promote/cleanup/detect-adr)\ntopic - Context grouping and switching (create/add/pin/distill/decision-audit/switch/export-context/list)\nworkitem - Item utilities (including add-decision write-back)\nview - Generate dashboards and reports\nbacklog - Initialize and manage backlog structure\ndoctor - Validate backlog health\n\n\nsrc/kano_backlog_core: canonical models/storage helpers\nsrc/kano_backlog_ops: use-cases (create/update/view/workset/topic)\nsrc/kano_backlog_cli: CLI wiring (commands + utilities)\n\nNote: backlog administration commands are grouped under kano-backlog backlog ....\nOptionally, create _kano/backlog/ in your project repo to store items, ADRs, views, and helper scripts as the system of record.\nInstall and run\nFrom this repository root:\npython -m pip install -e .\nkano-backlog --help\nNo-install option (useful for agents/tools that run scripts directly):\npython scripts/kano-backlog --help\nQuick start (see value in ~5 minutes)\n\nRun kano-backlog backlog init --product &lt;my-product&gt; --agent &lt;id&gt; to scaffold _kano/backlog/products/&lt;my-product&gt;/\n(Optional) Open the repo in Obsidian and enable Dataview or Bases\nOpen _kano/backlog/products/&lt;my-product&gt;/views/ (or regenerate them with kano-backlog view refresh --agent &lt;id&gt; --product &lt;my-product&gt;)\nBefore any code change, create a Task/Bug and satisfy the Ready gate\nWhen a load-bearing decision happens, append a Worklog line; create an ADR when it‚Äôs truly architectural and link it\n\nWorkset and Topic Usage\nWorksets: Focused Execution\nWorksets prevent agent drift during task execution by providing a structured working context:\n# Initialize workset for a task\nkano-backlog workset init --item TASK-0042 --agent kiro\n \n# Get next action from plan\nkano-backlog workset next --item TASK-0042\n \n# Detect decisions that should become ADRs\nkano-backlog workset detect-adr --item TASK-0042\n \n# Promote deliverables to canonical artifacts\nkano-backlog workset promote --item TASK-0042 --agent kiro\n \n# Clean up expired worksets\nkano-backlog workset cleanup --ttl-hours 72\nSee workset.md for complete documentation.\nTopics: Context Switching\nTopics enable rapid context switching when focus areas change.\nUnlike worksets (which live entirely under _kano/backlog/.cache/worksets/items/&lt;item-id&gt;/), topics are stored under _kano/backlog/topics/&lt;topic&gt;/ so the deterministic brief.md can be shared/reviewed. Raw materials under materials/ are treated as cache.\n# Create a topic for related work\nkano-backlog topic create auth-refactor --agent kiro\n \n# Add items to the topic\nkano-backlog topic add auth-refactor --item TASK-0042\nkano-backlog topic add auth-refactor --item BUG-0012\n \n# Pin relevant documents\nkano-backlog topic pin auth-refactor --doc _kano/backlog/decisions/ADR-0015.md\n \n# Collect a code snippet reference (optional cached snapshot)\nkano-backlog topic add-snippet auth-refactor --file src/auth.py --start 10 --end 25 --agent kiro --snapshot\n \n# Distill deterministic brief.generated.md from collected materials\nkano-backlog topic distill auth-refactor\n \n# Generate a decision write-back audit report (writes to topic publish/)\nkano-backlog topic decision-audit auth-refactor --format plain\n \n# Switch active topic (per-agent pointer lives in cache)\nkano-backlog topic switch auth-refactor --agent kiro\n \n# Export context bundle\nkano-backlog topic export-context auth-refactor --format json\n \n# Close and later cleanup closed topics\nkano-backlog topic close auth-refactor --agent kiro\nkano-backlog topic cleanup --ttl-days 14\nkano-backlog topic cleanup --ttl-days 14 --apply\n \n# Write back a decision to a work item (appends to the item&#039;s Decisions section + Worklog)\nkano-backlog workitem add-decision KABSD-TSK-0001 \\\n  --decision &quot;Use X over Y because ...&quot; \\\n  --source &quot;_kano/backlog/topics/auth-refactor/synthesis/decision-notes.md&quot; \\\n  --agent kiro \\\n  --product &lt;my-product&gt;\nSee topic.md for complete documentation.\nSkill version\nShow the current skill version:\npython -c &quot;import pathlib; print((pathlib.Path(&#039;VERSION&#039;)).read_text().strip())&quot;\nExternal references\n\nAgent skills overview (Anthropic/Claude): platform.claude.com/docs/en/agents-and-tools/agent-skills/overview\nVersioning policy: VERSIONING.md\nRelease notes: CHANGELOG.md\n\nContributing\nPRs welcome, with one rule: don‚Äôt turn this into another Jira.\nThe point is to preserve decisions and acceptance, not to worship process.\nLicense\nMIT"},"skill/REFERENCE":{"title":"REFERENCE","links":[],"tags":[],"content":"Reference Index\nThe references/ folder is intentionally split into multiple small files so an agent (or a human) can load only what‚Äôs needed.\nFiles under references/\n\nschema.md: item types, states, naming rules, Ready gate\ntemplates.md: work item + ADR templates\nworkflow.md: SOP for planning/decisions/worklog\nviews.md: view patterns (Dataview + plain Markdown generators)\nbases.md: Obsidian Bases notes (plugin-free table-style views)\nlogging.md: audit log schema, redaction, rotation defaults\nprocesses.md: process profile schema and examples\nindexing.md: optional indexing layer (artifacts, config, rebuild workflow)\ncontext_graph.md: context graph + Graph-assisted retrieval (weak graph)\nindexing_schema.sql: optional DB index schema (SQLite-first)\nindexing_schema.json: DB schema description (machine-readable)\n\nKano CLI (automation surface)\nscripts/ now ships a single entrypoint (scripts/kano). Subcommands map 1:1 to the ops layer:\nCore commands\n\nkano doctor: verify Python prerequisites and backlog initialization\nkano backlog init: scaffold a product backlog (directories, _config/config.json, dashboards)\nkano view refresh: regenerate dashboards (Active/New/Done)\n\nItem operations\n\nkano item read|validate: inspect canonical records\nkano item create: create items\nkano item set-ready: set Ready-gate body sections (Context/Goal/Approach/Acceptance/Risks)\nkano item update-state: state transitions + worklog append + optional dashboard refresh\n\nState and worklog\n\nkano state transition: declarative workflow actions (start, ready, review, done, block, drop)\nkano worklog append: structured worklog writes with agent/model attribution\n\nBacklog administration (nested under backlog)\n\nkano backlog index build|refresh: build/refresh SQLite index from markdown items\nkano backlog demo seed: seed demo data (1 epic ‚Üí 1 feature ‚Üí 3 tasks) for testing\nkano backlog persona summary|report: generate persona activity summaries/reports\nkano backlog sandbox init: scaffold isolated sandbox environments for experimentation\nkano config show|validate|migrate-json|export|init: inspect/validate/migrate/export config (TOML-first; product.* required)\n\nNo new standalone scripts will be added under scripts/; all operations flow through the unified CLI.\nRelated (demo repo convention)\nIn the demo host repo, the backlog lives under _kano/backlog/:\n\nItems: _kano/backlog/items/**\nDecisions/ADRs: _kano/backlog/decisions/**\nViews: _kano/backlog/views/**\nTools: _kano/backlog/tools/**\nProduct configs: _kano/backlog/products/&lt;product&gt;/_config/config.toml (product.name, product.prefix)\n\nVersioning\n\nVersioning policy: VERSIONING.md (Git tags vX.Y.Z)\n\nTemplates (optional)\n\ntemplates/AGENTS.block.md: snippet for Codex-style agent instructions (append/update into repo-root AGENTS.md)\ntemplates/CLAUDE.block.md: snippet for Claude-style instructions (append/update into repo-root CLAUDE.md)\n"},"skill/SKILL":{"title":"SKILL","links":[],"tags":[],"content":"Kano Agent Backlog Skill (local-first)\nScope\nUse this skill to:\n\nPlan new work by creating backlog items before code changes.\nMaintain hierarchy and relationships via parent links, as defined by the active process profile.\nRecord decisions with ADRs and link them to items.\nKeep a durable, append-only worklog for project evolution.\n\nAgent compatibility: read the whole skill\n\nAlways load the entire SKILL.md before acting; some agent shells only fetch the first ~100 lines by default.\nIf your client truncates, fetch in chunks (e.g., lines 1-200, 200-400, ‚Ä¶) until you see the footer marker END_OF_SKILL_SENTINEL.\nIf you cannot confirm the footer marker, stop and ask for help; do not proceed with partial rules.\nWhen generating per-agent guides, preserve this read-all requirement so downstream agents stay in sync.\n\nNon-negotiables\n\nPlanning before coding: create/update items and meet the Ready gate before making code changes.\nWorklog is append-only; never rewrite history.\nUpdate Worklog whenever:\n\na discussion produces a clear decision or direction,\nan item state changes,\nscope/approach changes,\nor an ADR is created/linked.\n\n\nArchive by view: hide Done/Dropped items in views by default; do not move files unless explicitly requested.\nBacklog volume control:\n\nOnly create items for work that changes code or design decisions.\nAvoid new items for exploratory discussion; record in existing Worklog instead.\nKeep Tasks/Bugs sized for a single focused session.\nAvoid ADRs unless a real architectural trade-off is made.\n\n\nTicketing threshold (agent-decided):\n\nOpen a new Task/Bug when you will change code/docs/views/scripts.\nOpen an ADR (and link it) when a real trade-off or direction change is decided.\nOtherwise, record the discussion in an existing Worklog; ask if unsure.\n\n\nTicket type selection (keep it lightweight):\n\nEpic: multi-release or multi-team milestone spanning multiple Features.\nFeature: a new capability that delivers multiple UserStories.\nUserStory: a single user-facing outcome that requires multiple Tasks.\nTask: a single focused implementation or doc change (typically one session).\nExample: ‚ÄúEnd-to-end embedding pipeline‚Äù = Epic; ‚ÄúPluggable vector backend‚Äù = Feature; ‚ÄúMVP chunking pipeline‚Äù = UserStory; ‚ÄúImplement tokenizer adapter‚Äù = Task.\n\n\nBug vs Task triage (when fixing behavior):\n\nIf you are correcting a behavior that was previously marked Done and the behavior violates the original intent/acceptance (defect or regression), open a Bug and link it to the original item.\nIf the change is a new requirement/scope change beyond the original acceptance, open a Task/UserStory (or Feature) instead, and link it for traceability.\n\n\nBug origin tracing (when diagnosing a defect/regression):\n\nRecord when the issue started and the evidence path you used to determine it.\nPrefer VCS-backed evidence when available:\n\nlast-known-good revision (commit hash or tag)\nfirst-known-bad revision (commit hash or tag)\nsuspected introducing change(s) (commit hash) and why (e.g., git blame on specific lines)\n\n\nIf git history is unavailable (zip export, shallow clone, missing remote), explicitly record that limitation and what alternative evidence you used (e.g., release notes, timestamps, reproduction reports).\nKeep evidence lightweight: record commit hashes + 1‚Äì2 line summaries; avoid pasting large diffs into Worklog. Attach artifacts when needed.\nSuggested Worklog template:\n\nBug origin: last_good=&lt;sha|tag&gt;, first_bad=&lt;sha|tag&gt;, suspect=&lt;sha&gt; (reason: blame &lt;path&gt;:&lt;line&gt;), evidence=&lt;git log/blame/bisect|other&gt;\n\n\n\n\nState ownership: the agent decides when to move items to InProgress or Done; humans observe and can add context.\nState semantics: Proposed = needs discovery/confirmation; Planned = approved but not started; Ready gate applies before start.\nHierarchy is in frontmatter links, not folder nesting; avoid moving files to reflect scope changes.\nFilenames stay stable; use ASCII slugs.\nNever include secrets in backlog files or logs.\nLanguage: backlog and documentation content must be English-only (no CJK), to keep parsing and cross-agent collaboration deterministic.\nAgent Identity: In Worklog and audit logs, use your own identity (e.g., [agent=antigravity]), never copy [agent=codex] blindly.\nAlways provide an explicit --agent value for auditability (some commands currently default to cli, but do not rely on it).\nModel attribution (optional but preferred): provide --model &lt;name&gt; (or env KANO_AGENT_MODEL / KANO_MODEL) when it is known deterministically.\n\nDo not guess model names; if unknown, omit the [model=...] segment.\n\n\nAgent Identity Protocol: Supply --agent &lt;ID&gt; with your real product name (e.g., cursor, copilot, windsurf, antigravity).\n\nForbidden (Placeholders): auto, user, assistant, &lt;AGENT_NAME&gt;, $AGENT_NAME.\n\n\nFile operations for backlog/skill artifacts must go through the kano-backlog CLI\r\n(python skills/kano-agent-backlog-skill/scripts/kano-backlog &lt;command&gt;) so audit logs capture the action.\nSkill scripts only operate on paths under _kano/backlog/ or _kano/backlog_sandbox/;\r\nrefuse other paths.\nAfter modifying backlog items, refresh the plain Markdown views immediately using\r\npython skills/kano-agent-backlog-skill/scripts/kano-backlog view refresh --agent &lt;agent-id&gt; --backlog-root &lt;path&gt; so the dashboards stay current.\n\nPersona summaries/reports are available via python skills/kano-agent-backlog-skill/scripts/kano-backlog admin persona summary|report ....\n\n\npython skills/kano-agent-backlog-skill/scripts/kano-backlog workitem update-state ... auto-syncs parent states forward-only by default; use --no-sync-parent\r\nfor manual re-plans where parent state should stay put.\nAdd Obsidian [[wikilink]] references in the body (e.g., a ## Links section) so Graph/backlinks work; frontmatter alone does not create graph edges.\n\nAgent compatibility: read the whole skill\n\nAlways load the entire SKILL.md before acting; some agent shells only fetch the first ~100 lines by default.\nIf your client truncates, fetch in chunks (e.g., lines 1-200, 200-400, ‚Ä¶) until you see the footer marker END_OF_SKILL_SENTINEL.\nIf you cannot confirm the footer marker, stop and ask for help; do not proceed with partial rules.\nWhen generating per-agent guides, preserve this read-all requirement so downstream agents stay in sync.\n\nFirst-run bootstrap (prereqs + initialization)\nBefore using this skill in a repo, the agent must confirm:\n\nPython prerequisites are available (or install them), and\nthe backlog scaffold exists for the target product/root.\n\nIf the backlog structure is missing, propose the bootstrap commands and wait for user approval before writing files.\nDeveloper vs user mode (where to declare it)\n\nPreferred source of truth: product config in _kano/backlog/products/&lt;product&gt;/_config/config.json.\n\nmode.skill_developer: true when this repo actively develops the skill itself (this demo repo).\nmode.persona: optional string describing the primary human persona (e.g. developer, pm, qa), used only for human-facing summaries/views.\n\n\nSecondary: agent guide files (e.g., AGENTS.md / CLAUDE.md) can document expectations, but are agent-specific and not script-readable.\n\nSkill developer gate (architecture compliance)\nIf mode.skill_developer=true, before writing any skill code (in scripts/ or src/), you must:\n\nRead ADR-0013 (‚ÄúCodebase Architecture and Module Boundaries‚Äù) in the product decisions folder.\nFollow the folder rules defined in ADR-0013:\n\nscripts/ is executable-only: no reusable module code.\nsrc/ is import-only: core logic lives here, never executed directly.\nAll agent-callable operations go through scripts/kano-backlog CLI.\n\n\nPlace new code in the correct package:\n\nModels/config/errors ‚Üí src/kano_backlog_core/\nUse-cases (create/update/view) ‚Üí src/kano_backlog_ops/\nStorage backends ‚Üí src/kano_backlog_adapters/\nCLI commands ‚Üí src/kano_backlog_cli/commands/\n\n\n\nViolating these boundaries will be flagged in code review.\nPrerequisite install (Python)\nDetect:\n\nRun python skills/kano-agent-backlog-skill/scripts/kano-backlog doctor --format plain.\n\nIf packages are missing, install once (recommended):\n\nDefault: python -m pip install -e skills/kano-agent-backlog-skill\nSkill contributors: python -m pip install -e skills/kano-agent-backlog-skill[dev]\nOptional heavy dependencies (FAISS, sentence-transformers) should be installed manually per platform requirements before running the CLI against embedding features.\n\nBacklog initialization (file scaffold + config + dashboards)\nDetect (multi-product / platform layout):\n\nProduct initialized if _kano/backlog/products/&lt;product&gt;/_config/config.json exists (or confirm via python skills/kano-agent-backlog-skill/scripts/kano-backlog doctor --product &lt;product&gt;).\n\nBootstrap:\n\nRun python skills/kano-agent-backlog-skill/scripts/kano-backlog admin init --product &lt;product&gt; --agent &lt;agent-id&gt; [--backlog-root &lt;path&gt;] to scaffold _kano/backlog/products/&lt;product&gt;/ (items/, decisions/, views/, _config/, _meta/, _index/).\nThe init command derives a project prefix, writes _config/config.json, and refreshes dashboards so views exist immediately after initialization.\nManual fallback (only if automation is unavailable): follow _kano/backlog/README.md to copy the template scaffold, then refresh views via kano-backlog view refresh.\n\nOptional LLM analysis over deterministic reports\nThis skill can optionally append an LLM-generated analysis to a deterministic report.\r\nThe deterministic report is the SSOT; analysis is treated as a derived artifact.\n\nDeterministic report: views/Report_&lt;persona&gt;.md\nDerived LLM output: views/_analysis/Report_&lt;persona&gt;_LLM.md (gitignored by default)\nDeterministic prompt artifact: views/_analysis/Report_&lt;persona&gt;_analysis_prompt.md\n\nEnable by config (per product):\n\nanalysis.llm.enabled = true\n\nExecution:\n\nThe default workflow is: generate the deterministic report ‚Üí use it as SSOT ‚Üí fill in the analysis template.\n\nThe skill generates a deterministic prompt file to guide the analysis, and a derived markdown file with placeholder headings.\n\n\nOptional automation: when analysis.llm.enabled = true in config, view refresh generates views/snapshots/_analysis/Report_&lt;persona&gt;_analysis_prompt.md (deterministic prompt) and Report_&lt;persona&gt;_LLM.md (template or LLM output)\nNever pass API keys as CLI args; keep secrets in env vars to avoid leaking into audit logs.\n\nID prefix derivation\n\nSource of truth:\n\nProduct config: _kano/backlog/products/&lt;product&gt;/_config/config.toml (product.name, product.prefix), or\nRepo config (single-product): _kano/backlog/_config/config.toml (product.name, product.prefix).\n\n\nDerivation:\n\nSplit product.name on non-alphanumeric separators and camel-case boundaries.\nTake the first letter of each segment.\nIf only one letter, take the first letter plus the next consonant (A/E/I/O/U skipped).\nIf still short, use the first two letters.\nUppercase the result.\n\n\nExample: product.name=kano-agent-backlog-skill-demo ‚Üí KABSD.\n\nRecommended layout\nThis skill supports both single-product and multi-product layouts:\n\nSingle-product (repo-level): _kano/backlog/\nMulti-product (monorepo): _kano/backlog/products/&lt;product&gt;/\n\nWithin each backlog root:\n\n_meta/ (schema, conventions)\nitems/&lt;type&gt;/&lt;bucket&gt;/ (work items)\ndecisions/ (ADR files)\nviews/ (dashboards / generated Markdown)\n\nItem bucket folders (per 100)\n\nStore items under _kano/backlog/items/&lt;type&gt;/&lt;bucket&gt;/.\nBucket names use 4 digits for the lower bound of each 100 range.\n\nExample: 0000, 0100, 0200, 0300, ‚Ä¶\n\n\nExample path:\n\n_kano/backlog/items/task/0000/KABSD-TSK-0007_define-secret-provider-validation.md\n\n\n\nIndex/MOC files\n\nFor Epic, create an adjacent index file:\n\n&lt;ID&gt;_&lt;slug&gt;.index.md\n\n\nIndex files should render a tree using Dataview/DataviewJS and rely on parent links.\nTrack epic index files in _kano/backlog/_meta/indexes.md (type, item_id, index_file, updated, notes).\n\nReferences\n\nReference index: REFERENCE.md\nSchema and rules: references/schema.md\nTemplates: references/templates.md\nWorkflow SOP: references/workflow.md\nView patterns: references/views.md\nObsidian Bases (plugin-free): references/bases.md\nContext Graph + Graph-assisted retrieval: references/context_graph.md\n\nIf the backlog structure is missing, propose creation and wait for user approval before writing files.\nKano CLI entrypoints (current surface)\nscripts/ exposes a single executable: scripts/kano-backlog. The CLI is intentionally organized as nested command groups so agents can discover operations via --help on-demand (instead of hard-coding the full command surface into this skill).\nHelp-driven discovery (preferred)\nRun these in order, expanding only what you need:\n\npython skills/kano-agent-backlog-skill/scripts/kano-backlog --help\n\nShows top-level groups (e.g., backlog, item, state, worklog, view) and global options.\n\n\npython skills/kano-agent-backlog-skill/scripts/kano-backlog &lt;group&gt; --help\n\nShows subcommands for that group.\n\n\npython skills/kano-agent-backlog-skill/scripts/kano-backlog &lt;group&gt; &lt;command&gt; --help\n\nShows required args/options for that command.\n\n\n\nGuideline: do not paste large --help output into chat; inspect it locally and run the command.\nCanonical examples (keep these few memorized)\n\nBootstrap:\n\npython skills/kano-agent-backlog-skill/scripts/kano-backlog doctor --format plain\npython skills/kano-agent-backlog-skill/scripts/kano-backlog admin init --product &lt;name&gt; --agent &lt;id&gt;\n\n\nDaily workflow:\n\npython skills/kano-agent-backlog-skill/scripts/kano-backlog workitem create --type task --title &quot;...&quot; --agent &lt;id&gt; --product &lt;name&gt;\npython skills/kano-agent-backlog-skill/scripts/kano-backlog workitem set-ready &lt;item-id&gt; --context &quot;...&quot; --goal &quot;...&quot; --approach &quot;...&quot; --acceptance-criteria &quot;...&quot; --risks &quot;...&quot; --product &lt;name&gt;\npython skills/kano-agent-backlog-skill/scripts/kano-backlog workitem validate &lt;item-id&gt; --product &lt;name&gt;\npython skills/kano-agent-backlog-skill/scripts/kano-backlog workitem update-state &lt;item-ref&gt; --state InProgress --agent &lt;id&gt; --message &quot;...&quot; --product &lt;name&gt;\npython skills/kano-agent-backlog-skill/scripts/kano-backlog workitem attach-artifact &lt;item-id&gt; --path &lt;file&gt; --shared --agent &lt;id&gt; --product &lt;name&gt; [--note &quot;...&quot;]\npython skills/kano-agent-backlog-skill/scripts/kano-backlog view refresh --agent &lt;id&gt; --product &lt;name&gt;\n\n\nBacklog integrity checks:\n\npython skills/kano-agent-backlog-skill/scripts/kano-backlog admin validate uids --product &lt;name&gt;\n\n\n\nConflict handling policy (configurable)\nUse product config to control how duplicate IDs and UIDs are handled by maintenance commands\r\nsuch as admin links normalize-ids.\n\nConfig keys (product _config/config.toml):\n\nconflict_policy.id_conflict: default rename (rename duplicate IDs).\nconflict_policy.uid_conflict: default trash_shorter (move shorter duplicate content to _trash/).\n\n\ntrash_shorter uses _trash/&lt;YYYYMMDD&gt;/... under the product root; items get a Worklog entry.\n\nSandbox workflow (isolated experimentation)\nFor testing, prototyping, or demos without affecting production backlog:\n\nCreate: python skills/kano-agent-backlog-skill/scripts/kano-backlog admin sandbox init &lt;sandbox-name&gt; --product &lt;source-product&gt; --agent &lt;id&gt;\nUse: python skills/kano-agent-backlog-skill/scripts/kano-backlog workitem create --product &lt;sandbox-name&gt; ... (same CLI, different product)\nCleanup: rm -rf _kano/backlog_sandbox/&lt;sandbox-name&gt; (git will ignore this directory)\nRationale: Sandboxes mirror production structure but live in _kano/backlog_sandbox/, so changes never leak into _kano/backlog/.\n\nArtifacts policy (local-first)\n\nStorage locations:\n\nShared across products: _kano/backlog/_shared/artifacts/&lt;ITEM_ID&gt;/ (use --shared).\nProduct-local: _kano/backlog/products/&lt;product&gt;/artifacts/&lt;ITEM_ID&gt;/ (use --no-shared).\n\n\nUsage:\n\nAttach via workitem attach-artifact ‚Äî copies the file and appends a Worklog link.\nPrefer lightweight, text-first artifacts (Markdown, Mermaid, small images). Use Git LFS for large binaries if needed.\n\n\nGit policy:\n\nCommit human-readable artifacts that aid review. Avoid committing generated binaries unless justified.\nSandboxes under _kano/backlog_sandbox/ are gitignored; artifacts there are ephemeral.\nFor derived analysis, store under views/_analysis/ (gitignored by default), and keep deterministic reports in views/.\n\n\nLinking:\n\nThe CLI appends a Markdown link relative to the item file. Optionally add a ## Links section for richer context.\n\n\n\nState update helper\n\nUse python skills/kano-agent-backlog-skill/scripts/kano-backlog workitem update-state ... to update state + append Worklog.\nPrefer --action on kano-backlog state transition for the common transitions (start, ready, review, done, block, drop).\nUse python skills/kano-agent-backlog-skill/scripts/kano-backlog workitem validate &lt;item-id&gt; to check the Ready gate explicitly.\n\nTopic and Workset workflow (context management)\nWhen to use Topics\nTopics are shareable context buffers for multi-step work that spans multiple work items or requires exploratory research before creating formal backlog items.\nUse Topics when:\n\nExploring a complex problem that may result in multiple work items\nCollecting code snippets, logs, and materials across multiple sessions\nCollaborating across agents/sessions with a shared context\nRefactoring work that requires tracking multiple code locations\n\nTopic lifecycle:\n\nCreate: python skills/kano-agent-backlog-skill/scripts/kano-backlog topic create &lt;topic-name&gt; --agent &lt;id&gt;\n\nCreates _kano/backlog/topics/&lt;topic&gt;/ with manifest.json, brief.md, brief.generated.md, notes.md, and materials/ subdirectories\n\n\nCollect materials:\n\nAdd items: topic add &lt;topic-name&gt; --item &lt;ITEM_ID&gt;\nAdd code snippets: topic add-snippet &lt;topic-name&gt; --file &lt;path&gt; --start &lt;line&gt; --end &lt;line&gt; --agent &lt;id&gt;\nPin docs: topic pin &lt;topic-name&gt; --doc &lt;path&gt;\n\n\nDistill: python skills/kano-agent-backlog-skill/scripts/kano-backlog topic distill &lt;topic-name&gt;\n\n\nGenerates/overwrites deterministic brief.generated.md from collected materials\nbrief.md is a stable, human-maintained brief (do not overwrite it automatically)\n\n\nSwitch context: python skills/kano-agent-backlog-skill/scripts/kano-backlog topic switch &lt;topic-name&gt; --agent &lt;id&gt;\n\nSets active topic (affects config overlays and workset behavior)\n\n\nClose: python skills/kano-agent-backlog-skill/scripts/kano-backlog topic close &lt;topic-name&gt; --agent &lt;id&gt;\n\nMarks topic as closed; eligible for TTL cleanup\n\n\nCleanup: python skills/kano-agent-backlog-skill/scripts/kano-backlog topic cleanup --ttl-days &lt;N&gt; [--dry-run]\n\nRemoves raw materials from closed topics older than TTL\n\n\n\nTopic snapshots (retention policy):\n\nSnapshots are intended for milestone checkpoints (pre-merge/split/restore, risky bulk edits), not every small edit.\nTo prevent noise, keep only the latest snapshot per topic in this demo repo.\nAfter creating a snapshot (or periodically), prune all but the newest snapshot:\n\npython skills/kano-agent-backlog-skill/scripts/kano-backlog topic snapshot cleanup &lt;topic-name&gt; --ttl-days 0 --keep-latest 1 --apply\n\n\n\nTopic structure:\n_kano/backlog/topics/&lt;topic&gt;/\r\n  manifest.json          # refs to items/docs/snippets, status, timestamps\r\n  brief.md               # stable, human-maintained brief (do not overwrite automatically)\r\n  brief.generated.md     # deterministic distilled brief (generated/overwritten by `topic distill`)\r\n  notes.md               # freeform notes (backward compat)\r\n  materials/             # raw collection (gitignored by default)\r\n    clips/               # code snippet refs + cached text\r\n    links/               # urls / notes\r\n    extracts/            # extracted paragraphs\r\n    logs/                # build logs / command outputs\r\n  synthesis/             # intermediate drafts\r\n  publish/               # prepared write-backs (patches/ADRs)\r\n  config.toml            # optional topic-specific config overrides\n\nWhen to use Worksets\nWorksets are per-item working directories (cached, derived data) for a single backlog item.\nUse Worksets when:\n\nStarting work on a specific Task/Bug/UserStory\nNeed scratch space for deliverables (patches, test artifacts, etc.)\nWant item-specific config overrides (rare)\n\nWorkset lifecycle:\n\nInitialize: python skills/kano-agent-backlog-skill/scripts/kano-backlog workset init &lt;ITEM_ID&gt; --agent &lt;id&gt; [--ttl-hours &lt;N&gt;]\n\nCreates _kano/backlog/.cache/worksets/items/&lt;ITEM_ID&gt;/ with meta.json, plan.md, notes.md, deliverables/\n\n\nWork: Store scratch files in deliverables/ (patches, test outputs, etc.)\nRefresh: python skills/kano-agent-backlog-skill/scripts/kano-backlog workset refresh &lt;ITEM_ID&gt; --agent &lt;id&gt;\n\nUpdates refreshed_at timestamp\n\n\nCleanup: python skills/kano-agent-backlog-skill/scripts/kano-backlog workset cleanup --ttl-hours &lt;N&gt; [--dry-run]\n\nRemoves stale worksets older than TTL\n\n\n\nWorkset structure:\n_kano/backlog/.cache/worksets/items/&lt;ITEM_ID&gt;/\r\n  meta.json              # workset metadata (item_id, agent, timestamps, ttl)\r\n  plan.md                # execution plan template\r\n  notes.md               # work notes with Decision: marker guidance\r\n  deliverables/          # scratch outputs (patches, logs, test artifacts)\r\n  config.toml            # optional item-specific config overrides\n\nTopic vs Workset decision guide\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScenarioUse TopicUse WorksetExploring before creating items‚úÖ Yes‚ùå NoMulti-item refactor‚úÖ Yes‚ùå NoCollecting code snippets across files‚úÖ Yes‚ùå NoShared context for collaboration‚úÖ Yes‚ùå NoSingle item scratch space‚ùå No‚úÖ YesItem-specific deliverables‚ùå No‚úÖ YesVersion-controlled distillation‚úÖ Yes (brief.generated.md)‚ùå No\nBest practice: Start exploration in a Topic, create work items as scope clarifies, then use Worksets for individual item execution.\nActive topic and config overlays\n\nActive topic is per-agent: _kano/backlog/.cache/worksets/active_topic.&lt;agent&gt;.txt\nWhen an agent has an active topic, config resolution includes topic overrides:\n\nLayer order: defaults ‚Üí product ‚Üí topic ‚Üí workset ‚Üí runtime\nTopic config: _kano/backlog/topics/&lt;topic&gt;/config.toml\nUse for temporary overrides (e.g., switch default_product during exploration)\n\n\nGet active topic: python skills/kano-agent-backlog-skill/scripts/kano-backlog topic show --agent &lt;id&gt;\n\nMaterials buffer (Topic-specific)\n\nReference-first snippet collection: Avoid large copy-paste; store file+line+hash+optional snapshot\nSnippet refs include:\n\nfile: relative path from workspace root\nlines: [start, end] (1-based inclusive)\nhash: sha256:... of content for staleness check\ncached_text: optional snapshot (use --snapshot to include)\nrevision: git commit hash if available\n\n\n\nHuman decision materials vs. machine manifest\nDual-Readability Design: Every artifact checks against both human and agent readability:\n\nHuman-Readable: High-level summaries, clear checklists, ‚Äúmanager-friendly‚Äù reports for rapid decision-making\nAgent-Readable: Structural precision, file paths, line numbers, explicit markers for action without hallucination\n\nImplementation in Topics:\n\nTreat manifest.json as machine-oriented metadata:\n\nseed_items: UUID list for precise agent reference\nsnippet_refs: file+line+hash for deterministic loading\npinned_docs: absolute paths for unambiguous reference\n\n\nKeep brief.generated.md deterministic and tool-owned (generated/overwritten by topic distill):\n\nReadable item titles (e.g., ‚ÄúKABSD-TSK-0042: Implement tokenizer adapter‚Äù)\nIf available, include item path and keep UID in a hidden HTML comment for deterministic mapping\nMaterials index with items/docs/snippets sorted for repeatability\n\n\nKeep brief.md human-oriented and stable (do not overwrite automatically):\n\nContext summary and key decisions\nOptional: include a human-friendly materials list (do not duplicate raw snippet text)\n\n\nPut human-facing decision support in _kano/backlog/topics/&lt;topic&gt;/notes.md (and/or pinned docs), e.g.:\n\nDecision to make\nOptions + trade-offs\nEvidence (ADR links, snippet refs, benchmark/log artifacts)\nRecommendation + follow-ups\n\n\nStaleness detection: Compare current file hash with stored hash to detect if code changed\nDistillation: topic distill generates deterministic brief.generated.md with a repeatable materials index\n\n\nEND_OF_SKILL_SENTINEL"},"skill/VERSIONING":{"title":"VERSIONING","links":[],"tags":[],"content":"Versioning\nThis skill uses Git tags as the source of truth for released versions: vX.Y.Z.\nWhere to check the current version\n\nFile: VERSION (the intended version for the next release tag)\nCommand: python -c &quot;import pathlib; print((pathlib.Path(&#039;skills/kano-agent-backlog-skill&#039;) / &#039;VERSION&#039;).read_text().strip())&quot;\nRelease notes: CHANGELOG.md\n\nPre-1.0 policy\nWhile &lt; 1.0.0, we treat releases as milestones and iterate quickly, but we still follow a predictable rule:\n\n0.0.Z: patch / bugfix / non-breaking improvement\n0.Y.0: may include breaking changes (schema/CLI/layout), with migration notes\n\n1.0+ policy (SemVer)\nAfter 1.0.0 we follow SemVer strictly:\n\nX.Y.Z\n\nZ (patch): backward-compatible bugfix only\nY (minor): backward-compatible features + optional deprecations\nX (major): breaking changes (must include migration guidance)\n\n\n\nWhat counts as breaking\nNon-exhaustive examples:\n\nRenaming/removing required frontmatter keys, or changing the meaning of states/groups\nChanging the canonical backlog root layout (_kano/backlog/**) or bucket rules\nRemoving/renaming CLI flags, or changing defaults that alter deterministic outputs\nRenaming/removing config keys under _kano/backlog/_config/config.json\nChanging canonical dashboard filenames or their grouping semantics\n\nRelease checklist (minimum)\n\nDocs reflect current behavior (README*, REFERENCE.md, references/*)\nCanonical CLI commands run end-to-end:\n\npython skills/kano-agent-backlog-skill/scripts/kano-backlog view refresh --agent &lt;id&gt;\npython skills/kano-agent-backlog-skill/scripts/kano-backlog workitem update-state &lt;item&gt; --state Done --agent &lt;id&gt;\n\n\nDemo views are regenerated\n"},"skill/docs/index":{"title":"index","links":["skill/docs/topic","skill/docs/workset","skill/docs/tokenizer-quickstart","skill/docs/tokenizer-adapters","skill/docs/tokenizer-configuration","skill/docs/tokenizer-cli-reference","skill/docs/tokenizer-troubleshooting","skill/docs/tokenizer-performance","releases/"],"tags":[],"content":"Kano Agent Backlog Skill\nAI Agent Skills for Spec-Driven Agentic Programming | Local-first backlog | Multi-agent collaboration\nCore Documentation\n\nCore Concepts - Topics\nCore Concepts - Worksets\n\nTokenizer Adapters\n\nQuick Start Guide - Get started in 5 minutes\nTokenizer Adapters Overview - Complete user guide and reference\nConfiguration Reference - Detailed configuration options and examples\nCLI Reference - Complete command-line interface reference\nTroubleshooting Guide - Common issues and solutions\nPerformance Tuning - Optimization strategies and benchmarking\n\nReleases\n\nRelease Notes\n"},"skill/docs/releases/0.0.1":{"title":"0.0.1","links":[],"tags":[],"content":"0.0.1 ‚Äî Initial local-first backlog workflow (MVP)\nThis is the first public cut of the Kano Agent Backlog demo workspace: a local-first, file-based backlog that supports agent collaboration with an auditable trail.\nNo server runtime is included (by design).\nHighlights\n\nFile-first backlog as the system of record under _kano/backlog/ (items + decisions + views)\nAgent-friendly CLI to create/update items, enforce Ready-gate discipline, and keep views in sync\nTopics + Worksets to manage multi-session context and per-item scratch space\nDeterministic views (dashboards/reports) refreshable from canonical data\n\nWhat‚Äôs included\n\nWork items\n\nCreate/read and validate Ready-gate fields\nUpdate state transitions with append-only Worklog entries\n\n\nTopics\n\nCreate topics and attach items/snippets/materials for shared exploration context\n\n\nWorksets\n\nInitialize per-item working directories for deliverables and notes\n\n\nViews\n\nRefresh dashboards/reports after changes to keep the backlog readable\n\n\nAuditability\n\nAppend-only Worklog patterns and CLI-driven changes for traceability\nWorklog entries support [agent=...] and [model=...] attribution (defaults to unknown when not provided)\n\n\n\nGetting started\n\nInstall the skill (editable)\n\npython -m pip install -e skills/kano-agent-backlog-skill\n\n\nCheck environment health\n\npython skills/kano-agent-backlog-skill/scripts/kano-backlog doctor --format plain\n\n\nCreate a work item\n\npython skills/kano-agent-backlog-skill/scripts/kano-backlog workitem create --type task --title &quot;...&quot; --agent &lt;agent&gt; --product &lt;product&gt;\n\n\nRefresh views\n\npython skills/kano-agent-backlog-skill/scripts/kano-backlog view refresh --agent &lt;agent&gt; --product &lt;product&gt;\n\n\n\nKnown limitations (MVP)\n\nNo server / MCP / HTTP runtime (local-first only)\nVector backends / embeddings provider integrations are not implemented in this release\nSome workflows are intentionally minimal to keep the system deterministic and auditable\n\nNotes for agents\n\nPrefer running backlog operations through the kano-backlog CLI to preserve the audit trail.\nIf the model name is unknown, record unknown (recommended over guessing).\n"},"skill/docs/releases/0.0.2":{"title":"0.0.2","links":[],"tags":[],"content":"0.0.2 ‚Äî Topics system hardening + embedding pipeline foundations\nThis release focuses on two areas:\n\nTopic system enhancements to make multi-session, multi-agent context management reliable.\nEmbedding preprocessing + vector backend research foundations to enable local-first semantic retrieval work in subsequent releases.\n\nNo server runtime is included (by design).\nHighlights\n\nTopics: templates/archetypes for consistent topic scaffolding\nTopics: cross-references (related_topics) with bidirectional linking\nTopics: snapshots (create/list/restore/cleanup) for safe checkpoints\nTopics: merge/split operations with history preservation and dry-run mode\nEmbeddings: clarified requirements and decisions (cross-lingual retrieval; per-model indexes)\nEmbeddings: deterministic chunking/token-budget contracts and vector backend adapter contract captured in backlog items\n\nRelease focus topics (backlog evidence)\n\n_kano/backlog/topics/topic-system-enhancements/brief.md\n_kano/backlog/topics/embedding-preprocessing-and-vector-backend-research/brief.md\n\nKey related decisions:\n\n_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0009_local-first-embedding-search-architecture.md\n_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0035_cross-lingual-retrieval-requirement-and-default-embedding-policy.md\n_kano/backlog/products/kano-agent-backlog-skill/decisions/ADR-0036_index-strategy-shared-index-now-per-model-indexes-later-via-config.md\n\nKnown limitations\n\nNo server / MCP / HTTP runtime (local-first only)\nEmbedding providers and benchmarking are still evolving; expect iteration and config churn\nVector backend selection remains local-first and dependency-sensitive (Windows compatibility is a primary constraint)\n"},"skill/docs/token-aware-overlap-implementation":{"title":"token-aware-overlap-implementation","links":[],"tags":[],"content":"Token-Aware Overlap Calculation Implementation\nOverview\nThis document describes the implementation of task 2.4 from the tokenizer-adapters spec: ‚ÄúImplement token-aware overlap calculation‚Äù. The implementation enhances the chunking system to provide accurate token-based overlap calculation using tokenizer adapters.\nTask Requirements\nThe task required implementing:\n\nOverlap calculation in token space rather than character space\nEnsure overlap doesn‚Äôt exceed chunk size\nHandle edge cases (very short chunks, large overlap)\nValidate overlap consistency across chunks\n\nImplementation Details\nEnhanced _calculate_overlap_start Function\nThe core improvement is in the _calculate_overlap_start function in chunking.py:\ndef _calculate_overlap_start(\n    text: str,\n    chunk_end: int,\n    options: ChunkingOptions,\n    tokenizer: &quot;TokenizerAdapter&quot;,\n    previous_chunk_start: int = 0\n) -&gt; int:\nKey enhancements:\n\nToken-space calculation: Uses the tokenizer adapter to count tokens accurately\nChunk size validation: Calculates previous chunk size to limit overlap appropriately\nEdge case handling: Special logic for very short chunks (‚â§2 tokens)\nOverlap limiting: Ensures overlap doesn‚Äôt exceed half the chunk size\nBinary search optimization: Efficiently finds the optimal overlap position\n\nEnhanced chunk_text Function\nImproved the overlap calculation in the original chunk_text function:\n# Enhanced overlap calculation that respects token boundaries and chunk sizes\nif options.overlap_tokens &lt;= 0:\n    # No overlap requested\n    start_token = end_token\nelif chunk_len &lt;= options.overlap_tokens:\n    # Chunk is smaller than or equal to overlap - use minimal overlap\n    overlap_amount = max(0, chunk_len - 1)  # Leave at least 1 token of new content\n    start_token = max(end_token - overlap_amount, start_token + 1)\nelif chunk_len &lt;= 2:\n    # Very small chunk - no overlap to ensure progress\n    start_token = end_token\nelse:\n    # Normal case: apply overlap but limit to half the chunk size\n    max_overlap = min(options.overlap_tokens, chunk_len // 2)\n    start_token = end_token - max_overlap\nOverlap Consistency Validation\nAdded a comprehensive validation function:\ndef validate_overlap_consistency(\n    chunks: List[Chunk], \n    options: ChunkingOptions,\n    tokenizer: Optional[&quot;TokenizerAdapter&quot;] = None\n) -&gt; List[str]:\nThis function validates:\n\nOverlaps don‚Äôt exceed configured limits\nOverlaps don‚Äôt exceed chunk sizes\nAdjacent chunks have reasonable overlap\nNo chunks are completely contained within overlaps\n\nEdge Case Handling\nVery Short Chunks\n\nChunks with ‚â§2 tokens get minimal or no overlap\nEnsures forward progress is always maintained\nPrevents overlap from consuming entire chunks\n\nLarge Overlap Configuration\n\nOverlap is limited to half the chunk size\nPrevents overlap from dominating chunk content\nMaintains meaningful new content in each chunk\n\nTokenizer Errors\n\nGraceful fallback when tokenizer fails\nMaintains system stability\nLogs warnings for debugging\n\nTesting\nUnit Tests (test_token_aware_overlap.py)\nComprehensive test suite covering:\n\nBasic overlap calculation functionality\nEdge cases (very short chunks, large overlap)\nDifferent tokenizer types\nOverlap consistency validation\nError handling\n\nProperty-Based Tests (test_overlap_properties.py)\nImplements the correctness properties from the spec:\nProperty 1.4: Overlap Consistency\n@given(text=st.text(...), target_tokens=st.integers(...), ...)\ndef test_property_1_4_overlap_consistency(self, ...):\n    &quot;&quot;&quot;Property 1.4: Overlap tokens are correctly applied between adjacent chunks.\n    \n    **Validates: Requirements US-2, FR-3**\n    &quot;&quot;&quot;\nTests validate:\n\nOverlap is applied between adjacent chunks\nOverlap doesn‚Äôt exceed configured limits\nOverlap doesn‚Äôt exceed chunk sizes\nOverall consistency across all chunks\n\nIntegration with Existing Tests\nAll existing chunking tests continue to pass, ensuring backward compatibility.\nPerformance Considerations\nBinary Search Optimization\n\nUses binary search to find optimal overlap positions\nReduces tokenizer calls from O(n) to O(log n)\nMaintains good performance even with large chunks\n\nTokenizer Caching\n\nReuses tokenizer instances across chunks\nMinimizes initialization overhead\nSupports different tokenizer types efficiently\n\nConfiguration\nThe overlap calculation respects all existing ChunkingOptions:\n@dataclass(frozen=True)\nclass ChunkingOptions:\n    target_tokens: int = 256\n    max_tokens: int = 512\n    overlap_tokens: int = 32        # Used by enhanced overlap calculation\n    version: str = &quot;chunk-v1&quot;\n    tokenizer_adapter: str = &quot;auto&quot; # Selects tokenizer for overlap calculation\nUsage Examples\nBasic Usage with Tokenizer Adapter\nfrom kano_backlog_core.chunking import chunk_text_with_tokenizer, ChunkingOptions\nfrom kano_backlog_core.tokenizer import HeuristicTokenizer\n \noptions = ChunkingOptions(\n    target_tokens=256,\n    max_tokens=512,\n    overlap_tokens=50\n)\ntokenizer = HeuristicTokenizer(&quot;gpt-3.5-turbo&quot;)\n \nchunks = chunk_text_with_tokenizer(&quot;doc-id&quot;, text, options, tokenizer)\nValidation\nfrom kano_backlog_core.chunking import validate_overlap_consistency\n \nerrors = validate_overlap_consistency(chunks, options, tokenizer)\nif errors:\n    print(&quot;Overlap validation issues:&quot;, errors)\nBenefits\n\nAccurate Token Counting: Uses actual tokenizer adapters instead of heuristics\nConsistent Overlap: Ensures overlap is meaningful and consistent\nEdge Case Handling: Robust handling of unusual text and configurations\nPerformance: Efficient binary search algorithm\nValidation: Comprehensive validation for debugging and quality assurance\nBackward Compatibility: Existing code continues to work unchanged\n\nFuture Enhancements\nThe implementation provides a solid foundation for future improvements:\n\nSemantic Overlap: Could be enhanced to consider semantic boundaries\nAdaptive Overlap: Could adjust overlap based on content type\nMulti-Model Support: Already supports different tokenizer adapters\nCaching: Could add token count caching for repeated text segments\n\nConclusion\nThe token-aware overlap calculation implementation successfully addresses all task requirements:\n‚úÖ Overlap calculation in token space - Uses tokenizer adapters for accurate counting\n‚úÖ Ensure overlap doesn‚Äôt exceed chunk size - Limits overlap to half chunk size\n‚úÖ Handle edge cases - Special handling for short chunks and large overlaps\n‚úÖ Validate overlap consistency - Comprehensive validation function\nThe implementation is thoroughly tested with both unit tests and property-based tests, ensuring reliability and correctness across a wide range of inputs and configurations."},"skill/docs/tokenizer-adapters":{"title":"tokenizer-adapters","links":[],"tags":[],"content":"Tokenizer Adapters Documentation\nAccurate token counting for reliable chunking, cost estimation, and token budget management\nThe tokenizer adapter system provides pluggable, accurate token counting for different model providers (OpenAI, HuggingFace, local models) to support reliable chunking operations in the kano-agent-backlog-skill embedding pipeline.\nTable of Contents\n\nQuick Start\nUser Guide\nConfiguration Reference\nTroubleshooting Guide\nPerformance Tuning\nCLI Reference\nAdvanced Usage\n\nQuick Start\n1. Check System Status\n# Check overall tokenizer system health\nkano-backlog tokenizer status\n \n# Check which adapters are available\nkano-backlog tokenizer adapter-status\n \n# Check dependencies\nkano-backlog tokenizer dependencies\n2. Test Tokenization\n# Test with default settings\nkano-backlog tokenizer test --text &quot;This is a sample text for tokenization testing.&quot;\n \n# Test specific adapter\nkano-backlog tokenizer test --adapter tiktoken --model gpt-4\n \n# Compare adapters\nkano-backlog tokenizer compare &quot;Sample text to compare across different tokenizers&quot;\n3. Create Configuration\n# Create example configuration file\nkano-backlog tokenizer create-example --output tokenizer_config.toml\n \n# Validate configuration\nkano-backlog tokenizer validate --config tokenizer_config.toml\nUser Guide\nUnderstanding Tokenizer Adapters\nTokenizer adapters provide accurate token counting for different model providers. Each adapter has different characteristics:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdapterAccuracySpeedDependenciesBest ForHeuristic~85-90%Very FastNoneDevelopment, fallbackTikToken100%FasttiktokenOpenAI modelsHuggingFace100%MediumtransformersHF models\nChoosing the Right Adapter\nFor OpenAI Models (GPT, text-embedding-*)\n# Recommended: TikToken for exact tokenization\nkano-backlog tokenizer recommend gpt-4\nkano-backlog tokenizer recommend text-embedding-3-small\nUse TikToken when:\n\nWorking with OpenAI models (GPT-3.5, GPT-4, text-embedding-*)\nNeed exact token counts for cost estimation\nHave tiktoken dependency available\n\nFor HuggingFace Models (BERT, sentence-transformers, etc.)\n# Recommended: HuggingFace for exact tokenization\nkano-backlog tokenizer recommend sentence-transformers/all-MiniLM-L6-v2\nkano-backlog tokenizer recommend bert-base-uncased\nUse HuggingFace when:\n\nWorking with transformer models from HuggingFace\nNeed exact tokenization for embedding models\nHave transformers dependency available\n\nFor Development and Fallback\n# Heuristic adapter works with any model\nkano-backlog tokenizer test --adapter heuristic --model any-model-name\nUse Heuristic when:\n\nDependencies not available\nNeed fast approximation for development\nWorking with unknown/custom models\n\nAdapter Selection Strategy\nThe system uses an intelligent fallback chain:\n\nPrimary Choice: Your configured adapter\nFallback Chain: tiktoken ‚Üí huggingface ‚Üí heuristic\nGraceful Degradation: Clear error messages and recovery suggestions\n\n# Check what adapter will be used\nkano-backlog tokenizer diagnose --model your-model-name\n \n# Test fallback behavior\nkano-backlog tokenizer test --adapter nonexistent-adapter\nModel Support\nOpenAI Models\n# List supported OpenAI models\nkano-backlog tokenizer list-models --adapter tiktoken\nSupported models include:\n\nGPT models: gpt-4, gpt-4-turbo, gpt-3.5-turbo, etc.\nEmbedding models: text-embedding-3-small, text-embedding-3-large, text-embedding-ada-002\nLegacy models: text-davinci-003, code-davinci-002\n\nHuggingFace Models\n# List supported HuggingFace models\nkano-backlog tokenizer list-models --adapter huggingface\nSupported model families:\n\nsentence-transformers: sentence-transformers/all-MiniLM-L6-v2, sentence-transformers/all-mpnet-base-v2\nBERT family: bert-base-uncased, distilbert-base-uncased\nRoBERTa family: roberta-base, distilroberta-base\nOther transformers: t5-base, facebook/bart-base\n\nIntegration with Embedding Pipeline\nThe tokenizer adapters integrate seamlessly with the embedding pipeline:\n# Use specific tokenizer in embedding commands\nkano-backlog embedding build --tokenizer-adapter tiktoken --tokenizer-model gpt-4\n \n# Configure via environment variables\nexport KANO_TOKENIZER_ADAPTER=huggingface\nexport KANO_TOKENIZER_MODEL=sentence-transformers/all-MiniLM-L6-v2\nkano-backlog embedding build\nConfiguration Reference\nConfiguration File Format\nTokenizer configuration uses TOML format with environment variable overrides:\n# tokenizer_config.toml\n[tokenizer]\n# Primary adapter (&quot;auto&quot; for fallback chain, or specific: &quot;heuristic&quot;, &quot;tiktoken&quot;, &quot;huggingface&quot;)\nadapter = &quot;auto&quot;\n \n# Model name (affects token limits and encoding selection)\nmodel = &quot;text-embedding-3-small&quot;\n \n# Optional: Override max tokens (uses model defaults if not specified)\nmax_tokens = 8192\n \n# Fallback chain when primary adapter fails\nfallback_chain = [&quot;tiktoken&quot;, &quot;huggingface&quot;, &quot;heuristic&quot;]\n \n# Heuristic adapter configuration\n[tokenizer.heuristic]\nchars_per_token = 4.0  # Characters per token estimation\n \n# TikToken adapter configuration\n[tokenizer.tiktoken]\n# encoding = &quot;cl100k_base&quot;  # Optional: specific encoding\n \n# HuggingFace adapter configuration\n[tokenizer.huggingface]\nuse_fast = true           # Use fast tokenizer when available\ntrust_remote_code = false # Security: don&#039;t execute remote code\nConfiguration Options\nMain Settings\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptionTypeDefaultDescriptionadapterstring&quot;auto&quot;Primary adapter name or ‚Äúauto‚Äù for fallbackmodelstring&quot;text-embedding-3-small&quot;Model name for tokenizationmax_tokensintegernullOverride model‚Äôs max token limitfallback_chainarray[&quot;tiktoken&quot;, &quot;huggingface&quot;, &quot;heuristic&quot;]Adapter fallback order\nHeuristic Adapter Options\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptionTypeDefaultDescriptionchars_per_tokenfloat4.0Average characters per token\nLanguage-Aware Estimation: The heuristic adapter automatically adjusts for different text types:\n\nCJK Text: ~1.2 chars/token (Chinese, Japanese, Korean)\nMixed Text: Blended ratio based on CJK content\nASCII/Latin: Uses configured chars_per_token ratio\n\nTikToken Adapter Options\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptionTypeDefaultDescriptionencodingstringnullSpecific encoding name (auto-detected if not set)\nSupported Encodings:\n\ncl100k_base: GPT-4, GPT-3.5-turbo, text-embedding-3-*\np50k_base: text-davinci-003, code-davinci-002\n\nHuggingFace Adapter Options\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptionTypeDefaultDescriptionuse_fastbooleantrueUse fast tokenizer implementationtrust_remote_codebooleanfalseAllow remote code execution (security risk)\nEnvironment Variable Overrides\nOverride any configuration setting using environment variables:\n# Main settings\nexport KANO_TOKENIZER_ADAPTER=tiktoken\nexport KANO_TOKENIZER_MODEL=gpt-4\nexport KANO_TOKENIZER_MAX_TOKENS=8192\n \n# Heuristic settings\nexport KANO_TOKENIZER_HEURISTIC_CHARS_PER_TOKEN=3.5\n \n# TikToken settings\nexport KANO_TOKENIZER_TIKTOKEN_ENCODING=p50k_base\n \n# HuggingFace settings\nexport KANO_TOKENIZER_HUGGINGFACE_USE_FAST=false\nexport KANO_TOKENIZER_HUGGINGFACE_TRUST_REMOTE_CODE=false\nView all available environment variables:\nkano-backlog tokenizer env\nConfiguration Examples\nDevelopment Setup (Fast, No Dependencies)\n[tokenizer]\nadapter = &quot;heuristic&quot;\nmodel = &quot;development-model&quot;\n \n[tokenizer.heuristic]\nchars_per_token = 4.0\nProduction OpenAI Setup\n[tokenizer]\nadapter = &quot;tiktoken&quot;\nmodel = &quot;text-embedding-3-small&quot;\nfallback_chain = [&quot;tiktoken&quot;, &quot;heuristic&quot;]\n \n[tokenizer.tiktoken]\nencoding = &quot;cl100k_base&quot;\nProduction HuggingFace Setup\n[tokenizer]\nadapter = &quot;huggingface&quot;\nmodel = &quot;sentence-transformers/all-MiniLM-L6-v2&quot;\nfallback_chain = [&quot;huggingface&quot;, &quot;heuristic&quot;]\n \n[tokenizer.huggingface]\nuse_fast = true\ntrust_remote_code = false\nMulti-Environment Setup\n[tokenizer]\nadapter = &quot;auto&quot;  # Use fallback chain\nmodel = &quot;text-embedding-3-small&quot;\nfallback_chain = [&quot;tiktoken&quot;, &quot;huggingface&quot;, &quot;heuristic&quot;]\n \n# All adapter configurations available\n[tokenizer.heuristic]\nchars_per_token = 4.0\n \n[tokenizer.tiktoken]\n# encoding auto-detected\n \n[tokenizer.huggingface]\nuse_fast = true\ntrust_remote_code = false\nConfiguration Management\n# Create example configuration\nkano-backlog tokenizer create-example --output my_config.toml\n \n# Validate configuration\nkano-backlog tokenizer validate --config my_config.toml\n \n# Show current configuration (with environment overrides)\nkano-backlog tokenizer config --config my_config.toml --format json\n \n# Migrate old configuration format\nkano-backlog tokenizer migrate old_config.json --output new_config.toml\nTroubleshooting Guide\nCommon Issues and Solutions\n1. ‚ÄúNo tokenizer adapter available‚Äù Error\nSymptoms:\nFallbackChainExhaustedError: No tokenizer available. Errors: [tiktoken: No module named &#039;tiktoken&#039;, huggingface: No module named &#039;transformers&#039;, heuristic: &lt;some error&gt;]\n\nSolutions:\nOption A: Install Dependencies\n# For OpenAI models\npip install tiktoken\n \n# For HuggingFace models\npip install transformers\n \n# Check installation\nkano-backlog tokenizer dependencies\nOption B: Use Heuristic Adapter\n# Force heuristic adapter\nexport KANO_TOKENIZER_ADAPTER=heuristic\nkano-backlog tokenizer test\n \n# Or in configuration\n[tokenizer]\nadapter = &quot;heuristic&quot;\nOption C: Check Installation Guide\nkano-backlog tokenizer install-guide\n2. TikToken Import Errors\nSymptoms:\nImportError: tiktoken package required for TiktokenAdapter. Install with: pip install tiktoken\n\nSolutions:\nInstall TikToken:\npip install tiktoken\n \n# Verify installation\npython -c &quot;import tiktoken; print(&#039;TikToken available&#039;)&quot;\n \n# Test adapter\nkano-backlog tokenizer health tiktoken\nCheck Python Environment:\n# Ensure you&#039;re in the right environment\nwhich python\npip list | grep tiktoken\n \n# If using conda\nconda install tiktoken -c conda-forge\nFallback to Heuristic:\nexport KANO_TOKENIZER_ADAPTER=heuristic\n3. HuggingFace Transformers Issues\nSymptoms:\nImportError: transformers package required for HuggingFaceAdapter\n\nor\nOSError: Can&#039;t load tokenizer for &#039;model-name&#039;. Make sure that &#039;model-name&#039; is a correct model identifier\n\nSolutions:\nInstall Transformers:\npip install transformers\n \n# For sentence-transformers models\npip install sentence-transformers\n \n# Verify installation\npython -c &quot;import transformers; print(&#039;Transformers available&#039;)&quot;\nCheck Model Name:\n# List supported models\nkano-backlog tokenizer list-models --adapter huggingface\n \n# Test with known model\nkano-backlog tokenizer test --adapter huggingface --model bert-base-uncased\nNetwork/Cache Issues:\n# Clear HuggingFace cache\nrm -rf ~/.cache/huggingface/\n \n# Use offline mode if needed\nexport TRANSFORMERS_OFFLINE=1\n4. Configuration Validation Errors\nSymptoms:\nConfigError: heuristic.chars_per_token must be a positive number\n\nSolutions:\nFix Configuration Values:\n[tokenizer.heuristic]\nchars_per_token = 4.0  # Must be positive number\n \n[tokenizer.huggingface]\nuse_fast = true        # Must be boolean\ntrust_remote_code = false  # Must be boolean\nValidate Configuration:\nkano-backlog tokenizer validate --config your_config.toml\nCheck Environment Variables:\n# Ensure environment variables are valid\nexport KANO_TOKENIZER_HEURISTIC_CHARS_PER_TOKEN=4.0  # Not &quot;4.0.0&quot;\nexport KANO_TOKENIZER_HUGGINGFACE_USE_FAST=true      # Not &quot;True&quot;\n5. Token Count Inconsistencies\nSymptoms:\n\nDifferent token counts between adapters\nUnexpected chunking behavior\nBudget overruns\n\nSolutions:\nCompare Adapters:\n# Compare token counts across adapters\nkano-backlog tokenizer compare &quot;Your sample text here&quot;\n \n# Benchmark adapters\nkano-backlog tokenizer benchmark --text &quot;Your sample text&quot; --iterations 5\nCheck Model Configuration:\n# Verify model settings\nkano-backlog tokenizer diagnose --model your-model-name\n \n# Check model max tokens\nkano-backlog tokenizer list-models | grep your-model\nUse Exact Adapters:\n# For OpenAI models, use tiktoken\nexport KANO_TOKENIZER_ADAPTER=tiktoken\n \n# For HuggingFace models, use huggingface\nexport KANO_TOKENIZER_ADAPTER=huggingface\n6. Performance Issues\nSymptoms:\n\nSlow tokenization\nHigh memory usage\nTimeouts\n\nSolutions:\nUse Faster Adapters:\n# Heuristic is fastest\nexport KANO_TOKENIZER_ADAPTER=heuristic\n \n# Enable fast tokenizers for HuggingFace\nexport KANO_TOKENIZER_HUGGINGFACE_USE_FAST=true\nBenchmark Performance:\nkano-backlog tokenizer benchmark --adapters heuristic,tiktoken --iterations 10\nCheck System Resources:\n# Monitor during tokenization\nkano-backlog tokenizer test --text &quot;$(cat large_file.txt)&quot; &amp;\ntop -p $!\nDiagnostic Commands\nSystem Health Check\n# Comprehensive status\nkano-backlog tokenizer status --verbose\n \n# Check specific adapter\nkano-backlog tokenizer health tiktoken\n \n# Dependency report\nkano-backlog tokenizer dependencies --verbose\nConfiguration Debugging\n# Show effective configuration\nkano-backlog tokenizer config --format json\n \n# Validate configuration\nkano-backlog tokenizer validate --config your_config.toml\n \n# Test configuration\nkano-backlog tokenizer test --config your_config.toml\nPerformance Analysis\n# Benchmark all adapters\nkano-backlog tokenizer benchmark\n \n# Compare specific adapters\nkano-backlog tokenizer compare &quot;test text&quot; --adapters tiktoken,heuristic\n \n# Profile memory usage\nkano-backlog tokenizer benchmark --text &quot;$(cat large_file.txt)&quot; --format json\nGetting Help\nBuilt-in Help\n# Command help\nkano-backlog tokenizer --help\nkano-backlog tokenizer test --help\n \n# Environment variables\nkano-backlog tokenizer env\n \n# Installation guide\nkano-backlog tokenizer install-guide\nDiagnostic Information\n# System information for bug reports\nkano-backlog tokenizer status --format json &gt; tokenizer_status.json\n \n# Dependency information\nkano-backlog tokenizer dependencies --verbose &gt; dependencies.txt\n \n# Configuration dump\nkano-backlog tokenizer config --format json &gt; current_config.json\nPerformance Tuning\nPerformance Characteristics\nUnderstanding the performance trade-offs helps choose the right adapter:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdapterSpeedMemoryAccuracyStartup TimeHeuristic‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠êTikToken‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠êHuggingFace‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\nBenchmarking Your Setup\nBasic Performance Test\n# Test with your typical text\nkano-backlog tokenizer benchmark --text &quot;Your typical document content here&quot;\n \n# Test with different text sizes\nkano-backlog tokenizer benchmark --text &quot;$(head -c 1000 large_file.txt)&quot;   # 1KB\nkano-backlog tokenizer benchmark --text &quot;$(head -c 10000 large_file.txt)&quot;  # 10KB\nkano-backlog tokenizer benchmark --text &quot;$(head -c 100000 large_file.txt)&quot; # 100KB\nComprehensive Benchmark\n# Benchmark all available adapters\nkano-backlog tokenizer benchmark --iterations 20 --format json &gt; benchmark_results.json\n \n# Compare specific adapters\nkano-backlog tokenizer benchmark --adapters tiktoken,heuristic --iterations 50\nMemory Profiling\n# Monitor memory usage during tokenization\nkano-backlog tokenizer benchmark --text &quot;$(cat very_large_file.txt)&quot; &amp;\nPID=$!\nwhile kill -0 $PID 2&gt;/dev/null; do\n    ps -o pid,vsz,rss,comm -p $PID\n    sleep 1\ndone\nOptimization Strategies\n1. Adapter Selection for Performance\nFor Maximum Speed:\n[tokenizer]\nadapter = &quot;heuristic&quot;\nfallback_chain = [&quot;heuristic&quot;]\n \n[tokenizer.heuristic]\nchars_per_token = 4.0  # Tune based on your content\nFor Balanced Performance:\n[tokenizer]\nadapter = &quot;tiktoken&quot;\nfallback_chain = [&quot;tiktoken&quot;, &quot;heuristic&quot;]\nFor Maximum Accuracy:\n[tokenizer]\nadapter = &quot;auto&quot;  # Use exact adapters when available\nfallback_chain = [&quot;tiktoken&quot;, &quot;huggingface&quot;, &quot;heuristic&quot;]\n2. HuggingFace Optimization\nEnable Fast Tokenizers:\n[tokenizer.huggingface]\nuse_fast = true  # Significant speed improvement\ntrust_remote_code = false\nModel Selection:\n# Prefer smaller, faster models when possible\n# Instead of: sentence-transformers/all-mpnet-base-v2\n# Use: sentence-transformers/all-MiniLM-L6-v2\n3. Heuristic Tuning\nOptimize chars_per_token for Your Content:\n# Test different ratios\nfor ratio in 3.0 3.5 4.0 4.5 5.0; do\n    export KANO_TOKENIZER_HEURISTIC_CHARS_PER_TOKEN=$ratio\n    echo &quot;Ratio $ratio:&quot;\n    kano-backlog tokenizer compare &quot;Your typical content&quot; --adapters heuristic,tiktoken\ndone\nLanguage-Specific Tuning:\n[tokenizer.heuristic]\n# For English technical content\nchars_per_token = 4.2\n \n# For mixed English/code content\nchars_per_token = 3.8\n \n# For documentation with lots of punctuation\nchars_per_token = 4.5\n4. Environment Optimization\nPython Environment:\n# Use Python 3.11+ for better performance\npython --version\n \n# Ensure packages are up to date\npip install --upgrade tiktoken transformers\nSystem Resources:\n# Monitor resource usage\nkano-backlog tokenizer benchmark --verbose\n \n# For large documents, ensure adequate memory\nfree -h\nPerformance Monitoring\nBuilt-in Metrics\n# Get performance statistics\nkano-backlog tokenizer benchmark --format json | jq &#039;.results[] | {adapter, avg_time_ms, avg_tokens}&#039;\n \n# Monitor consistency\nkano-backlog tokenizer benchmark --iterations 100 | grep &quot;Consistent&quot;\nCustom Monitoring\n# Example: Monitor tokenization performance in your application\nimport time\nfrom kano_backlog_core.tokenizer import get_default_registry\n \nregistry = get_default_registry()\nadapter = registry.resolve(&quot;tiktoken&quot;, model_name=&quot;gpt-4&quot;)\n \nstart_time = time.perf_counter()\nresult = adapter.count_tokens(your_text)\nend_time = time.perf_counter()\n \nprint(f&quot;Tokenization took {(end_time - start_time) * 1000:.2f}ms&quot;)\nprint(f&quot;Token count: {result.count}&quot;)\nprint(f&quot;Tokens per second: {result.count / (end_time - start_time):.0f}&quot;)\nProduction Recommendations\nConfiguration for Production\n[tokenizer]\n# Use exact adapters with heuristic fallback\nadapter = &quot;auto&quot;\nfallback_chain = [&quot;tiktoken&quot;, &quot;huggingface&quot;, &quot;heuristic&quot;]\n \n# Set appropriate model\nmodel = &quot;text-embedding-3-small&quot;  # Or your production model\n \n[tokenizer.heuristic]\n# Tune based on your content analysis\nchars_per_token = 4.0\n \n[tokenizer.tiktoken]\n# Let encoding be auto-detected\n \n[tokenizer.huggingface]\nuse_fast = true\ntrust_remote_code = false  # Security best practice\nMonitoring in Production\n# Regular health checks\nkano-backlog tokenizer status --format json\n \n# Performance monitoring\nkano-backlog tokenizer benchmark --adapters auto --iterations 10\n \n# Dependency monitoring\nkano-backlog tokenizer dependencies --format json\nScaling Considerations\nFor High-Volume Processing:\n\nUse TikToken for OpenAI models (fastest exact adapter)\nEnable fast tokenizers for HuggingFace models\nConsider heuristic adapter for non-critical paths\nMonitor memory usage with large documents\n\nFor Multi-Model Environments:\n\nUse ‚Äúauto‚Äù adapter with comprehensive fallback chain\nMonitor adapter usage statistics\nOptimize fallback chain based on your model distribution\n\nFor Resource-Constrained Environments:\n\nUse heuristic adapter primarily\nTune chars_per_token based on accuracy requirements\nMinimize dependency footprint\n\nCLI Reference\nCore Commands\nkano-backlog tokenizer status\nShow comprehensive system status including configuration, adapters, dependencies, and health.\nkano-backlog tokenizer status [OPTIONS]\n \nOptions:\n  --config PATH     Configuration file path\n  --verbose         Show detailed information\n  --format FORMAT   Output format (markdown, json)\nkano-backlog tokenizer test\nTest tokenizer adapters with sample text.\nkano-backlog tokenizer test [OPTIONS]\n \nOptions:\n  --config PATH     Configuration file path\n  --text TEXT       Text to tokenize (default: sample text)\n  --adapter NAME    Specific adapter to test\n  --model NAME      Model name to use\nkano-backlog tokenizer compare\nCompare tokenization results across different adapters.\nkano-backlog tokenizer compare TEXT [OPTIONS]\n \nArguments:\n  TEXT              Text to tokenize and compare\n \nOptions:\n  --adapters LIST   Comma-separated adapter names\n  --model NAME      Model name to use\n  --show-tokens     Show token breakdown (when supported)\nConfiguration Commands\nkano-backlog tokenizer config\nShow current configuration with environment overrides.\nkano-backlog tokenizer config [OPTIONS]\n \nOptions:\n  --config PATH     Configuration file path\n  --format FORMAT   Output format (json, toml, yaml)\nkano-backlog tokenizer validate\nValidate tokenizer configuration.\nkano-backlog tokenizer validate [OPTIONS]\n \nOptions:\n  --config PATH     Configuration file path\nkano-backlog tokenizer create-example\nCreate example configuration file.\nkano-backlog tokenizer create-example [OPTIONS]\n \nOptions:\n  --output PATH     Output file path\n  --force           Overwrite existing file\nkano-backlog tokenizer migrate\nMigrate configuration from old format to TOML.\nkano-backlog tokenizer migrate INPUT_PATH [OPTIONS]\n \nArguments:\n  INPUT_PATH        Input configuration file\n \nOptions:\n  --output PATH     Output TOML file path\n  --force           Overwrite existing output\nDiagnostic Commands\nkano-backlog tokenizer diagnose\nRun comprehensive tokenizer diagnostics.\nkano-backlog tokenizer diagnose [OPTIONS]\n \nOptions:\n  --config PATH     Configuration file path\n  --model NAME      Specific model to diagnose\n  --verbose         Show detailed diagnostic information\nkano-backlog tokenizer health\nCheck health of specific tokenizer adapter.\nkano-backlog tokenizer health ADAPTER [OPTIONS]\n \nArguments:\n  ADAPTER           Adapter name (heuristic, tiktoken, huggingface)\n \nOptions:\n  --model NAME      Model name to test with\nkano-backlog tokenizer dependencies\nCheck status of tokenizer dependencies.\nkano-backlog tokenizer dependencies [OPTIONS]\n \nOptions:\n  --verbose         Show detailed dependency information\n  --refresh         Force refresh of dependency cache\nkano-backlog tokenizer adapter-status\nShow status of tokenizer adapters.\nkano-backlog tokenizer adapter-status [OPTIONS]\n \nOptions:\n  --adapter NAME    Show status for specific adapter only\nPerformance Commands\nkano-backlog tokenizer benchmark\nBenchmark tokenizer adapter performance.\nkano-backlog tokenizer benchmark [OPTIONS]\n \nOptions:\n  --text TEXT       Text for benchmarking\n  --iterations N    Number of test iterations\n  --adapters LIST   Comma-separated adapter names\n  --model NAME      Model name to use\n  --format FORMAT   Output format (markdown, json, csv)\nUtility Commands\nkano-backlog tokenizer env\nShow available environment variables.\nkano-backlog tokenizer env\nkano-backlog tokenizer install-guide\nShow installation guide for missing dependencies.\nkano-backlog tokenizer install-guide\nkano-backlog tokenizer list-models\nList supported models and token limits.\nkano-backlog tokenizer list-models [OPTIONS]\n \nOptions:\n  --adapter NAME    Show models for specific adapter\n  --format FORMAT   Output format (markdown, json, csv)\nkano-backlog tokenizer recommend\nGet adapter recommendation for specific model.\nkano-backlog tokenizer recommend MODEL [OPTIONS]\n \nArguments:\n  MODEL             Model name\n \nOptions:\n  --requirements    Requirements (e.g., &#039;accuracy=high,speed=medium&#039;)\nAdvanced Usage\nCustom Integration\nUsing Tokenizer Adapters in Python Code\nfrom kano_backlog_core.tokenizer import get_default_registry\nfrom kano_backlog_core.tokenizer_config import load_tokenizer_config\n \n# Load configuration\nconfig = load_tokenizer_config()\n \n# Get registry and set fallback chain\nregistry = get_default_registry()\nregistry.set_fallback_chain(config.fallback_chain)\n \n# Resolve adapter\nadapter = registry.resolve(\n    adapter_name=config.adapter,\n    model_name=config.model,\n    max_tokens=config.max_tokens,\n    **config.get_adapter_options(config.adapter)\n)\n \n# Count tokens\nresult = adapter.count_tokens(&quot;Your text here&quot;)\nprint(f&quot;Token count: {result.count}&quot;)\nprint(f&quot;Method: {result.method}&quot;)\nprint(f&quot;Is exact: {result.is_exact}&quot;)\nCustom Adapter Implementation\nfrom kano_backlog_core.tokenizer import TokenizerAdapter, TokenCount\n \nclass CustomTokenizerAdapter(TokenizerAdapter):\n    &quot;&quot;&quot;Custom tokenizer adapter implementation.&quot;&quot;&quot;\n    \n    def __init__(self, model_name: str, max_tokens: Optional[int] = None, **kwargs):\n        super().__init__(model_name, max_tokens)\n        # Initialize your custom tokenizer\n        \n    @property\n    def adapter_id(self) -&gt; str:\n        return &quot;custom&quot;\n    \n    def count_tokens(self, text: str) -&gt; TokenCount:\n        # Implement your tokenization logic\n        token_count = your_tokenization_function(text)\n        \n        return TokenCount(\n            count=token_count,\n            method=&quot;custom&quot;,\n            tokenizer_id=f&quot;custom:{self.model_name}&quot;,\n            is_exact=True,  # or False for approximations\n            model_max_tokens=self.max_tokens()\n        )\n    \n    def max_tokens(self) -&gt; int:\n        # Return max tokens for your model\n        return self._max_tokens or 8192\n \n# Register custom adapter\nregistry = get_default_registry()\nregistry.register(&quot;custom&quot;, CustomTokenizerAdapter, custom_param=&quot;value&quot;)\nIntegration with Chunking Pipeline\nThe tokenizer adapters integrate with the chunking system for accurate token-aware document processing:\nfrom kano_backlog_core.chunking import ChunkingOptions, chunk_text\nfrom kano_backlog_core.tokenizer_config import load_tokenizer_config\n \n# Load tokenizer configuration\ntokenizer_config = load_tokenizer_config()\n \n# Configure chunking with tokenizer settings\nchunking_options = ChunkingOptions(\n    target_tokens=512,\n    max_tokens=1024,\n    overlap_tokens=50,\n    safety_margin=0.1,\n    tokenizer_adapter=tokenizer_config.adapter,\n    tokenizer_model=tokenizer_config.model\n)\n \n# Chunk document with accurate token counting\nchunks = chunk_text(\n    text=&quot;Your document content here&quot;,\n    source_id=&quot;document_id&quot;,\n    options=chunking_options\n)\n \nfor chunk in chunks:\n    print(f&quot;Chunk {chunk.chunk_index}: {chunk.token_count.count} tokens&quot;)\n    print(f&quot;Method: {chunk.token_count.method}&quot;)\n    print(f&quot;Was trimmed: {chunk.was_trimmed}&quot;)\nError Handling and Recovery\nfrom kano_backlog_core.tokenizer import get_default_registry\nfrom kano_backlog_core.tokenizer_errors import (\n    TokenizerError,\n    AdapterNotAvailableError,\n    FallbackChainExhaustedError\n)\n \nregistry = get_default_registry()\n \ntry:\n    adapter = registry.resolve(&quot;tiktoken&quot;, model_name=&quot;gpt-4&quot;)\n    result = adapter.count_tokens(&quot;Your text&quot;)\nexcept AdapterNotAvailableError as e:\n    print(f&quot;Adapter not available: {e}&quot;)\n    # Get recovery suggestions\n    suggestions = registry.suggest_recovery_strategy(e, &quot;tiktoken&quot;, &quot;gpt-4&quot;)\n    print(f&quot;Suggestion: {suggestions[&#039;user_message&#039;]}&quot;)\nexcept FallbackChainExhaustedError as e:\n    print(f&quot;No adapters available: {e}&quot;)\n    # Check dependencies\n    from kano_backlog_core.tokenizer_dependencies import get_dependency_manager\n    manager = get_dependency_manager()\n    report = manager.check_all_dependencies()\n    print(f&quot;Missing dependencies: {report.get_missing_dependencies()}&quot;)\nexcept TokenizerError as e:\n    print(f&quot;Tokenization error: {e}&quot;)\nMonitoring and Telemetry\nfrom kano_backlog_core.tokenizer import get_default_registry\nimport time\n \nregistry = get_default_registry()\n \n# Monitor adapter usage\nadapter = registry.resolve(&quot;auto&quot;, model_name=&quot;gpt-4&quot;)\nstart_time = time.perf_counter()\nresult = adapter.count_tokens(&quot;Your text&quot;)\nend_time = time.perf_counter()\n \n# Log telemetry\ntelemetry = {\n    &quot;adapter_used&quot;: adapter.adapter_id,\n    &quot;model_name&quot;: adapter.model_name,\n    &quot;token_count&quot;: result.count,\n    &quot;is_exact&quot;: result.is_exact,\n    &quot;processing_time_ms&quot;: (end_time - start_time) * 1000,\n    &quot;text_length&quot;: len(&quot;Your text&quot;)\n}\n \nprint(f&quot;Tokenization telemetry: {telemetry}&quot;)\n \n# Get recovery statistics\nstats = registry.get_recovery_statistics()\nif stats[&quot;total_recovery_attempts&quot;] &gt; 0:\n    print(f&quot;Recovery attempts: {stats[&#039;total_recovery_attempts&#039;]}&quot;)\n    print(f&quot;Most problematic adapter: {stats[&#039;most_problematic_adapter&#039;]}&quot;)\n\nSupport and Contributing\nGetting Help\n\nCheck Status: kano-backlog tokenizer status --verbose\nRun Diagnostics: kano-backlog tokenizer diagnose\nCheck Dependencies: kano-backlog tokenizer dependencies\nReview Configuration: kano-backlog tokenizer config --format json\n\nReporting Issues\nWhen reporting issues, please include:\n# System information\nkano-backlog tokenizer status --format json &gt; system_status.json\n \n# Dependency information  \nkano-backlog tokenizer dependencies --verbose &gt; dependencies.txt\n \n# Configuration\nkano-backlog tokenizer config --format json &gt; config.json\n \n# Test results\nkano-backlog tokenizer test --verbose &gt; test_results.txt 2&gt;&amp;1\nContributing\nThe tokenizer adapter system is designed to be extensible. Contributions are welcome for:\n\nNew adapter implementations\nPerformance optimizations\nAdditional model support\nDocumentation improvements\nTest coverage enhancements\n\nSee the main project README for contribution guidelines."},"skill/docs/tokenizer-cli-reference":{"title":"tokenizer-cli-reference","links":[],"tags":[],"content":"Tokenizer Adapters CLI Reference\nComplete command-line interface reference for tokenizer adapters\nThis document provides comprehensive reference for all tokenizer CLI commands, options, and usage patterns.\nTable of Contents\n\nCommand Overview\nGlobal Options\nCore Commands\nConfiguration Commands\nDiagnostic Commands\nPerformance Commands\nUtility Commands\nUsage Patterns\nOutput Formats\n\nCommand Overview\nAll tokenizer commands are accessed through the kano-backlog tokenizer subcommand:\nkano-backlog tokenizer &lt;command&gt; [options]\nCommand Categories\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCategoryCommandsPurposeCorestatus, test, compareBasic functionality and testingConfigurationconfig, validate, create-example, migrateConfiguration managementDiagnosticdiagnose, health, dependencies, adapter-statusSystem diagnosticsPerformancebenchmarkPerformance testing and optimizationUtilityenv, install-guide, list-models, recommendHelper utilities\nGlobal Options\nThese options are available for most commands:\n--config PATH\nSpecify configuration file path.\nkano-backlog tokenizer test --config /path/to/config.toml\nkano-backlog tokenizer validate --config my_config.toml\n--help\nShow command help.\nkano-backlog tokenizer --help\nkano-backlog tokenizer test --help\nCore Commands\nkano-backlog tokenizer status\nShow comprehensive system status including configuration, adapters, dependencies, and health.\nSyntax:\nkano-backlog tokenizer status [OPTIONS]\nOptions:\n\n--config PATH - Configuration file path\n--verbose - Show detailed information\n--format FORMAT - Output format (markdown, json)\n\nExamples:\n# Basic status\nkano-backlog tokenizer status\n \n# Detailed status\nkano-backlog tokenizer status --verbose\n \n# JSON output for scripting\nkano-backlog tokenizer status --format json\n \n# With custom configuration\nkano-backlog tokenizer status --config production_config.toml --verbose\nSample Output:\n# Tokenizer System Status\n \n**Overall Health:** ‚úÖ HEALTHY\n**Python Version:** 3.11.0 ‚úÖ\n \n## Configuration\n- **Adapter:** auto\n- **Model:** text-embedding-3-small\n- **Max Tokens:** auto\n- **Fallback Chain:** tiktoken ‚Üí huggingface ‚Üí heuristic\n \n## Adapter Status\n### ‚úÖ HEURISTIC\n- **Status:** Available\n- **Dependencies:** Ready\n \n### ‚úÖ TIKTOKEN\n- **Status:** Available\n- **Dependencies:** Ready\n \n### ‚ùå HUGGINGFACE\n- **Status:** Not available\n- **Error:** No module named &#039;transformers&#039;\nkano-backlog tokenizer test\nTest tokenizer adapters with sample text.\nSyntax:\nkano-backlog tokenizer test [OPTIONS]\nOptions:\n\n--config PATH - Configuration file path\n--text TEXT - Text to tokenize (default: sample text)\n--adapter NAME - Specific adapter to test\n--model NAME - Model name to use\n\nExamples:\n# Test with default settings\nkano-backlog tokenizer test\n \n# Test with custom text\nkano-backlog tokenizer test --text &quot;Your custom text here&quot;\n \n# Test specific adapter\nkano-backlog tokenizer test --adapter tiktoken --model gpt-4\n \n# Test with configuration file\nkano-backlog tokenizer test --config my_config.toml\n \n# Test with environment override\nKANO_TOKENIZER_ADAPTER=heuristic kano-backlog tokenizer test\nSample Output:\nTesting tokenizers with text: &#039;This is a test sentence for tokenizer adapter testing.&#039;\r\nText length: 58 characters\r\n\r\n‚úì HEURISTIC Adapter:\r\n  Token count: 14\r\n  Method: heuristic\r\n  Tokenizer ID: heuristic:text-embedding-3-small:chars_4.0\r\n  Is exact: False\r\n  Max tokens: 8192\r\n\r\n‚úì TIKTOKEN Adapter:\r\n  Token count: 12\r\n  Method: tiktoken\r\n  Tokenizer ID: tiktoken:text-embedding-3-small:cl100k_base\r\n  Is exact: True\r\n  Max tokens: 8192\r\n\r\nPrimary adapter resolution (auto):\r\n  Resolved to: tiktoken\r\n  Token count: 12\r\n  Is exact: True\n\nkano-backlog tokenizer compare\nCompare tokenization results across different adapters.\nSyntax:\nkano-backlog tokenizer compare TEXT [OPTIONS]\nArguments:\n\nTEXT - Text to tokenize and compare (required)\n\nOptions:\n\n--adapters LIST - Comma-separated adapter names (default: all available)\n--model NAME - Model name to use\n--show-tokens - Show actual token breakdown (when supported)\n\nExamples:\n# Compare all available adapters\nkano-backlog tokenizer compare &quot;Sample text to compare&quot;\n \n# Compare specific adapters\nkano-backlog tokenizer compare &quot;Sample text&quot; --adapters tiktoken,heuristic\n \n# Compare with specific model\nkano-backlog tokenizer compare &quot;Sample text&quot; --model gpt-4\n \n# Show token breakdown\nkano-backlog tokenizer compare &quot;Sample text&quot; --show-tokens\nSample Output:\n# Tokenizer Comparison\n**Text:** Sample text to compare across different tokenizers\n**Length:** 52 characters\n**Model:** text-embedding-3-small\n \n## Results\n| Adapter | Token Count | Exact | Method | Max Tokens | Status |\n|---------|-------------|-------|--------|------------|--------|\n| heuristic | 13 | ‚ùå | heuristic | 8192 | ‚úÖ |\n| tiktoken | 11 | ‚úÖ | tiktoken | 8192 | ‚úÖ |\n| huggingface | N/A | N/A | N/A | N/A | ‚ùå |\n \n## Analysis\n- **Token Count Range:** 11 - 13\n- **Variance:** 2 tokens (18.2%)\n- **Exact Adapters:** tiktoken\n \n## Recommendations\n- For maximum accuracy, use: **tiktoken**\n- For OpenAI models, **tiktoken** is recommended\nConfiguration Commands\nkano-backlog tokenizer config\nShow current configuration with environment overrides applied.\nSyntax:\nkano-backlog tokenizer config [OPTIONS]\nOptions:\n\n--config PATH - Configuration file path\n--format FORMAT - Output format (json, toml, yaml)\n\nExamples:\n# Show configuration in JSON\nkano-backlog tokenizer config --format json\n \n# Show configuration from specific file\nkano-backlog tokenizer config --config my_config.toml\n \n# Show configuration in TOML format\nkano-backlog tokenizer config --format toml\nSample Output (JSON):\n{\n  &quot;adapter&quot;: &quot;auto&quot;,\n  &quot;model&quot;: &quot;text-embedding-3-small&quot;,\n  &quot;max_tokens&quot;: null,\n  &quot;fallback_chain&quot;: [&quot;tiktoken&quot;, &quot;huggingface&quot;, &quot;heuristic&quot;],\n  &quot;options&quot;: {},\n  &quot;heuristic&quot;: {\n    &quot;chars_per_token&quot;: 4.0\n  },\n  &quot;tiktoken&quot;: {},\n  &quot;huggingface&quot;: {\n    &quot;use_fast&quot;: true,\n    &quot;trust_remote_code&quot;: false\n  }\n}\nkano-backlog tokenizer validate\nValidate tokenizer configuration.\nSyntax:\nkano-backlog tokenizer validate [OPTIONS]\nOptions:\n\n--config PATH - Configuration file path\n\nExamples:\n# Validate default configuration\nkano-backlog tokenizer validate\n \n# Validate specific configuration file\nkano-backlog tokenizer validate --config my_config.toml\nSample Output:\n‚úì Configuration is valid\r\n  Adapter: auto\r\n  Model: text-embedding-3-small\r\n  Max tokens: auto\r\n  Fallback chain: tiktoken ‚Üí huggingface ‚Üí heuristic\n\nkano-backlog tokenizer create-example\nCreate an example tokenizer configuration file.\nSyntax:\nkano-backlog tokenizer create-example [OPTIONS]\nOptions:\n\n--output PATH - Output file path (default: tokenizer_config.toml)\n--force - Overwrite existing file\n\nExamples:\n# Create example configuration\nkano-backlog tokenizer create-example\n \n# Create with custom name\nkano-backlog tokenizer create-example --output my_config.toml\n \n# Overwrite existing file\nkano-backlog tokenizer create-example --output existing_config.toml --force\nSample Output:\n‚úì Created example tokenizer configuration: tokenizer_config.toml\r\n\r\nEdit the file to customize your tokenizer settings.\r\nUse &#039;kano-backlog tokenizer validate --config &lt;path&gt;&#039; to validate your changes.\n\nkano-backlog tokenizer migrate\nMigrate configuration from old format to new TOML format.\nSyntax:\nkano-backlog tokenizer migrate INPUT_PATH [OPTIONS]\nArguments:\n\nINPUT_PATH - Input configuration file (JSON or TOML)\n\nOptions:\n\n--output PATH - Output TOML file path (default: input path with .toml extension)\n--force - Overwrite existing output file\n\nExamples:\n# Migrate JSON to TOML\nkano-backlog tokenizer migrate old_config.json\n \n# Migrate with custom output path\nkano-backlog tokenizer migrate old_config.json --output new_config.toml\n \n# Force overwrite\nkano-backlog tokenizer migrate old_config.json --output existing.toml --force\nSample Output:\n‚úì Migrated configuration from old_config.json to old_config.toml\r\n\r\nValidate the migrated configuration with:\r\n  kano-backlog tokenizer validate --config old_config.toml\n\nDiagnostic Commands\nkano-backlog tokenizer diagnose\nRun comprehensive tokenizer diagnostics.\nSyntax:\nkano-backlog tokenizer diagnose [OPTIONS]\nOptions:\n\n--config PATH - Configuration file path\n--model NAME - Specific model to diagnose\n--verbose - Show detailed diagnostic information\n\nExamples:\n# Basic diagnostics\nkano-backlog tokenizer diagnose\n \n# Diagnose specific model\nkano-backlog tokenizer diagnose --model gpt-4\n \n# Verbose diagnostics\nkano-backlog tokenizer diagnose --verbose\n \n# Diagnose with custom configuration\nkano-backlog tokenizer diagnose --config my_config.toml --verbose\nkano-backlog tokenizer health\nCheck health of a specific tokenizer adapter.\nSyntax:\nkano-backlog tokenizer health ADAPTER [OPTIONS]\nArguments:\n\nADAPTER - Adapter name (heuristic, tiktoken, huggingface)\n\nOptions:\n\n--model NAME - Model name to test with (default: test-model)\n\nExamples:\n# Check tiktoken health\nkano-backlog tokenizer health tiktoken\n \n# Check with specific model\nkano-backlog tokenizer health huggingface --model bert-base-uncased\n \n# Check all adapters\nfor adapter in heuristic tiktoken huggingface; do\n    echo &quot;Checking $adapter:&quot;\n    kano-backlog tokenizer health $adapter\n    echo\ndone\nSample Output:\n‚úÖ TIKTOKEN adapter is healthy\r\n   Token count: 12\r\n   Method: tiktoken\r\n   Is exact: True\r\n   Tokenizer ID: tiktoken:test-model:cl100k_base\r\n   Max tokens: 8192\n\nkano-backlog tokenizer dependencies\nCheck status of tokenizer dependencies.\nSyntax:\nkano-backlog tokenizer dependencies [OPTIONS]\nOptions:\n\n--verbose - Show detailed dependency information\n--refresh - Force refresh of dependency cache\n\nExamples:\n# Basic dependency check\nkano-backlog tokenizer dependencies\n \n# Detailed dependency information\nkano-backlog tokenizer dependencies --verbose\n \n# Force refresh cache\nkano-backlog tokenizer dependencies --refresh\nSample Output:\n‚úÖ Overall Health: HEALTHY\r\nüêç Python Version: 3.11.0 ‚úÖ\r\n\r\nüì¶ Dependencies:\r\n  ‚úÖ tiktoken\r\n      Version: 0.5.1\r\n  ‚ùå transformers\r\n      Error: No module named &#039;transformers&#039;\r\n      Installation:\r\n        pip install transformers\r\n        conda install transformers -c conda-forge\r\n\r\nüí° Recommendations:\r\n  ‚Ä¢ Install transformers for HuggingFace model support\r\n  ‚Ä¢ Consider using tiktoken for OpenAI models\r\n\r\n‚ùå Missing Dependencies: transformers\r\n   Use &#039;kano-backlog tokenizer install-guide&#039; for installation instructions\n\nkano-backlog tokenizer adapter-status\nShow status of tokenizer adapters including dependency checks.\nSyntax:\nkano-backlog tokenizer adapter-status [OPTIONS]\nOptions:\n\n--adapter NAME - Show status for specific adapter only\n\nExamples:\n# Show all adapter status\nkano-backlog tokenizer adapter-status\n \n# Show specific adapter status\nkano-backlog tokenizer adapter-status --adapter tiktoken\nSample Output:\nüîß Tokenizer Adapter Status:\r\n\r\n  ‚úÖ HEURISTIC\r\n      Status: Available\r\n      Dependencies: Ready\r\n\r\n  ‚úÖ TIKTOKEN\r\n      Status: Available\r\n      Dependencies: Ready\r\n\r\n  ‚ùå HUGGINGFACE\r\n      Status: Not available\r\n      Error: No module named &#039;transformers&#039;\r\n      Missing deps: transformers\r\n\r\nüìä Overall Health: DEGRADED\r\n‚ùå Missing: transformers\n\nPerformance Commands\nkano-backlog tokenizer benchmark\nBenchmark tokenizer adapter performance and accuracy.\nSyntax:\nkano-backlog tokenizer benchmark [OPTIONS]\nOptions:\n\n--text TEXT - Text for benchmarking (default: sample text)\n--iterations N - Number of test iterations (default: 10)\n--adapters LIST - Comma-separated adapter names (default: all available)\n--model NAME - Model name to use (default: text-embedding-3-small)\n--format FORMAT - Output format (markdown, json, csv)\n\nExamples:\n# Basic benchmark\nkano-backlog tokenizer benchmark\n \n# Benchmark with custom text\nkano-backlog tokenizer benchmark --text &quot;$(cat large_document.txt)&quot;\n \n# Benchmark specific adapters\nkano-backlog tokenizer benchmark --adapters tiktoken,heuristic --iterations 50\n \n# JSON output for analysis\nkano-backlog tokenizer benchmark --format json &gt; benchmark_results.json\n \n# CSV output for spreadsheet analysis\nkano-backlog tokenizer benchmark --format csv &gt; benchmark_results.csv\nSample Output (Markdown):\n# Tokenizer Adapter Benchmark Results\n \n## Performance Summary\n| Adapter | Avg Time (ms) | Tokens | Exact | Consistent | Status |\n|---------|---------------|--------|-------|------------|--------|\n| heuristic | 0.12 | 14 | ‚ùå | ‚úÖ | ‚úÖ |\n| tiktoken | 2.45 | 12 | ‚úÖ | ‚úÖ | ‚úÖ |\n \n## Detailed Results\n### HEURISTIC\n- **Average Time:** 0.12 ms\n- **Time Range:** 0.10 - 0.15 ms\n- **Token Count:** 14\n- **Exact Count:** No\n- **Consistent:** Yes\n- **Method:** heuristic\n- **Tokenizer ID:** heuristic:text-embedding-3-small:chars_4.0\n \n### TIKTOKEN\n- **Average Time:** 2.45 ms\n- **Time Range:** 2.20 - 2.80 ms\n- **Token Count:** 12\n- **Exact Count:** Yes\n- **Consistent:** Yes\n- **Method:** tiktoken\n- **Tokenizer ID:** tiktoken:text-embedding-3-small:cl100k_base\n \n## Performance Ranking\n**By Speed (fastest first):**\n1. heuristic (0.12 ms)\n2. tiktoken (2.45 ms)\n \n**By Accuracy (most accurate first):**\n1. tiktoken (exact, consistent)\n2. heuristic (consistent)\nUtility Commands\nkano-backlog tokenizer env\nShow available environment variables for tokenizer configuration.\nSyntax:\nkano-backlog tokenizer env\nSample Output:\nTokenizer Configuration Environment Variables:\r\n\r\n  KANO_TOKENIZER_ADAPTER\r\n    Description: Override adapter selection (auto, heuristic, tiktoken, huggingface)\r\n    Current value: not set\r\n\r\n  KANO_TOKENIZER_MODEL\r\n    Description: Override model name\r\n    Current value: not set\r\n\r\n  KANO_TOKENIZER_MAX_TOKENS\r\n    Description: Override max tokens (integer)\r\n    Current value: not set\r\n\r\n  KANO_TOKENIZER_HEURISTIC_CHARS_PER_TOKEN\r\n    Description: Override chars per token ratio (float)\r\n    Current value: not set\r\n\r\n  KANO_TOKENIZER_TIKTOKEN_ENCODING\r\n    Description: Override TikToken encoding\r\n    Current value: not set\r\n\r\n  KANO_TOKENIZER_HUGGINGFACE_USE_FAST\r\n    Description: Override use_fast setting (true/false)\r\n    Current value: not set\r\n\r\n  KANO_TOKENIZER_HUGGINGFACE_TRUST_REMOTE_CODE\r\n    Description: Override trust_remote_code (true/false)\r\n    Current value: not set\r\n\r\nExample usage:\r\n  export KANO_TOKENIZER_ADAPTER=heuristic\r\n  export KANO_TOKENIZER_HEURISTIC_CHARS_PER_TOKEN=3.5\r\n  kano-backlog tokenizer test\n\nkano-backlog tokenizer install-guide\nShow installation guide for missing dependencies.\nSyntax:\nkano-backlog tokenizer install-guide\nSample Output:\n# Tokenizer Dependencies Installation Guide\r\n\r\n## Missing Dependencies\r\nBased on your system check, the following dependencies are missing:\r\n\r\n### transformers (for HuggingFace adapter)\r\n**Installation Options:**\r\n```bash\r\n# Using pip (recommended)\r\npip install transformers\r\n\r\n# Using conda\r\nconda install transformers -c conda-forge\r\n\r\n# With specific version\r\npip install &quot;transformers&gt;=4.21.0&quot;\n\nVerification:\npython -c &quot;import transformers; print(&#039;Transformers version:&#039;, transformers.__version__)&quot;\nkano-backlog tokenizer health huggingface\nOptional Dependencies\nsentence-transformers (for sentence embedding models)\npip install sentence-transformers\ntorch (for GPU acceleration)\n# CPU-only version\npip install torch --index-url download.pytorch.org/whl/cpu\n \n# GPU version (CUDA 11.8)\npip install torch --index-url download.pytorch.org/whl/cu118\nVerification Commands\nAfter installation, verify your setup:\nkano-backlog tokenizer dependencies\nkano-backlog tokenizer status\nkano-backlog tokenizer test\n\r\n### `kano-backlog tokenizer list-models`\r\n\r\nList supported models and their token limits.\r\n\r\n**Syntax:**\r\n```bash\r\nkano-backlog tokenizer list-models [OPTIONS]\n\nOptions:\n\n--adapter NAME - Show models for specific adapter only\n--format FORMAT - Output format (markdown, json, csv)\n\nExamples:\n# List all supported models\nkano-backlog tokenizer list-models\n \n# List OpenAI models only\nkano-backlog tokenizer list-models --adapter tiktoken\n \n# List HuggingFace models only\nkano-backlog tokenizer list-models --adapter huggingface\n \n# JSON output for scripting\nkano-backlog tokenizer list-models --format json\nSample Output:\n# Supported Models\n \n**Total Models:** 45\n \n## OpenAI Models (15 models)\n \n| Model | Max Tokens | Encoding | Recommended Adapter |\n|-------|------------|----------|-------------------|\n| gpt-4 | 8192 | cl100k_base | tiktoken |\n| gpt-4-turbo | 128000 | cl100k_base | tiktoken |\n| gpt-3.5-turbo | 4096 | cl100k_base | tiktoken |\n| text-embedding-3-small | 8192 | cl100k_base | tiktoken |\n| text-embedding-3-large | 8192 | cl100k_base | tiktoken |\n \n## HuggingFace Models (25 models)\n \n| Model | Max Tokens | Encoding | Recommended Adapter |\n|-------|------------|----------|-------------------|\n| bert-base-uncased | 512 | N/A | huggingface |\n| sentence-transformers/all-MiniLM-L6-v2 | 512 | N/A | huggingface |\n| sentence-transformers/all-mpnet-base-v2 | 512 | N/A | huggingface |\n \n## Usage Notes\n- **Max Tokens:** Maximum context length for the model\n- **Encoding:** TikToken encoding used (for OpenAI models)\n- **Recommended Adapter:** Best adapter for accurate tokenization\n \n### Examples\n```bash\n# Use with embedding command\nkano-backlog embedding build --tokenizer-model text-embedding-3-small\n \n# Test tokenization\nkano-backlog tokenizer test --model bert-base-uncased --adapter huggingface\n\r\n### `kano-backlog tokenizer recommend`\r\n\r\nGet adapter recommendation for a specific model and requirements.\r\n\r\n**Syntax:**\r\n```bash\r\nkano-backlog tokenizer recommend MODEL [OPTIONS]\n\nArguments:\n\nMODEL - Model name to get recommendation for\n\nOptions:\n\n--requirements - Requirements as key=value pairs (e.g., accuracy=high,speed=medium)\n\nExamples:\n# Get recommendation for OpenAI model\nkano-backlog tokenizer recommend gpt-4\n \n# Get recommendation for HuggingFace model\nkano-backlog tokenizer recommend bert-base-uncased\n \n# Get recommendation with requirements\nkano-backlog tokenizer recommend gpt-4 --requirements &quot;accuracy=high,speed=medium&quot;\nSample Output:\n# Adapter Recommendation for &#039;gpt-4&#039;\n \n**Recommended Adapter:** tiktoken\n \n## Reasoning\n- Model appears to be an OpenAI model\n- TikToken provides exact tokenization for OpenAI models\n \n## Available Alternatives\n- ‚úÖ **heuristic**\n  - Fast approximation, good for development\n- ‚ùå **huggingface**\n  - Not available: No module named &#039;transformers&#039;\n \n## Usage Example\n```bash\n# Use recommended adapter in embedding command\nkano-backlog embedding build --tokenizer-adapter tiktoken --tokenizer-model gpt-4\n \n# Test the adapter\nkano-backlog tokenizer test --text &#039;Sample text&#039; --adapter tiktoken --model gpt-4\n\r\n## Usage Patterns\r\n\r\n### Basic Testing Workflow\r\n\r\n```bash\r\n# 1. Check system status\r\nkano-backlog tokenizer status\r\n\r\n# 2. Test basic functionality\r\nkano-backlog tokenizer test\r\n\r\n# 3. Compare adapters\r\nkano-backlog tokenizer compare &quot;Your sample text&quot;\r\n\r\n# 4. Validate configuration\r\nkano-backlog tokenizer validate\n\nConfiguration Workflow\n# 1. Create example configuration\nkano-backlog tokenizer create-example --output my_config.toml\n \n# 2. Edit configuration file\n# (edit my_config.toml)\n \n# 3. Validate configuration\nkano-backlog tokenizer validate --config my_config.toml\n \n# 4. Test configuration\nkano-backlog tokenizer test --config my_config.toml\n \n# 5. Benchmark performance\nkano-backlog tokenizer benchmark --config my_config.toml\nTroubleshooting Workflow\n# 1. Check overall health\nkano-backlog tokenizer status --verbose\n \n# 2. Check dependencies\nkano-backlog tokenizer dependencies --verbose\n \n# 3. Check adapter health\nkano-backlog tokenizer health tiktoken\nkano-backlog tokenizer health huggingface\nkano-backlog tokenizer health heuristic\n \n# 4. Run diagnostics\nkano-backlog tokenizer diagnose --verbose\n \n# 5. Get installation guide\nkano-backlog tokenizer install-guide\nPerformance Analysis Workflow\n# 1. Benchmark current setup\nkano-backlog tokenizer benchmark --format json &gt; baseline.json\n \n# 2. Test with different configurations\nexport KANO_TOKENIZER_ADAPTER=heuristic\nkano-backlog tokenizer benchmark --format json &gt; heuristic.json\n \nexport KANO_TOKENIZER_ADAPTER=tiktoken\nkano-backlog tokenizer benchmark --format json &gt; tiktoken.json\n \n# 3. Compare results\n# (analyze JSON files)\n \n# 4. Choose optimal configuration\nProduction Deployment Workflow\n# 1. Create production configuration\nkano-backlog tokenizer create-example --output production_config.toml\n# (edit production_config.toml)\n \n# 2. Validate configuration\nkano-backlog tokenizer validate --config production_config.toml\n \n# 3. Test with production-like data\nkano-backlog tokenizer test --config production_config.toml --text &quot;$(cat sample_production_data.txt)&quot;\n \n# 4. Benchmark performance\nkano-backlog tokenizer benchmark --config production_config.toml --iterations 50\n \n# 5. Check system health\nkano-backlog tokenizer status --config production_config.toml --verbose\n \n# 6. Deploy configuration\ncp production_config.toml /etc/kano/tokenizer.toml\nOutput Formats\nJSON Format\nMost commands support --format json for machine-readable output:\nkano-backlog tokenizer status --format json\nkano-backlog tokenizer benchmark --format json\nkano-backlog tokenizer config --format json\nExample JSON Output:\n{\n  &quot;overall_health&quot;: &quot;healthy&quot;,\n  &quot;python_version&quot;: &quot;3.11.0&quot;,\n  &quot;python_compatible&quot;: true,\n  &quot;configuration&quot;: {\n    &quot;adapter&quot;: &quot;auto&quot;,\n    &quot;model&quot;: &quot;text-embedding-3-small&quot;,\n    &quot;max_tokens&quot;: null,\n    &quot;fallback_chain&quot;: [&quot;tiktoken&quot;, &quot;huggingface&quot;, &quot;heuristic&quot;]\n  },\n  &quot;adapters&quot;: {\n    &quot;heuristic&quot;: {\n      &quot;available&quot;: true,\n      &quot;error&quot;: null\n    },\n    &quot;tiktoken&quot;: {\n      &quot;available&quot;: true,\n      &quot;error&quot;: null\n    },\n    &quot;huggingface&quot;: {\n      &quot;available&quot;: false,\n      &quot;error&quot;: &quot;No module named &#039;transformers&#039;&quot;\n    }\n  }\n}\nCSV Format\nBenchmark and list commands support CSV output:\nkano-backlog tokenizer benchmark --format csv\nkano-backlog tokenizer list-models --format csv\nTOML/YAML Formats\nConfiguration commands support multiple formats:\nkano-backlog tokenizer config --format toml\nkano-backlog tokenizer config --format yaml\n\nThis CLI reference provides comprehensive documentation for all tokenizer adapter commands. Use kano-backlog tokenizer &lt;command&gt; --help for detailed help on any specific command."},"skill/docs/tokenizer-configuration":{"title":"tokenizer-configuration","links":[],"tags":[],"content":"Tokenizer Adapters Configuration Reference\nComplete configuration reference for tokenizer adapters\nThis guide provides comprehensive configuration options, examples, and best practices for configuring tokenizer adapters.\nTable of Contents\n\nConfiguration Overview\nConfiguration File Format\nConfiguration Options\nEnvironment Variables\nConfiguration Examples\nMigration Guide\nValidation and Testing\n\nConfiguration Overview\nConfiguration Hierarchy\nTokenizer adapter configuration follows this precedence order (highest to lowest):\n\nEnvironment Variables - Runtime overrides\nConfiguration File - Explicit TOML/JSON configuration\nDefault Values - Built-in sensible defaults\n\nConfiguration Sources\n# 1. Environment variables (highest precedence)\nexport KANO_TOKENIZER_ADAPTER=tiktoken\n \n# 2. Configuration file\nkano-backlog tokenizer test --config my_config.toml\n \n# 3. Default configuration (lowest precedence)\nkano-backlog tokenizer test  # Uses built-in defaults\nConfiguration File Locations\nThe system searches for configuration files in this order:\n\nExplicitly specified: --config path/to/config.toml\nCurrent directory: ./tokenizer_config.toml\nCurrent directory: ./config.toml (tokenizer section)\nUser config: ~/.config/kano/tokenizer.toml\n\nConfiguration File Format\nTOML Format (Recommended)\n# tokenizer_config.toml\n[tokenizer]\n# Main configuration\nadapter = &quot;auto&quot;\nmodel = &quot;text-embedding-3-small&quot;\nmax_tokens = 8192\nfallback_chain = [&quot;tiktoken&quot;, &quot;huggingface&quot;, &quot;heuristic&quot;]\n \n# Adapter-specific configurations\n[tokenizer.heuristic]\nchars_per_token = 4.0\n \n[tokenizer.tiktoken]\nencoding = &quot;cl100k_base&quot;\n \n[tokenizer.huggingface]\nuse_fast = true\ntrust_remote_code = false\nJSON Format (Legacy)\n{\n  &quot;tokenizer&quot;: {\n    &quot;adapter&quot;: &quot;auto&quot;,\n    &quot;model&quot;: &quot;text-embedding-3-small&quot;,\n    &quot;max_tokens&quot;: 8192,\n    &quot;fallback_chain&quot;: [&quot;tiktoken&quot;, &quot;huggingface&quot;, &quot;heuristic&quot;],\n    &quot;heuristic&quot;: {\n      &quot;chars_per_token&quot;: 4.0\n    },\n    &quot;tiktoken&quot;: {\n      &quot;encoding&quot;: &quot;cl100k_base&quot;\n    },\n    &quot;huggingface&quot;: {\n      &quot;use_fast&quot;: true,\n      &quot;trust_remote_code&quot;: false\n    }\n  }\n}\nEmbedded Configuration\nConfiguration can be embedded in larger configuration files:\n# config.toml (application configuration)\n[app]\nname = &quot;My Application&quot;\nversion = &quot;1.0.0&quot;\n \n[database]\nurl = &quot;sqlite:///app.db&quot;\n \n# Tokenizer configuration section\n[tokenizer]\nadapter = &quot;tiktoken&quot;\nmodel = &quot;gpt-4&quot;\n \n[tokenizer.tiktoken]\nencoding = &quot;cl100k_base&quot;\nConfiguration Options\nMain Configuration Section\nadapter (string)\nDescription: Primary tokenizer adapter to use.\nValues:\n\n&quot;auto&quot; - Use fallback chain (recommended)\n&quot;heuristic&quot; - Fast approximation adapter\n&quot;tiktoken&quot; - OpenAI TikToken adapter\n&quot;huggingface&quot; - HuggingFace transformers adapter\n\nDefault: &quot;auto&quot;\nExamples:\n# Use specific adapter\nadapter = &quot;tiktoken&quot;\n \n# Use fallback chain\nadapter = &quot;auto&quot;\nmodel (string)\nDescription: Model name for tokenization and token limit resolution.\nDefault: &quot;text-embedding-3-small&quot;\nExamples:\n# OpenAI models\nmodel = &quot;gpt-4&quot;\nmodel = &quot;gpt-3.5-turbo&quot;\nmodel = &quot;text-embedding-3-large&quot;\n \n# HuggingFace models\nmodel = &quot;bert-base-uncased&quot;\nmodel = &quot;sentence-transformers/all-MiniLM-L6-v2&quot;\n \n# Custom models\nmodel = &quot;my-custom-model&quot;\nmax_tokens (integer, optional)\nDescription: Override the model‚Äôs maximum token limit.\nDefault: null (use model‚Äôs default limit)\nExamples:\n# Override token limit\nmax_tokens = 4096\n \n# Use model default\n# max_tokens = null  (or omit)\nfallback_chain (array of strings)\nDescription: Ordered list of adapters to try when primary adapter fails.\nDefault: [&quot;tiktoken&quot;, &quot;huggingface&quot;, &quot;heuristic&quot;]\nExamples:\n# Standard fallback chain\nfallback_chain = [&quot;tiktoken&quot;, &quot;huggingface&quot;, &quot;heuristic&quot;]\n \n# Speed-optimized chain\nfallback_chain = [&quot;heuristic&quot;]\n \n# Accuracy-first chain\nfallback_chain = [&quot;tiktoken&quot;, &quot;huggingface&quot;]\n \n# Custom order\nfallback_chain = [&quot;huggingface&quot;, &quot;tiktoken&quot;, &quot;heuristic&quot;]\nHeuristic Adapter Configuration\n[tokenizer.heuristic]\nDescription: Configuration for the heuristic (approximation) adapter.\nchars_per_token (float)\nDescription: Average number of characters per token for estimation.\nDefault: 4.0\nRange: &gt; 0.0 (must be positive)\nLanguage-Specific Recommendations:\n# English technical content\n[tokenizer.heuristic]\nchars_per_token = 4.2\n \n# Code-heavy content\n[tokenizer.heuristic]\nchars_per_token = 3.8\n \n# Academic papers (punctuation-heavy)\n[tokenizer.heuristic]\nchars_per_token = 4.5\n \n# CJK-heavy content\n[tokenizer.heuristic]\nchars_per_token = 2.0\n \n# Mixed content\n[tokenizer.heuristic]\nchars_per_token = 4.0  # Good general-purpose value\nTikToken Adapter Configuration\n[tokenizer.tiktoken]\nDescription: Configuration for the TikToken (OpenAI) adapter.\nencoding (string, optional)\nDescription: Specific TikToken encoding to use.\nDefault: null (auto-detect based on model)\nAvailable Encodings:\n\n&quot;cl100k_base&quot; - GPT-4, GPT-3.5-turbo, text-embedding-3-*\n&quot;p50k_base&quot; - text-davinci-003, code-davinci-002\n&quot;r50k_base&quot; - GPT-3 models (davinci, curie, babbage, ada)\n&quot;gpt2&quot; - GPT-2 models\n\nExamples:\n# Auto-detect encoding (recommended)\n[tokenizer.tiktoken]\n# encoding not specified\n \n# Explicit encoding\n[tokenizer.tiktoken]\nencoding = &quot;cl100k_base&quot;\n \n# Legacy model encoding\n[tokenizer.tiktoken]\nencoding = &quot;p50k_base&quot;\nHuggingFace Adapter Configuration\n[tokenizer.huggingface]\nDescription: Configuration for the HuggingFace transformers adapter.\nuse_fast (boolean)\nDescription: Use fast tokenizer implementation when available.\nDefault: true\nBenefits of Fast Tokenizers:\n\nSignificantly faster tokenization\nBetter memory efficiency\nConsistent with HuggingFace model training\n\nExamples:\n# Use fast tokenizers (recommended)\n[tokenizer.huggingface]\nuse_fast = true\n \n# Use slow tokenizers (compatibility)\n[tokenizer.huggingface]\nuse_fast = false\ntrust_remote_code (boolean)\nDescription: Allow execution of remote code from model repositories.\nDefault: false\nSecurity Note: Setting this to true allows arbitrary code execution from model repositories. Only enable for trusted models.\nExamples:\n# Secure setting (recommended)\n[tokenizer.huggingface]\ntrust_remote_code = false\n \n# Allow remote code (use with caution)\n[tokenizer.huggingface]\ntrust_remote_code = true\nEnvironment Variables\nMain Configuration Variables\nKANO_TOKENIZER_ADAPTER\nDescription: Override adapter selection.\nValues: auto, heuristic, tiktoken, huggingface\nExample:\nexport KANO_TOKENIZER_ADAPTER=tiktoken\nKANO_TOKENIZER_MODEL\nDescription: Override model name.\nExample:\nexport KANO_TOKENIZER_MODEL=gpt-4\nexport KANO_TOKENIZER_MODEL=bert-base-uncased\nKANO_TOKENIZER_MAX_TOKENS\nDescription: Override maximum token limit.\nExample:\nexport KANO_TOKENIZER_MAX_TOKENS=4096\nAdapter-Specific Variables\nHeuristic Adapter\n# Override chars per token ratio\nexport KANO_TOKENIZER_HEURISTIC_CHARS_PER_TOKEN=4.2\nTikToken Adapter\n# Override encoding\nexport KANO_TOKENIZER_TIKTOKEN_ENCODING=cl100k_base\nHuggingFace Adapter\n# Override fast tokenizer setting\nexport KANO_TOKENIZER_HUGGINGFACE_USE_FAST=true\nexport KANO_TOKENIZER_HUGGINGFACE_USE_FAST=false\n \n# Override trust remote code setting\nexport KANO_TOKENIZER_HUGGINGFACE_TRUST_REMOTE_CODE=false\nexport KANO_TOKENIZER_HUGGINGFACE_TRUST_REMOTE_CODE=true\nEnvironment Variable Format\nBoolean Values:\n# Accepted true values\nexport KANO_TOKENIZER_HUGGINGFACE_USE_FAST=true\nexport KANO_TOKENIZER_HUGGINGFACE_USE_FAST=1\nexport KANO_TOKENIZER_HUGGINGFACE_USE_FAST=yes\nexport KANO_TOKENIZER_HUGGINGFACE_USE_FAST=on\n \n# Accepted false values\nexport KANO_TOKENIZER_HUGGINGFACE_USE_FAST=false\nexport KANO_TOKENIZER_HUGGINGFACE_USE_FAST=0\nexport KANO_TOKENIZER_HUGGINGFACE_USE_FAST=no\nexport KANO_TOKENIZER_HUGGINGFACE_USE_FAST=off\nNumeric Values:\n# Integer values\nexport KANO_TOKENIZER_MAX_TOKENS=4096\n \n# Float values\nexport KANO_TOKENIZER_HEURISTIC_CHARS_PER_TOKEN=4.2\nConfiguration Examples\nDevelopment Configuration\nFast, dependency-free setup for development:\n# dev_config.toml\n[tokenizer]\nadapter = &quot;heuristic&quot;\nmodel = &quot;development-model&quot;\nfallback_chain = [&quot;heuristic&quot;]\n \n[tokenizer.heuristic]\nchars_per_token = 4.0\nUsage:\nexport KANO_TOKENIZER_CONFIG=dev_config.toml\nkano-backlog tokenizer test\nProduction OpenAI Configuration\nOptimized for OpenAI models in production:\n# production_openai_config.toml\n[tokenizer]\nadapter = &quot;tiktoken&quot;\nmodel = &quot;text-embedding-3-small&quot;\nfallback_chain = [&quot;tiktoken&quot;, &quot;heuristic&quot;]\n \n[tokenizer.tiktoken]\n# Let encoding be auto-detected\n \n[tokenizer.heuristic]\nchars_per_token = 4.0  # Fallback configuration\nEnvironment overrides:\n# Override for different models\nexport KANO_TOKENIZER_MODEL=gpt-4\nexport KANO_TOKENIZER_MAX_TOKENS=8192\nProduction HuggingFace Configuration\nOptimized for HuggingFace models:\n# production_hf_config.toml\n[tokenizer]\nadapter = &quot;huggingface&quot;\nmodel = &quot;sentence-transformers/all-MiniLM-L6-v2&quot;\nfallback_chain = [&quot;huggingface&quot;, &quot;heuristic&quot;]\n \n[tokenizer.huggingface]\nuse_fast = true\ntrust_remote_code = false\n \n[tokenizer.heuristic]\nchars_per_token = 4.0\nMulti-Environment Configuration\nFlexible configuration for multiple environments:\n# multi_env_config.toml\n[tokenizer]\nadapter = &quot;auto&quot;\nmodel = &quot;text-embedding-3-small&quot;\nfallback_chain = [&quot;tiktoken&quot;, &quot;huggingface&quot;, &quot;heuristic&quot;]\n \n# All adapter configurations available\n[tokenizer.heuristic]\nchars_per_token = 4.0\n \n[tokenizer.tiktoken]\n# encoding auto-detected\n \n[tokenizer.huggingface]\nuse_fast = true\ntrust_remote_code = false\nEnvironment-specific overrides:\n# Development\nexport KANO_TOKENIZER_ADAPTER=heuristic\n \n# Staging\nexport KANO_TOKENIZER_ADAPTER=tiktoken\n \n# Production\nexport KANO_TOKENIZER_ADAPTER=auto\nHigh-Performance Configuration\nOptimized for maximum performance:\n# performance_config.toml\n[tokenizer]\nadapter = &quot;heuristic&quot;\nmodel = &quot;performance-model&quot;\nfallback_chain = [&quot;heuristic&quot;]\n \n[tokenizer.heuristic]\nchars_per_token = 4.0\nHigh-Accuracy Configuration\nOptimized for maximum accuracy:\n# accuracy_config.toml\n[tokenizer]\nadapter = &quot;auto&quot;\nmodel = &quot;gpt-4&quot;\nfallback_chain = [&quot;tiktoken&quot;, &quot;huggingface&quot;, &quot;heuristic&quot;]\n \n[tokenizer.tiktoken]\n# Use auto-detection for best accuracy\n \n[tokenizer.huggingface]\nuse_fast = true  # Fast tokenizers are still accurate\ntrust_remote_code = false\nLanguage-Specific Configurations\nEnglish Technical Content\n[tokenizer]\nadapter = &quot;auto&quot;\nmodel = &quot;text-embedding-3-small&quot;\n \n[tokenizer.heuristic]\nchars_per_token = 4.2  # Optimized for technical English\nCode-Heavy Content\n[tokenizer]\nadapter = &quot;auto&quot;\nmodel = &quot;code-davinci-002&quot;\n \n[tokenizer.heuristic]\nchars_per_token = 3.8  # Code has more punctuation/symbols\nAcademic Papers\n[tokenizer]\nadapter = &quot;auto&quot;\nmodel = &quot;gpt-4&quot;\n \n[tokenizer.heuristic]\nchars_per_token = 4.5  # Academic text has more punctuation\nMultilingual Content\n[tokenizer]\nadapter = &quot;auto&quot;\nmodel = &quot;sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2&quot;\n \n[tokenizer.heuristic]\nchars_per_token = 3.5  # Mixed languages, more conservative estimate\nDocker Configuration\nConfiguration for containerized environments:\n# docker_config.toml\n[tokenizer]\nadapter = &quot;auto&quot;\nmodel = &quot;text-embedding-3-small&quot;\nfallback_chain = [&quot;heuristic&quot;, &quot;tiktoken&quot;, &quot;huggingface&quot;]  # Heuristic first for reliability\n \n[tokenizer.heuristic]\nchars_per_token = 4.0\n \n[tokenizer.tiktoken]\n# encoding auto-detected\n \n[tokenizer.huggingface]\nuse_fast = true\ntrust_remote_code = false\nDockerfile:\nFROM python:3.11-slim\n \n# Install dependencies\nRUN pip install tiktoken transformers\n \n# Copy configuration\nCOPY docker_config.toml /app/tokenizer_config.toml\n \n# Set environment\nENV KANO_TOKENIZER_CONFIG=/app/tokenizer_config.toml\n \n# Your application\nCOPY . /app\nWORKDIR /app\nCI/CD Configuration\nConfiguration for continuous integration:\n# ci_config.toml\n[tokenizer]\nadapter = &quot;heuristic&quot;  # Reliable in CI environments\nmodel = &quot;ci-test-model&quot;\nfallback_chain = [&quot;heuristic&quot;]\n \n[tokenizer.heuristic]\nchars_per_token = 4.0\nGitHub Actions:\n- name: Test tokenizers\n  env:\n    KANO_TOKENIZER_CONFIG: ci_config.toml\n  run: |\n    kano-backlog tokenizer test\n    kano-backlog tokenizer validate\nMigration Guide\nFrom JSON to TOML\nOld JSON Configuration:\n{\n  &quot;tokenizer&quot;: {\n    &quot;adapter&quot;: &quot;tiktoken&quot;,\n    &quot;model&quot;: &quot;gpt-4&quot;,\n    &quot;options&quot;: {\n      &quot;heuristic&quot;: {\n        &quot;chars_per_token&quot;: 4.0\n      },\n      &quot;tiktoken&quot;: {\n        &quot;encoding&quot;: &quot;cl100k_base&quot;\n      }\n    }\n  }\n}\nNew TOML Configuration:\n[tokenizer]\nadapter = &quot;tiktoken&quot;\nmodel = &quot;gpt-4&quot;\n \n[tokenizer.heuristic]\nchars_per_token = 4.0\n \n[tokenizer.tiktoken]\nencoding = &quot;cl100k_base&quot;\nMigration Command:\nkano-backlog tokenizer migrate old_config.json --output new_config.toml\nFrom Environment-Only to Configuration File\nOld Environment Variables:\nexport KANO_TOKENIZER_ADAPTER=tiktoken\nexport KANO_TOKENIZER_MODEL=gpt-4\nexport KANO_TOKENIZER_HEURISTIC_CHARS_PER_TOKEN=4.2\nNew Configuration File:\n[tokenizer]\nadapter = &quot;tiktoken&quot;\nmodel = &quot;gpt-4&quot;\n \n[tokenizer.heuristic]\nchars_per_token = 4.2\nMigration Steps:\n\nCreate configuration file with current settings\nTest configuration: kano-backlog tokenizer validate --config new_config.toml\nUpdate deployment to use configuration file\nRemove environment variables (optional)\n\nFrom Legacy Pipeline Configuration\nOld Pipeline Configuration:\n[pipeline]\ntokenizer_adapter = &quot;tiktoken&quot;\ntokenizer_model = &quot;gpt-4&quot;\nchars_per_token = 4.0\nNew Tokenizer Configuration:\n[tokenizer]\nadapter = &quot;tiktoken&quot;\nmodel = &quot;gpt-4&quot;\n \n[tokenizer.heuristic]\nchars_per_token = 4.0\nValidation and Testing\nConfiguration Validation\nValidate Configuration File:\n# Validate TOML syntax and values\nkano-backlog tokenizer validate --config your_config.toml\n \n# Show parsed configuration\nkano-backlog tokenizer config --config your_config.toml --format json\nCommon Validation Errors:\n\nInvalid TOML Syntax:\n\n# Wrong: string instead of number\nchars_per_token = &quot;4.0&quot;\n \n# Right: numeric value\nchars_per_token = 4.0\n\nInvalid Boolean Values:\n\n# Wrong: string instead of boolean\nuse_fast = &quot;true&quot;\n \n# Right: boolean value\nuse_fast = true\n\nInvalid Array Format:\n\n# Wrong: string instead of array\nfallback_chain = &quot;tiktoken,heuristic&quot;\n \n# Right: array format\nfallback_chain = [&quot;tiktoken&quot;, &quot;heuristic&quot;]\nConfiguration Testing\nTest Configuration:\n# Test with sample text\nkano-backlog tokenizer test --config your_config.toml\n \n# Test all adapters in fallback chain\nkano-backlog tokenizer test --config your_config.toml --verbose\n \n# Benchmark performance\nkano-backlog tokenizer benchmark --config your_config.toml\nTest Environment Variable Overrides:\n# Test override\nexport KANO_TOKENIZER_ADAPTER=heuristic\nkano-backlog tokenizer test --config your_config.toml\n \n# Verify override took effect\nkano-backlog tokenizer config --config your_config.toml --format json | jq &#039;.adapter&#039;\nConfiguration Debugging\nDebug Configuration Loading:\n# Show effective configuration\nkano-backlog tokenizer config --format json\n \n# Show configuration with specific file\nkano-backlog tokenizer config --config your_config.toml --format json\n \n# Show environment variables\nkano-backlog tokenizer env\nDebug Adapter Resolution:\n# Show which adapter will be used\nkano-backlog tokenizer diagnose --config your_config.toml\n \n# Test adapter availability\nkano-backlog tokenizer adapter-status\nConfiguration Best Practices\n\nUse TOML Format: More readable and maintainable than JSON\nVersion Control: Keep configuration files in version control\nEnvironment Separation: Use different configs for dev/staging/prod\nValidation: Always validate configuration before deployment\nDocumentation: Document any custom configuration choices\nTesting: Test configuration changes thoroughly\nFallback Chain: Always include heuristic adapter as final fallback\nSecurity: Never set trust_remote_code = true in production\n\nConfiguration Templates\nCreate Configuration Template:\n# Create example configuration\nkano-backlog tokenizer create-example --output template_config.toml\n \n# Customize for your needs\ncp template_config.toml my_config.toml\n# Edit my_config.toml\n \n# Validate customized configuration\nkano-backlog tokenizer validate --config my_config.toml\n\nThis configuration reference provides comprehensive guidance for configuring tokenizer adapters across different environments and use cases. Regular validation and testing ensure reliable operation in production systems."},"skill/docs/tokenizer-performance":{"title":"tokenizer-performance","links":[],"tags":[],"content":"Tokenizer Adapters Performance Tuning Guide\nOptimize tokenizer adapter performance for your specific use case\nThis guide provides comprehensive performance tuning recommendations, benchmarking strategies, and optimization techniques for tokenizer adapters.\nTable of Contents\n\nPerformance Overview\nBenchmarking Your Setup\nAdapter Selection for Performance\nConfiguration Optimization\nSystem-Level Optimization\nUse Case Specific Tuning\nMonitoring and Profiling\nProduction Recommendations\n\nPerformance Overview\nAdapter Performance Characteristics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdapterSpeedMemoryAccuracyStartupBest ForHeuristic‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠êDevelopment, high-throughputTikToken‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠êOpenAI models, balancedHuggingFace‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠êHF models, accuracy-critical\nPerformance Targets\nTypical Performance Expectations:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nText SizeHeuristicTikTokenHuggingFace1KB&lt;1ms&lt;5ms&lt;20ms10KB&lt;5ms&lt;20ms&lt;100ms100KB&lt;20ms&lt;100ms&lt;500ms1MB&lt;100ms&lt;500ms&lt;2s\nMemory Usage:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdapterBase MemoryPer-Text OverheadHeuristic~1MBMinimalTikToken~10MBLowHuggingFace~100MB-1GBMedium\nBenchmarking Your Setup\nBasic Performance Testing\nQuick Performance Check:\n# Test current setup\nkano-backlog tokenizer benchmark\n \n# Test with your typical content\nkano-backlog tokenizer benchmark --text &quot;$(cat typical_document.txt)&quot;\n \n# Compare adapters\nkano-backlog tokenizer benchmark --adapters heuristic,tiktoken,huggingface\nDetailed Benchmarking:\n# Comprehensive benchmark with multiple iterations\nkano-backlog tokenizer benchmark \\\n  --text &quot;$(cat sample_document.txt)&quot; \\\n  --iterations 50 \\\n  --format json &gt; benchmark_results.json\n \n# Analyze results\ncat benchmark_results.json | jq &#039;.results[] | {adapter, avg_time_ms, avg_tokens, consistent}&#039;\nCustom Benchmarking Scripts\nText Size Performance:\n#!/bin/bash\n# benchmark_text_sizes.sh\n \necho &quot;Benchmarking different text sizes...&quot;\n \nfor size in 1000 5000 10000 50000 100000; do\n    echo &quot;Testing ${size} characters:&quot;\n    head -c $size large_document.txt &gt; test_${size}.txt\n    \n    kano-backlog tokenizer benchmark \\\n      --text &quot;$(cat test_${size}.txt)&quot; \\\n      --iterations 10 \\\n      --format json | jq -r &#039;.results[] | &quot;\\(.adapter): \\(.avg_time_ms)ms&quot;&#039;\n    \n    rm test_${size}.txt\n    echo\ndone\nAdapter Comparison:\n#!/bin/bash\n# compare_adapters.sh\n \nadapters=(&quot;heuristic&quot; &quot;tiktoken&quot; &quot;huggingface&quot;)\ntest_text=&quot;Your typical document content here. This should represent the kind of text you process most often in your application.&quot;\n \necho &quot;Adapter Performance Comparison&quot;\necho &quot;==============================&quot;\n \nfor adapter in &quot;${adapters[@]}&quot;; do\n    echo &quot;Testing $adapter:&quot;\n    \n    # Check if adapter is available\n    if kano-backlog tokenizer health $adapter &gt;/dev/null 2&gt;&amp;1; then\n        result=$(kano-backlog tokenizer benchmark \\\n          --adapters $adapter \\\n          --text &quot;$test_text&quot; \\\n          --iterations 20 \\\n          --format json)\n        \n        avg_time=$(echo $result | jq -r &#039;.results[0].avg_time_ms&#039;)\n        tokens=$(echo $result | jq -r &#039;.results[0].avg_tokens&#039;)\n        consistent=$(echo $result | jq -r &#039;.results[0].consistent&#039;)\n        \n        echo &quot;  Average time: ${avg_time}ms&quot;\n        echo &quot;  Token count: $tokens&quot;\n        echo &quot;  Consistent: $consistent&quot;\n    else\n        echo &quot;  Not available&quot;\n    fi\n    echo\ndone\nMemory Profiling\nMonitor Memory Usage:\n# Install memory profiler\npip install memory-profiler psutil\n \n# Create memory profiling script\ncat &gt; profile_memory.py &lt;&lt; &#039;EOF&#039;\nfrom memory_profiler import profile\nfrom kano_backlog_core.tokenizer import get_default_registry\nimport psutil\nimport os\n \n@profile\ndef test_tokenizer_memory(adapter_name, model_name, text):\n    &quot;&quot;&quot;Profile memory usage of tokenizer adapter.&quot;&quot;&quot;\n    process = psutil.Process(os.getpid())\n    initial_memory = process.memory_info().rss / 1024 / 1024  # MB\n    \n    registry = get_default_registry()\n    adapter = registry.resolve(adapter_name, model_name=model_name)\n    \n    after_init_memory = process.memory_info().rss / 1024 / 1024  # MB\n    \n    # Tokenize multiple times\n    for i in range(100):\n        result = adapter.count_tokens(text)\n    \n    final_memory = process.memory_info().rss / 1024 / 1024  # MB\n    \n    print(f&quot;Initial memory: {initial_memory:.1f}MB&quot;)\n    print(f&quot;After init: {after_init_memory:.1f}MB&quot;)\n    print(f&quot;Final memory: {final_memory:.1f}MB&quot;)\n    print(f&quot;Init overhead: {after_init_memory - initial_memory:.1f}MB&quot;)\n    print(f&quot;Processing overhead: {final_memory - after_init_memory:.1f}MB&quot;)\n \nif __name__ == &quot;__main__&quot;:\n    test_text = &quot;Your test text here&quot; * 100  # Repeat for larger text\n    test_tokenizer_memory(&quot;tiktoken&quot;, &quot;gpt-4&quot;, test_text)\nEOF\n \n# Run memory profiling\npython profile_memory.py\nAdapter Selection for Performance\nSpeed-Optimized Configuration\nMaximum Speed (Development/Testing):\n[tokenizer]\nadapter = &quot;heuristic&quot;\nfallback_chain = [&quot;heuristic&quot;]  # Skip slower adapters\n \n[tokenizer.heuristic]\nchars_per_token = 4.0  # Tune for your content\nBalanced Speed/Accuracy:\n[tokenizer]\nadapter = &quot;tiktoken&quot;\nfallback_chain = [&quot;tiktoken&quot;, &quot;heuristic&quot;]  # Fast fallback\n \n[tokenizer.tiktoken]\n# Let encoding be auto-detected for speed\nAccuracy-First (Production):\n[tokenizer]\nadapter = &quot;auto&quot;\nfallback_chain = [&quot;tiktoken&quot;, &quot;huggingface&quot;, &quot;heuristic&quot;]\n \n[tokenizer.huggingface]\nuse_fast = true  # Enable fast tokenizers\nAdapter Selection Decision Tree\nStart\r\n  ‚Üì\r\nNeed exact tokenization?\r\n  ‚îú‚îÄ No ‚Üí Use Heuristic (fastest)\r\n  ‚îî‚îÄ Yes\r\n      ‚Üì\r\n      OpenAI model?\r\n      ‚îú‚îÄ Yes ‚Üí Use TikToken\r\n      ‚îî‚îÄ No\r\n          ‚Üì\r\n          HuggingFace model?\r\n          ‚îú‚îÄ Yes ‚Üí Use HuggingFace (with use_fast=true)\r\n          ‚îî‚îÄ No ‚Üí Use Heuristic with tuned chars_per_token\n\nPerformance Testing Script\n#!/usr/bin/env python3\n&quot;&quot;&quot;\nPerformance testing script for tokenizer adapters.\n&quot;&quot;&quot;\n \nimport time\nimport statistics\nfrom kano_backlog_core.tokenizer import get_default_registry\n \ndef benchmark_adapter(adapter_name, model_name, text, iterations=10):\n    &quot;&quot;&quot;Benchmark a specific adapter.&quot;&quot;&quot;\n    registry = get_default_registry()\n    \n    try:\n        adapter = registry.resolve(adapter_name, model_name=model_name)\n        \n        # Warmup\n        adapter.count_tokens(&quot;warmup&quot;)\n        \n        # Benchmark\n        times = []\n        token_counts = []\n        \n        for _ in range(iterations):\n            start = time.perf_counter()\n            result = adapter.count_tokens(text)\n            end = time.perf_counter()\n            \n            times.append((end - start) * 1000)  # Convert to ms\n            token_counts.append(result.count)\n        \n        return {\n            &#039;adapter&#039;: adapter_name,\n            &#039;avg_time_ms&#039;: statistics.mean(times),\n            &#039;median_time_ms&#039;: statistics.median(times),\n            &#039;min_time_ms&#039;: min(times),\n            &#039;max_time_ms&#039;: max(times),\n            &#039;std_time_ms&#039;: statistics.stdev(times) if len(times) &gt; 1 else 0,\n            &#039;avg_tokens&#039;: statistics.mean(token_counts),\n            &#039;consistent&#039;: len(set(token_counts)) == 1,\n            &#039;is_exact&#039;: result.is_exact,\n            &#039;success&#039;: True\n        }\n    except Exception as e:\n        return {\n            &#039;adapter&#039;: adapter_name,\n            &#039;success&#039;: False,\n            &#039;error&#039;: str(e)\n        }\n \ndef main():\n    # Test configuration\n    test_text = &quot;Your typical document content here. &quot; * 50  # Adjust size\n    model_name = &quot;gpt-4&quot;  # Adjust for your use case\n    adapters = [&quot;heuristic&quot;, &quot;tiktoken&quot;, &quot;huggingface&quot;]\n    iterations = 20\n    \n    print(f&quot;Benchmarking tokenizer adapters&quot;)\n    print(f&quot;Text length: {len(test_text)} characters&quot;)\n    print(f&quot;Iterations: {iterations}&quot;)\n    print(f&quot;Model: {model_name}&quot;)\n    print(&quot;-&quot; * 60)\n    \n    results = []\n    for adapter in adapters:\n        print(f&quot;Testing {adapter}...&quot;)\n        result = benchmark_adapter(adapter, model_name, test_text, iterations)\n        results.append(result)\n        \n        if result[&#039;success&#039;]:\n            print(f&quot;  Average: {result[&#039;avg_time_ms&#039;]:.2f}ms&quot;)\n            print(f&quot;  Range: {result[&#039;min_time_ms&#039;]:.2f}-{result[&#039;max_time_ms&#039;]:.2f}ms&quot;)\n            print(f&quot;  Tokens: {result[&#039;avg_tokens&#039;]:.0f}&quot;)\n            print(f&quot;  Exact: {result[&#039;is_exact&#039;]}&quot;)\n            print(f&quot;  Consistent: {result[&#039;consistent&#039;]}&quot;)\n        else:\n            print(f&quot;  Failed: {result[&#039;error&#039;]}&quot;)\n        print()\n    \n    # Performance ranking\n    successful = [r for r in results if r[&#039;success&#039;]]\n    if successful:\n        print(&quot;Performance Ranking (by speed):&quot;)\n        by_speed = sorted(successful, key=lambda x: x[&#039;avg_time_ms&#039;])\n        for i, result in enumerate(by_speed, 1):\n            print(f&quot;{i}. {result[&#039;adapter&#039;]}: {result[&#039;avg_time_ms&#039;]:.2f}ms&quot;)\n \nif __name__ == &quot;__main__&quot;:\n    main()\nConfiguration Optimization\nHeuristic Adapter Tuning\nOptimize chars_per_token for Your Content:\n# Test different ratios with your content\ntest_text=&quot;$(cat your_typical_document.txt)&quot;\n \necho &quot;Testing chars_per_token ratios:&quot;\nfor ratio in 3.0 3.5 4.0 4.5 5.0; do\n    export KANO_TOKENIZER_HEURISTIC_CHARS_PER_TOKEN=$ratio\n    \n    # Compare with exact adapter\n    result=$(kano-backlog tokenizer compare &quot;$test_text&quot; --adapters heuristic,tiktoken --format json 2&gt;/dev/null)\n    \n    if [ $? -eq 0 ]; then\n        heuristic_tokens=$(echo $result | jq -r &#039;.results[] | select(.adapter==&quot;heuristic&quot;) | .count&#039;)\n        tiktoken_tokens=$(echo $result | jq -r &#039;.results[] | select(.adapter==&quot;tiktoken&quot;) | .count&#039;)\n        \n        if [ &quot;$tiktoken_tokens&quot; != &quot;null&quot; ] &amp;&amp; [ &quot;$heuristic_tokens&quot; != &quot;null&quot; ]; then\n            accuracy=$(echo &quot;scale=2; 100 - (($heuristic_tokens - $tiktoken_tokens) / $tiktoken_tokens * 100)&quot; | bc -l 2&gt;/dev/null || echo &quot;N/A&quot;)\n            echo &quot;  Ratio $ratio: $heuristic_tokens tokens (accuracy: ${accuracy}%)&quot;\n        fi\n    fi\ndone\nLanguage-Specific Tuning:\n# For English technical documentation\n[tokenizer.heuristic]\nchars_per_token = 4.2\n \n# For code-heavy content\n[tokenizer.heuristic]\nchars_per_token = 3.8\n \n# For academic papers (lots of punctuation)\n[tokenizer.heuristic]\nchars_per_token = 4.5\n \n# For CJK-heavy content\n[tokenizer.heuristic]\nchars_per_token = 2.0  # CJK characters are typically 1 token each\nTikToken Optimization\nEncoding Selection:\n# Test different encodings for your model\nencodings=(&quot;cl100k_base&quot; &quot;p50k_base&quot;)\n \nfor encoding in &quot;${encodings[@]}&quot;; do\n    export KANO_TOKENIZER_TIKTOKEN_ENCODING=$encoding\n    echo &quot;Testing encoding: $encoding&quot;\n    \n    time kano-backlog tokenizer test --adapter tiktoken --text &quot;$(cat test_document.txt)&quot;\ndone\n \n# Let system auto-detect (usually optimal)\nunset KANO_TOKENIZER_TIKTOKEN_ENCODING\nHuggingFace Optimization\nFast Tokenizer Configuration:\n[tokenizer.huggingface]\nuse_fast = true  # Significant speed improvement\ntrust_remote_code = false  # Security best practice\nModel Selection for Performance:\n# Compare model performance\nmodels=(\n    &quot;distilbert-base-uncased&quot;  # Smaller, faster\n    &quot;bert-base-uncased&quot;        # Standard\n    &quot;bert-large-uncased&quot;       # Larger, slower\n)\n \nfor model in &quot;${models[@]}&quot;; do\n    echo &quot;Testing model: $model&quot;\n    kano-backlog tokenizer benchmark \\\n      --adapters huggingface \\\n      --model &quot;$model&quot; \\\n      --iterations 5\ndone\nSystem-Level Optimization\nPython Environment Optimization\nPython Version:\n# Use Python 3.11+ for better performance\npython --version\n \n# If using older Python, consider upgrading\npyenv install 3.11.0\npyenv local 3.11.0\nPackage Optimization:\n# Keep packages updated\npip install --upgrade tiktoken transformers\n \n# Use optimized builds if available\npip install --upgrade --force-reinstall tiktoken\n \n# For HuggingFace, consider torch optimizations\npip install torch --index-url download.pytorch.org/whl/cpu  # CPU-only\nMemory Optimization\nHuggingFace Cache Management:\n# Set cache location to fast storage\nexport HF_HOME=/path/to/fast/ssd/cache\n \n# Limit cache size\nexport HF_DATASETS_CACHE=/path/to/cache\nexport TRANSFORMERS_CACHE=/path/to/cache\n \n# Pre-download models to avoid runtime delays\npython -c &quot;\nfrom transformers import AutoTokenizer\nmodels = [&#039;bert-base-uncased&#039;, &#039;distilbert-base-uncased&#039;]\nfor model in models:\n    print(f&#039;Downloading {model}...&#039;)\n    AutoTokenizer.from_pretrained(model)\nprint(&#039;All models cached&#039;)\n&quot;\nMemory Limits:\n# Monitor and limit memory usage\nimport resource\nimport psutil\n \ndef set_memory_limit(max_memory_mb):\n    &quot;&quot;&quot;Set maximum memory usage.&quot;&quot;&quot;\n    max_memory_bytes = max_memory_mb * 1024 * 1024\n    resource.setrlimit(resource.RLIMIT_AS, (max_memory_bytes, max_memory_bytes))\n \ndef check_memory_usage():\n    &quot;&quot;&quot;Check current memory usage.&quot;&quot;&quot;\n    process = psutil.Process()\n    memory_mb = process.memory_info().rss / 1024 / 1024\n    return memory_mb\n \n# Set 1GB limit\nset_memory_limit(1024)\nDisk I/O Optimization\nSSD Configuration:\n# Use SSD for caches\nexport HF_HOME=/mnt/ssd/huggingface_cache\nexport PIP_CACHE_DIR=/mnt/ssd/pip_cache\n \n# Pre-warm caches\nkano-backlog tokenizer test --adapter huggingface --model bert-base-uncased\nNetwork Optimization:\n# Use CDN/mirror if available\nexport HF_ENDPOINT=hf-mirror.com\n \n# Parallel downloads\nexport HF_HUB_ENABLE_HF_TRANSFER=1\npip install hf-transfer\nUse Case Specific Tuning\nHigh-Throughput Processing\nConfiguration for Maximum Throughput:\n[tokenizer]\nadapter = &quot;heuristic&quot;\nfallback_chain = [&quot;heuristic&quot;]\n \n[tokenizer.heuristic]\nchars_per_token = 4.0  # Tune based on accuracy requirements\nBatch Processing Optimization:\ndef process_documents_batch(documents, adapter):\n    &quot;&quot;&quot;Process multiple documents efficiently.&quot;&quot;&quot;\n    # Pre-warm adapter\n    adapter.count_tokens(&quot;warmup&quot;)\n    \n    results = []\n    for doc in documents:\n        # Process without re-initializing adapter\n        result = adapter.count_tokens(doc)\n        results.append(result)\n    \n    return results\n \n# Usage\nfrom kano_backlog_core.tokenizer import get_default_registry\n \nregistry = get_default_registry()\nadapter = registry.resolve(&quot;heuristic&quot;, model_name=&quot;default&quot;)\n \n# Process batch\ndocuments = [&quot;doc1&quot;, &quot;doc2&quot;, &quot;doc3&quot;]  # Your documents\nresults = process_documents_batch(documents, adapter)\nReal-Time Processing\nLow-Latency Configuration:\n[tokenizer]\nadapter = &quot;heuristic&quot;\nfallback_chain = [&quot;heuristic&quot;]\n \n[tokenizer.heuristic]\nchars_per_token = 4.0\nPre-Initialization:\n# Initialize adapters at startup\nclass TokenizerService:\n    def __init__(self):\n        from kano_backlog_core.tokenizer import get_default_registry\n        self.registry = get_default_registry()\n        \n        # Pre-initialize common adapters\n        self.heuristic = self.registry.resolve(&quot;heuristic&quot;, model_name=&quot;default&quot;)\n        self.tiktoken = None\n        \n        try:\n            self.tiktoken = self.registry.resolve(&quot;tiktoken&quot;, model_name=&quot;gpt-4&quot;)\n        except:\n            pass  # TikToken not available\n    \n    def count_tokens(self, text, prefer_exact=False):\n        &quot;&quot;&quot;Count tokens with pre-initialized adapters.&quot;&quot;&quot;\n        if prefer_exact and self.tiktoken:\n            return self.tiktoken.count_tokens(text)\n        else:\n            return self.heuristic.count_tokens(text)\n \n# Initialize once at startup\ntokenizer_service = TokenizerService()\nAccuracy-Critical Applications\nConfiguration for Maximum Accuracy:\n[tokenizer]\nadapter = &quot;auto&quot;\nfallback_chain = [&quot;tiktoken&quot;, &quot;huggingface&quot;, &quot;heuristic&quot;]\n \n[tokenizer.huggingface]\nuse_fast = true\ntrust_remote_code = false\nAccuracy Validation:\ndef validate_tokenization_accuracy(text, model_name):\n    &quot;&quot;&quot;Validate tokenization accuracy across adapters.&quot;&quot;&quot;\n    from kano_backlog_core.tokenizer import get_default_registry\n    \n    registry = get_default_registry()\n    results = {}\n    \n    # Test available adapters\n    for adapter_name in [&quot;tiktoken&quot;, &quot;huggingface&quot;, &quot;heuristic&quot;]:\n        try:\n            adapter = registry.resolve(adapter_name, model_name=model_name)\n            result = adapter.count_tokens(text)\n            results[adapter_name] = {\n                &#039;count&#039;: result.count,\n                &#039;is_exact&#039;: result.is_exact,\n                &#039;method&#039;: result.method\n            }\n        except:\n            continue\n    \n    # Compare results\n    exact_results = {k: v for k, v in results.items() if v[&#039;is_exact&#039;]}\n    if len(exact_results) &gt; 1:\n        counts = [v[&#039;count&#039;] for v in exact_results.values()]\n        if len(set(counts)) &gt; 1:\n            print(f&quot;Warning: Exact adapters disagree on token count: {exact_results}&quot;)\n    \n    return results\nResource-Constrained Environments\nMinimal Resource Configuration:\n[tokenizer]\nadapter = &quot;heuristic&quot;\nfallback_chain = [&quot;heuristic&quot;]\n \n[tokenizer.heuristic]\nchars_per_token = 4.0\nMemory-Efficient Processing:\ndef process_large_document_chunked(file_path, chunk_size=10000):\n    &quot;&quot;&quot;Process large documents in chunks to limit memory usage.&quot;&quot;&quot;\n    from kano_backlog_core.tokenizer import get_default_registry\n    \n    registry = get_default_registry()\n    adapter = registry.resolve(&quot;heuristic&quot;, model_name=&quot;default&quot;)\n    \n    total_tokens = 0\n    with open(file_path, &#039;r&#039;) as f:\n        while True:\n            chunk = f.read(chunk_size)\n            if not chunk:\n                break\n            \n            result = adapter.count_tokens(chunk)\n            total_tokens += result.count\n    \n    return total_tokens\nMonitoring and Profiling\nPerformance Monitoring\nBuilt-in Metrics:\n# Regular performance checks\nkano-backlog tokenizer benchmark --format json &gt; daily_benchmark.json\n \n# Monitor over time\necho &quot;$(date): $(kano-backlog tokenizer benchmark --format json | jq &#039;.results[0].avg_time_ms&#039;)&quot; &gt;&gt; performance_log.txt\nCustom Monitoring:\nimport time\nimport json\nfrom datetime import datetime\nfrom kano_backlog_core.tokenizer import get_default_registry\n \nclass TokenizerMonitor:\n    def __init__(self):\n        self.metrics = []\n    \n    def time_tokenization(self, adapter_name, model_name, text):\n        &quot;&quot;&quot;Time a tokenization operation and record metrics.&quot;&quot;&quot;\n        registry = get_default_registry()\n        adapter = registry.resolve(adapter_name, model_name=model_name)\n        \n        start_time = time.perf_counter()\n        result = adapter.count_tokens(text)\n        end_time = time.perf_counter()\n        \n        metric = {\n            &#039;timestamp&#039;: datetime.now().isoformat(),\n            &#039;adapter&#039;: adapter_name,\n            &#039;model&#039;: model_name,\n            &#039;text_length&#039;: len(text),\n            &#039;token_count&#039;: result.count,\n            &#039;duration_ms&#039;: (end_time - start_time) * 1000,\n            &#039;is_exact&#039;: result.is_exact\n        }\n        \n        self.metrics.append(metric)\n        return result\n    \n    def get_performance_summary(self):\n        &quot;&quot;&quot;Get performance summary statistics.&quot;&quot;&quot;\n        if not self.metrics:\n            return {}\n        \n        by_adapter = {}\n        for metric in self.metrics:\n            adapter = metric[&#039;adapter&#039;]\n            if adapter not in by_adapter:\n                by_adapter[adapter] = []\n            by_adapter[adapter].append(metric[&#039;duration_ms&#039;])\n        \n        summary = {}\n        for adapter, times in by_adapter.items():\n            summary[adapter] = {\n                &#039;count&#039;: len(times),\n                &#039;avg_time_ms&#039;: sum(times) / len(times),\n                &#039;min_time_ms&#039;: min(times),\n                &#039;max_time_ms&#039;: max(times)\n            }\n        \n        return summary\n    \n    def save_metrics(self, filename):\n        &quot;&quot;&quot;Save metrics to file.&quot;&quot;&quot;\n        with open(filename, &#039;w&#039;) as f:\n            json.dump(self.metrics, f, indent=2)\n \n# Usage\nmonitor = TokenizerMonitor()\nmonitor.time_tokenization(&quot;tiktoken&quot;, &quot;gpt-4&quot;, &quot;Sample text&quot;)\nprint(monitor.get_performance_summary())\nProfiling Tools\nCPU Profiling:\n# Install profiling tools\npip install cProfile line_profiler py-spy\n \n# Profile tokenization\npython -m cProfile -o tokenizer.prof -c &quot;\nfrom kano_backlog_core.tokenizer import get_default_registry\nregistry = get_default_registry()\nadapter = registry.resolve(&#039;tiktoken&#039;, model_name=&#039;gpt-4&#039;)\nfor i in range(1000):\n    adapter.count_tokens(&#039;Sample text for profiling&#039;)\n&quot;\n \n# Analyze profile\npython -c &quot;\nimport pstats\nstats = pstats.Stats(&#039;tokenizer.prof&#039;)\nstats.sort_stats(&#039;cumulative&#039;).print_stats(20)\n&quot;\nMemory Profiling:\n# Install memory profiler\npip install memory-profiler\n \n# Profile memory usage\npython -m memory_profiler profile_memory.py\nReal-time Profiling:\n# Install py-spy for real-time profiling\npip install py-spy\n \n# Profile running process\nkano-backlog tokenizer benchmark --iterations 1000 &amp;\nPID=$!\npy-spy record -o profile.svg -p $PID\nProduction Recommendations\nProduction Configuration Template\n# production_tokenizer_config.toml\n[tokenizer]\n# Use auto for intelligent fallback\nadapter = &quot;auto&quot;\n \n# Set production model\nmodel = &quot;text-embedding-3-small&quot;  # Adjust for your use case\n \n# Comprehensive fallback chain\nfallback_chain = [&quot;tiktoken&quot;, &quot;huggingface&quot;, &quot;heuristic&quot;]\n \n# Production-tuned heuristic settings\n[tokenizer.heuristic]\nchars_per_token = 4.0  # Tune based on your content analysis\n \n# TikToken settings\n[tokenizer.tiktoken]\n# Let encoding be auto-detected for reliability\n \n# HuggingFace settings\n[tokenizer.huggingface]\nuse_fast = true           # Enable fast tokenizers\ntrust_remote_code = false # Security best practice\nDeployment Checklist\nPre-Deployment Testing:\n# 1. Validate configuration\nkano-backlog tokenizer validate --config production_config.toml\n \n# 2. Test all adapters\nkano-backlog tokenizer test --config production_config.toml\n \n# 3. Benchmark performance\nkano-backlog tokenizer benchmark --config production_config.toml --iterations 50\n \n# 4. Check dependencies\nkano-backlog tokenizer dependencies --verbose\n \n# 5. Test with production-like data\nkano-backlog tokenizer benchmark --text &quot;$(cat production_sample.txt)&quot; --iterations 20\nProduction Monitoring:\n# Health check script\n#!/bin/bash\n# health_check.sh\n \necho &quot;Tokenizer Health Check - $(date)&quot;\necho &quot;================================&quot;\n \n# System status\nkano-backlog tokenizer status --format json &gt; /tmp/tokenizer_status.json\n \n# Check if any adapters failed\nfailed_adapters=$(cat /tmp/tokenizer_status.json | jq -r &#039;.adapters | to_entries[] | select(.value.available == false) | .key&#039;)\n \nif [ -n &quot;$failed_adapters&quot; ]; then\n    echo &quot;WARNING: Failed adapters: $failed_adapters&quot;\n    exit 1\nelse\n    echo &quot;All adapters healthy&quot;\nfi\n \n# Performance check\navg_time=$(kano-backlog tokenizer benchmark --iterations 5 --format json | jq -r &#039;.results[0].avg_time_ms&#039;)\nif (( $(echo &quot;$avg_time &gt; 100&quot; | bc -l) )); then\n    echo &quot;WARNING: Performance degraded (${avg_time}ms)&quot;\n    exit 1\nelse\n    echo &quot;Performance OK (${avg_time}ms)&quot;\nfi\n \necho &quot;Health check passed&quot;\nScaling Considerations\nHorizontal Scaling:\n# Multi-process tokenization\nfrom multiprocessing import Pool\nfrom kano_backlog_core.tokenizer import get_default_registry\n \ndef tokenize_document(doc_text):\n    &quot;&quot;&quot;Tokenize a single document (for multiprocessing).&quot;&quot;&quot;\n    registry = get_default_registry()\n    adapter = registry.resolve(&quot;tiktoken&quot;, model_name=&quot;gpt-4&quot;)\n    return adapter.count_tokens(doc_text)\n \ndef process_documents_parallel(documents, num_processes=4):\n    &quot;&quot;&quot;Process documents in parallel.&quot;&quot;&quot;\n    with Pool(num_processes) as pool:\n        results = pool.map(tokenize_document, documents)\n    return results\n \n# Usage\ndocuments = [&quot;doc1&quot;, &quot;doc2&quot;, &quot;doc3&quot;]  # Your documents\nresults = process_documents_parallel(documents)\nLoad Balancing:\n# Round-robin adapter selection for load balancing\nclass LoadBalancedTokenizer:\n    def __init__(self):\n        from kano_backlog_core.tokenizer import get_default_registry\n        self.registry = get_default_registry()\n        self.adapters = [&quot;tiktoken&quot;, &quot;heuristic&quot;]  # Available adapters\n        self.current = 0\n    \n    def count_tokens(self, text, model_name=&quot;gpt-4&quot;):\n        &quot;&quot;&quot;Count tokens with load balancing.&quot;&quot;&quot;\n        adapter_name = self.adapters[self.current]\n        self.current = (self.current + 1) % len(self.adapters)\n        \n        try:\n            adapter = self.registry.resolve(adapter_name, model_name=model_name)\n            return adapter.count_tokens(text)\n        except:\n            # Fallback to heuristic\n            adapter = self.registry.resolve(&quot;heuristic&quot;, model_name=model_name)\n            return adapter.count_tokens(text)\nPerformance Alerting\nPerformance Degradation Detection:\nimport json\nimport time\nfrom datetime import datetime, timedelta\n \nclass PerformanceAlert:\n    def __init__(self, baseline_ms=50, threshold_factor=2.0):\n        self.baseline_ms = baseline_ms\n        self.threshold_factor = threshold_factor\n        self.recent_times = []\n    \n    def record_time(self, duration_ms):\n        &quot;&quot;&quot;Record a tokenization time.&quot;&quot;&quot;\n        self.recent_times.append({\n            &#039;time&#039;: datetime.now(),\n            &#039;duration_ms&#039;: duration_ms\n        })\n        \n        # Keep only recent times (last hour)\n        cutoff = datetime.now() - timedelta(hours=1)\n        self.recent_times = [t for t in self.recent_times if t[&#039;time&#039;] &gt; cutoff]\n        \n        # Check for performance degradation\n        if len(self.recent_times) &gt;= 10:\n            recent_avg = sum(t[&#039;duration_ms&#039;] for t in self.recent_times[-10:]) / 10\n            if recent_avg &gt; self.baseline_ms * self.threshold_factor:\n                self.alert_performance_degradation(recent_avg)\n    \n    def alert_performance_degradation(self, current_avg):\n        &quot;&quot;&quot;Alert on performance degradation.&quot;&quot;&quot;\n        print(f&quot;ALERT: Performance degraded!&quot;)\n        print(f&quot;Baseline: {self.baseline_ms}ms&quot;)\n        print(f&quot;Current average: {current_avg:.1f}ms&quot;)\n        print(f&quot;Degradation factor: {current_avg / self.baseline_ms:.1f}x&quot;)\n        \n        # In production, send to monitoring system\n        # send_alert_to_monitoring_system(...)\n \n# Usage\nalert_system = PerformanceAlert(baseline_ms=20, threshold_factor=3.0)\n \n# Record times during operation\nstart = time.perf_counter()\n# ... tokenization operation ...\nend = time.perf_counter()\nalert_system.record_time((end - start) * 1000)\n\nThis performance tuning guide provides comprehensive strategies for optimizing tokenizer adapter performance across different use cases and environments. Regular benchmarking and monitoring will help maintain optimal performance in production systems."},"skill/docs/tokenizer-quickstart":{"title":"tokenizer-quickstart","links":["skill/docs/tokenizer-adapters","skill/docs/tokenizer-configuration","skill/docs/tokenizer-troubleshooting","skill/docs/tokenizer-performance"],"tags":[],"content":"Tokenizer Adapters Quick Start Guide\nGet started with tokenizer adapters in 5 minutes\nThis guide gets you up and running with tokenizer adapters quickly, covering the most common use cases.\n1. Check System Status (30 seconds)\n# Check overall health\nkano-backlog tokenizer status\n \n# Check which adapters are available\nkano-backlog tokenizer adapter-status\nExpected Output:\n‚úÖ Overall Health: HEALTHY\r\nüêç Python Version: 3.11.0 ‚úÖ\r\n\r\n## Configuration\r\n- Adapter: auto\r\n- Model: text-embedding-3-small\r\n- Max Tokens: auto\r\n- Fallback Chain: tiktoken ‚Üí huggingface ‚Üí heuristic\r\n\r\n## Adapter Status\r\n‚úÖ HEURISTIC\r\n- Status: Available\r\n- Dependencies: Ready\r\n\r\n‚úÖ TIKTOKEN  \r\n- Status: Available\r\n- Dependencies: Ready\r\n\r\n‚ùå HUGGINGFACE\r\n- Status: Not available\r\n- Error: No module named &#039;transformers&#039;\n\n2. Install Missing Dependencies (1-2 minutes)\nBased on your status check, install missing dependencies:\n# For OpenAI models (recommended)\npip install tiktoken\n \n# For HuggingFace models (optional)\npip install transformers\n \n# Verify installation\nkano-backlog tokenizer dependencies\n3. Test Basic Functionality (30 seconds)\n# Test with sample text\nkano-backlog tokenizer test\n \n# Test specific adapter\nkano-backlog tokenizer test --adapter tiktoken --model gpt-4\n \n# Compare adapters\nkano-backlog tokenizer compare &quot;This is a sample text for tokenization testing.&quot;\nExpected Output:\nTesting tokenizers with text: &#039;This is a sample text for tokenization testing.&#039;\r\nText length: 58 characters\r\n\r\n‚úì HEURISTIC Adapter:\r\n  Token count: 14\r\n  Method: heuristic\r\n  Tokenizer ID: heuristic:text-embedding-3-small:chars_4.0\r\n  Is exact: False\r\n  Max tokens: 8192\r\n\r\n‚úì TIKTOKEN Adapter:\r\n  Token count: 12\r\n  Method: tiktoken\r\n  Tokenizer ID: tiktoken:text-embedding-3-small:cl100k_base\r\n  Is exact: True\r\n  Max tokens: 8192\n\n4. Choose Your Configuration (1 minute)\nOption A: OpenAI Models (Recommended)\n# Create configuration for OpenAI models\nkano-backlog tokenizer create-example --output openai_config.toml\n \n# Edit the file to use tiktoken\ncat &gt; openai_config.toml &lt;&lt; &#039;EOF&#039;\n[tokenizer]\nadapter = &quot;tiktoken&quot;\nmodel = &quot;text-embedding-3-small&quot;\nfallback_chain = [&quot;tiktoken&quot;, &quot;heuristic&quot;]\n \n[tokenizer.tiktoken]\n# encoding auto-detected\n \n[tokenizer.heuristic]\nchars_per_token = 4.0\nEOF\n \n# Test configuration\nkano-backlog tokenizer validate --config openai_config.toml\nkano-backlog tokenizer test --config openai_config.toml\nOption B: HuggingFace Models\n# Create configuration for HuggingFace models\ncat &gt; huggingface_config.toml &lt;&lt; &#039;EOF&#039;\n[tokenizer]\nadapter = &quot;huggingface&quot;\nmodel = &quot;sentence-transformers/all-MiniLM-L6-v2&quot;\nfallback_chain = [&quot;huggingface&quot;, &quot;heuristic&quot;]\n \n[tokenizer.huggingface]\nuse_fast = true\ntrust_remote_code = false\n \n[tokenizer.heuristic]\nchars_per_token = 4.0\nEOF\n \n# Test configuration\nkano-backlog tokenizer validate --config huggingface_config.toml\nkano-backlog tokenizer test --config huggingface_config.toml\nOption C: Development/Fast Setup\n# Use heuristic adapter (no dependencies required)\nexport KANO_TOKENIZER_ADAPTER=heuristic\nkano-backlog tokenizer test\n5. Integration with Embedding Pipeline (30 seconds)\n# Use tokenizer in embedding commands\nkano-backlog embedding build --tokenizer-adapter tiktoken --tokenizer-model gpt-4\n \n# Or set environment variables\nexport KANO_TOKENIZER_ADAPTER=tiktoken\nexport KANO_TOKENIZER_MODEL=gpt-4\nkano-backlog embedding build\nCommon Use Cases\nUse Case 1: OpenAI API Integration\nGoal: Accurate token counting for OpenAI API cost estimation.\nSetup:\npip install tiktoken\nexport KANO_TOKENIZER_ADAPTER=tiktoken\nexport KANO_TOKENIZER_MODEL=gpt-4\nTest:\nkano-backlog tokenizer test --text &quot;Your API request text here&quot;\nUse Case 2: HuggingFace Model Processing\nGoal: Exact tokenization for HuggingFace embedding models.\nSetup:\npip install transformers\nexport KANO_TOKENIZER_ADAPTER=huggingface\nexport KANO_TOKENIZER_MODEL=sentence-transformers/all-MiniLM-L6-v2\nTest:\nkano-backlog tokenizer test --text &quot;Your document text here&quot;\nUse Case 3: Development/Testing\nGoal: Fast tokenization without external dependencies.\nSetup:\nexport KANO_TOKENIZER_ADAPTER=heuristic\nexport KANO_TOKENIZER_HEURISTIC_CHARS_PER_TOKEN=4.0\nTest:\nkano-backlog tokenizer test --text &quot;Development test text&quot;\nUse Case 4: Production Multi-Model\nGoal: Reliable tokenization with fallback for multiple model types.\nSetup:\npip install tiktoken transformers\n \ncat &gt; production_config.toml &lt;&lt; &#039;EOF&#039;\n[tokenizer]\nadapter = &quot;auto&quot;\nmodel = &quot;text-embedding-3-small&quot;\nfallback_chain = [&quot;tiktoken&quot;, &quot;huggingface&quot;, &quot;heuristic&quot;]\n \n[tokenizer.heuristic]\nchars_per_token = 4.0\n \n[tokenizer.tiktoken]\n# encoding auto-detected\n \n[tokenizer.huggingface]\nuse_fast = true\ntrust_remote_code = false\nEOF\nTest:\nkano-backlog tokenizer validate --config production_config.toml\nkano-backlog tokenizer test --config production_config.toml\nQuick Troubleshooting\nProblem: ‚ÄúNo tokenizer adapter available‚Äù\nSolution:\n# Use heuristic adapter as fallback\nexport KANO_TOKENIZER_ADAPTER=heuristic\nkano-backlog tokenizer test\nProblem: ‚Äútiktoken package required‚Äù\nSolution:\npip install tiktoken\nkano-backlog tokenizer test --adapter tiktoken\nProblem: ‚Äútransformers package required‚Äù\nSolution:\npip install transformers\nkano-backlog tokenizer test --adapter huggingface\nProblem: Token counts seem inaccurate\nSolution:\n# Compare adapters to see differences\nkano-backlog tokenizer compare &quot;Your text here&quot;\n \n# Use exact adapter for your model type\nkano-backlog tokenizer recommend your-model-name\nProblem: Slow performance\nSolution:\n# Use heuristic adapter for speed\nexport KANO_TOKENIZER_ADAPTER=heuristic\n \n# Or enable fast tokenizers for HuggingFace\nexport KANO_TOKENIZER_HUGGINGFACE_USE_FAST=true\nNext Steps\nLearn More\n\nComplete Documentation - Comprehensive user guide\nConfiguration Reference - All configuration options\nTroubleshooting Guide - Detailed problem solving\nPerformance Tuning - Optimization strategies\n\nAdvanced Usage\n# Benchmark your setup\nkano-backlog tokenizer benchmark --text &quot;$(cat your_typical_document.txt)&quot;\n \n# Get model recommendations\nkano-backlog tokenizer recommend gpt-4\nkano-backlog tokenizer recommend bert-base-uncased\n \n# Monitor system health\nkano-backlog tokenizer status --verbose\n \n# Create custom configuration\nkano-backlog tokenizer create-example --output my_config.toml\n# Edit my_config.toml for your needs\nkano-backlog tokenizer validate --config my_config.toml\nProduction Deployment\n\nChoose Configuration: Create appropriate config file for your environment\nValidate Setup: Run kano-backlog tokenizer status and kano-backlog tokenizer test\nPerformance Test: Run kano-backlog tokenizer benchmark with your typical content\nMonitor Health: Set up regular kano-backlog tokenizer status checks\nUpdate Dependencies: Keep tiktoken and transformers packages updated\n\n\nYou‚Äôre now ready to use tokenizer adapters!\nFor detailed information on any topic, see the complete documentation linked above."},"skill/docs/tokenizer-troubleshooting":{"title":"tokenizer-troubleshooting","links":[],"tags":[],"content":"Tokenizer Adapters Troubleshooting Guide\nComprehensive troubleshooting for tokenizer adapter issues\nThis guide provides detailed solutions for common tokenizer adapter problems, error messages, and configuration issues.\nTable of Contents\n\nQuick Diagnosis\nCommon Error Messages\nDependency Issues\nConfiguration Problems\nPerformance Issues\nModel-Specific Issues\nEnvironment Problems\nAdvanced Troubleshooting\n\nQuick Diagnosis\nStep 1: Check System Health\n# Get overall system status\nkano-backlog tokenizer status\n \n# Check what&#039;s broken\nkano-backlog tokenizer diagnose --verbose\n \n# Check dependencies\nkano-backlog tokenizer dependencies\nStep 2: Test Basic Functionality\n# Test with simple text\nkano-backlog tokenizer test --text &quot;Hello world&quot;\n \n# Test specific adapter\nkano-backlog tokenizer test --adapter heuristic --text &quot;Hello world&quot;\n \n# Compare adapters\nkano-backlog tokenizer compare &quot;Hello world&quot;\nStep 3: Check Configuration\n# Validate configuration\nkano-backlog tokenizer validate\n \n# Show effective configuration\nkano-backlog tokenizer config --format json\n \n# Check environment variables\nkano-backlog tokenizer env\nCommon Error Messages\n1. ‚ÄúNo tokenizer adapter available‚Äù\nFull Error:\nFallbackChainExhaustedError: No tokenizer available. Errors: [tiktoken: No module named &#039;tiktoken&#039;, huggingface: No module named &#039;transformers&#039;, heuristic: &lt;error&gt;]\n\nCause: All adapters in the fallback chain failed to initialize.\nSolutions:\nQuick Fix - Use Heuristic:\nexport KANO_TOKENIZER_ADAPTER=heuristic\nkano-backlog tokenizer test\nInstall Dependencies:\n# For OpenAI models\npip install tiktoken\n \n# For HuggingFace models  \npip install transformers\n \n# Verify installation\nkano-backlog tokenizer dependencies\nCheck Python Environment:\n# Ensure correct environment\nwhich python\npip list | grep -E &quot;(tiktoken|transformers)&quot;\n \n# If using conda\nconda list | grep -E &quot;(tiktoken|transformers)&quot;\n2. ‚Äútiktoken package required‚Äù\nFull Error:\nImportError: tiktoken package required for TiktokenAdapter. Install with: pip install tiktoken\n\nCause: TikToken dependency not installed.\nSolutions:\nInstall TikToken:\npip install tiktoken\n \n# Verify installation\npython -c &quot;import tiktoken; print(&#039;TikToken version:&#039;, tiktoken.__version__)&quot;\n \n# Test adapter\nkano-backlog tokenizer health tiktoken\nAlternative Installation Methods:\n# Using conda\nconda install tiktoken -c conda-forge\n \n# Using pip with specific version\npip install &quot;tiktoken&gt;=0.5.0&quot;\n \n# In requirements.txt\necho &quot;tiktoken&gt;=0.5.0&quot; &gt;&gt; requirements.txt\npip install -r requirements.txt\nIf Installation Fails:\n# Check pip version\npip --version\n \n# Upgrade pip\npip install --upgrade pip\n \n# Try with --user flag\npip install --user tiktoken\n \n# Check for conflicts\npip check\n3. ‚Äútransformers package required‚Äù\nFull Error:\nImportError: transformers package required for HuggingFaceAdapter\n\nCause: HuggingFace transformers dependency not installed.\nSolutions:\nInstall Transformers:\npip install transformers\n \n# For sentence-transformers models\npip install sentence-transformers\n \n# Verify installation\npython -c &quot;import transformers; print(&#039;Transformers version:&#039;, transformers.__version__)&quot;\nHandle Large Installation:\n# Transformers is large, ensure enough space\ndf -h\n \n# Install with progress bar\npip install transformers --progress-bar pretty\n \n# Install minimal version (if available)\npip install transformers[torch]\n4. ‚ÄúCan‚Äôt load tokenizer for model‚Äù\nFull Error:\nOSError: Can&#039;t load tokenizer for &#039;unknown-model&#039;. Make sure that &#039;unknown-model&#039; is a correct model identifier\n\nCause: Invalid or unsupported model name.\nSolutions:\nCheck Supported Models:\n# List all supported models\nkano-backlog tokenizer list-models\n \n# Check specific adapter models\nkano-backlog tokenizer list-models --adapter huggingface\nkano-backlog tokenizer list-models --adapter tiktoken\nUse Valid Model Names:\n# For HuggingFace models\nkano-backlog tokenizer test --adapter huggingface --model bert-base-uncased\n \n# For OpenAI models\nkano-backlog tokenizer test --adapter tiktoken --model gpt-4\nGet Model Recommendations:\n# Get recommendation for your use case\nkano-backlog tokenizer recommend bert-base-uncased\nkano-backlog tokenizer recommend gpt-4\n5. ‚ÄúConfiguration validation failed‚Äù\nFull Error:\nConfigError: heuristic.chars_per_token must be a positive number\n\nCause: Invalid configuration values.\nSolutions:\nFix Configuration File:\n[tokenizer.heuristic]\nchars_per_token = 4.0  # Must be positive number, not string\n \n[tokenizer.huggingface]\nuse_fast = true        # Must be boolean, not &quot;true&quot;\ntrust_remote_code = false  # Must be boolean, not &quot;false&quot;\nValidate Configuration:\n# Check configuration syntax\nkano-backlog tokenizer validate --config your_config.toml\n \n# Show parsed configuration\nkano-backlog tokenizer config --config your_config.toml --format json\nFix Environment Variables:\n# Correct format\nexport KANO_TOKENIZER_HEURISTIC_CHARS_PER_TOKEN=4.0\nexport KANO_TOKENIZER_HUGGINGFACE_USE_FAST=true\n \n# Not these formats\n# export KANO_TOKENIZER_HEURISTIC_CHARS_PER_TOKEN=&quot;4.0&quot;\n# export KANO_TOKENIZER_HUGGINGFACE_USE_FAST=&quot;True&quot;\n6. ‚ÄúTokenization failed‚Äù\nFull Error:\nTokenizationFailedError: Tokenization failed for adapter &#039;tiktoken&#039; with model &#039;gpt-4&#039;: &lt;specific error&gt;\n\nCause: Runtime error during tokenization.\nSolutions:\nCheck Text Content:\n# Test with simple text first\nkano-backlog tokenizer test --adapter tiktoken --text &quot;Hello world&quot;\n \n# Check for problematic characters\npython -c &quot;\ntext = &#039;&#039;&#039;your problematic text here&#039;&#039;&#039;\nprint(&#039;Text length:&#039;, len(text))\nprint(&#039;Non-ASCII chars:&#039;, [c for c in text if ord(c) &gt; 127])\nprint(&#039;Control chars:&#039;, [c for c in text if ord(c) &lt; 32])\n&quot;\nTry Different Adapters:\n# Compare results\nkano-backlog tokenizer compare &quot;your problematic text&quot;\n \n# Use fallback\nexport KANO_TOKENIZER_ADAPTER=heuristic\nkano-backlog tokenizer test --text &quot;your problematic text&quot;\nDependency Issues\nTikToken Installation Problems\nIssue: Compilation Errors\n# Error during pip install tiktoken\n# Building wheel for tiktoken (pyproject.toml) ... error\nSolutions:\n# Install build dependencies\npip install --upgrade pip setuptools wheel\n \n# Install Rust (required for tiktoken)\ncurl --proto &#039;=https&#039; --tlsv1.2 -sSf sh.rustup.rs | sh\nsource ~/.cargo/env\n \n# Try installation again\npip install tiktoken\n \n# Alternative: use pre-built wheels\npip install --only-binary=all tiktoken\nIssue: Version Conflicts\n# ERROR: pip&#039;s dependency resolver does not currently take into account all the packages that are installed\nSolutions:\n# Check for conflicts\npip check\n \n# Create clean environment\npython -m venv tokenizer_env\nsource tokenizer_env/bin/activate  # On Windows: tokenizer_env\\Scripts\\activate\npip install tiktoken\n \n# Or use conda\nconda create -n tokenizer_env python=3.11\nconda activate tokenizer_env\nconda install tiktoken -c conda-forge\nHuggingFace Transformers Issues\nIssue: Large Download Size\n# Downloading transformers models takes too long or fails\nSolutions:\n# Set cache directory with more space\nexport HF_HOME=/path/to/large/disk/huggingface_cache\n \n# Use offline mode after initial download\nexport TRANSFORMERS_OFFLINE=1\n \n# Download specific model manually\npython -c &quot;\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(&#039;bert-base-uncased&#039;)\nprint(&#039;Model downloaded successfully&#039;)\n&quot;\nIssue: Network/Firewall Problems\n# HTTPSConnectionPool: Max retries exceeded\nSolutions:\n# Use proxy if needed\nexport HTTP_PROXY=http://your-proxy:port\nexport HTTPS_PROXY=http://your-proxy:port\n \n# Use mirror (if available)\nexport HF_ENDPOINT=hf-mirror.com\n \n# Download manually and use local path\n# Download model files to local directory, then:\nkano-backlog tokenizer test --adapter huggingface --model /path/to/local/model\nPython Environment Issues\nIssue: Wrong Python Version\n# Python 3.7 or older not supported\nSolutions:\n# Check Python version\npython --version\n \n# Install Python 3.8+\n# On Ubuntu/Debian:\nsudo apt update\nsudo apt install python3.11\n \n# On macOS with Homebrew:\nbrew install python@3.11\n \n# On Windows: Download from python.org\n \n# Use pyenv for version management\npyenv install 3.11.0\npyenv local 3.11.0\nIssue: Virtual Environment Problems\n# Packages installed but not found\nSolutions:\n# Ensure you&#039;re in the right environment\nwhich python\nwhich pip\n \n# Activate virtual environment\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n \n# Reinstall packages in current environment\npip install --force-reinstall tiktoken transformers\n \n# Check installation location\npip show tiktoken\npip show transformers\nConfiguration Problems\nTOML Syntax Errors\nIssue: Invalid TOML Format\n# toml.decoder.TomlDecodeError: Invalid value\nSolutions:\n# Validate TOML syntax\npython -c &quot;\nimport tomllib\nwith open(&#039;your_config.toml&#039;, &#039;rb&#039;) as f:\n    data = tomllib.load(f)\nprint(&#039;TOML is valid&#039;)\n&quot;\n \n# Common TOML mistakes:\n# Wrong: chars_per_token = &quot;4.0&quot;  (string instead of number)\n# Right: chars_per_token = 4.0\n \n# Wrong: use_fast = &quot;true&quot;  (string instead of boolean)  \n# Right: use_fast = true\n \n# Wrong: fallback_chain = &quot;tiktoken,heuristic&quot;  (string instead of array)\n# Right: fallback_chain = [&quot;tiktoken&quot;, &quot;heuristic&quot;]\nEnvironment Variable Issues\nIssue: Environment Variables Not Working\n# Set KANO_TOKENIZER_ADAPTER=heuristic but still using tiktoken\nSolutions:\n# Check if variables are set\nenv | grep KANO_TOKENIZER\n \n# Ensure proper export\nexport KANO_TOKENIZER_ADAPTER=heuristic  # Not just KANO_TOKENIZER_ADAPTER=heuristic\n \n# Check variable precedence\nkano-backlog tokenizer config --format json | jq &#039;.adapter&#039;\n \n# Clear conflicting variables\nunset KANO_TOKENIZER_ADAPTER\nexport KANO_TOKENIZER_ADAPTER=heuristic\n \n# Test immediately\nkano-backlog tokenizer test\nIssue: Boolean Environment Variables\n# Boolean values not recognized\nSolutions:\n# Correct boolean formats\nexport KANO_TOKENIZER_HUGGINGFACE_USE_FAST=true     # lowercase\nexport KANO_TOKENIZER_HUGGINGFACE_USE_FAST=false    # lowercase\nexport KANO_TOKENIZER_HUGGINGFACE_USE_FAST=1        # numeric\nexport KANO_TOKENIZER_HUGGINGFACE_USE_FAST=0        # numeric\n \n# Not these formats:\n# export KANO_TOKENIZER_HUGGINGFACE_USE_FAST=True   # Wrong case\n# export KANO_TOKENIZER_HUGGINGFACE_USE_FAST=&quot;true&quot; # Quoted\nConfiguration File Location\nIssue: Configuration File Not Found\n# Config file not being loaded\nSolutions:\n# Specify config file explicitly\nkano-backlog tokenizer test --config /path/to/your/config.toml\n \n# Check default locations\nls -la tokenizer_config.toml\nls -la config.toml\nls -la ~/.config/kano/tokenizer.toml\n \n# Create config in current directory\nkano-backlog tokenizer create-example --output tokenizer_config.toml\n \n# Verify config is loaded\nkano-backlog tokenizer config --config tokenizer_config.toml\nPerformance Issues\nSlow Tokenization\nIssue: Tokenization Takes Too Long\nDiagnosis:\n# Benchmark current performance\nkano-backlog tokenizer benchmark --iterations 10\n \n# Test with different adapters\nkano-backlog tokenizer benchmark --adapters heuristic,tiktoken --iterations 10\n \n# Profile with large text\ntime kano-backlog tokenizer test --text &quot;$(cat large_file.txt)&quot;\nSolutions:\nUse Faster Adapter:\n# Heuristic is fastest\nexport KANO_TOKENIZER_ADAPTER=heuristic\n \n# Enable fast tokenizers for HuggingFace\nexport KANO_TOKENIZER_HUGGINGFACE_USE_FAST=true\nOptimize Configuration:\n[tokenizer]\nadapter = &quot;heuristic&quot;  # Fastest option\nfallback_chain = [&quot;heuristic&quot;]  # Skip slow adapters\n \n[tokenizer.heuristic]\nchars_per_token = 4.0  # Tune for your content\nSystem Optimization:\n# Check system resources\ntop\nfree -h\n \n# Close unnecessary applications\n# Ensure adequate RAM for large models\n \n# Use SSD for HuggingFace cache\nexport HF_HOME=/path/to/ssd/cache\nHigh Memory Usage\nIssue: Tokenization Uses Too Much Memory\nDiagnosis:\n# Monitor memory during tokenization\nkano-backlog tokenizer test --text &quot;$(cat large_file.txt)&quot; &amp;\nPID=$!\nwhile kill -0 $PID 2&gt;/dev/null; do\n    ps -o pid,vsz,rss,comm -p $PID\n    sleep 1\ndone\nSolutions:\nUse Memory-Efficient Adapters:\n# Heuristic uses minimal memory\nexport KANO_TOKENIZER_ADAPTER=heuristic\n \n# For HuggingFace, use smaller models\nexport KANO_TOKENIZER_MODEL=distilbert-base-uncased  # Instead of bert-large\nProcess Large Files in Chunks:\n# Instead of processing entire file at once\ndef process_large_file(file_path, chunk_size=10000):\n    with open(file_path, &#039;r&#039;) as f:\n        while True:\n            chunk = f.read(chunk_size)\n            if not chunk:\n                break\n            # Process chunk\n            result = adapter.count_tokens(chunk)\n            yield result\nStartup Time Issues\nIssue: Slow Adapter Initialization\nSolutions:\nUse Lightweight Adapters:\n[tokenizer]\nadapter = &quot;heuristic&quot;  # Instant startup\nfallback_chain = [&quot;heuristic&quot;, &quot;tiktoken&quot;]  # Heuristic first\nPre-warm Adapters:\n# Pre-initialize adapters at application startup\nfrom kano_backlog_core.tokenizer import get_default_registry\n \nregistry = get_default_registry()\n# This will initialize and cache adapters\nadapter = registry.resolve(&quot;tiktoken&quot;, model_name=&quot;gpt-4&quot;)\nadapter.count_tokens(&quot;warmup&quot;)  # Prime the adapter\nModel-Specific Issues\nOpenAI Model Issues\nIssue: Wrong Encoding for OpenAI Model\n# Token counts don&#039;t match OpenAI API\nSolutions:\n# Check model encoding\nkano-backlog tokenizer list-models --adapter tiktoken | grep your-model\n \n# Use correct encoding\nexport KANO_TOKENIZER_TIKTOKEN_ENCODING=cl100k_base  # For GPT-4, GPT-3.5-turbo\nexport KANO_TOKENIZER_TIKTOKEN_ENCODING=p50k_base    # For text-davinci-003\n \n# Let system auto-detect\nunset KANO_TOKENIZER_TIKTOKEN_ENCODING\nkano-backlog tokenizer test --adapter tiktoken --model gpt-4\nIssue: Legacy Model Support\n# Old OpenAI models not recognized\nSolutions:\n# Check supported models\nkano-backlog tokenizer list-models --adapter tiktoken\n \n# Use manual encoding for unsupported models\nexport KANO_TOKENIZER_TIKTOKEN_ENCODING=p50k_base\nkano-backlog tokenizer test --adapter tiktoken --model your-legacy-model\nHuggingFace Model Issues\nIssue: Model Not Found\n# OSError: Can&#039;t load tokenizer for &#039;custom-model&#039;\nSolutions:\n# Check model exists on HuggingFace Hub\ncurl -s &quot;huggingface.co/api/models/your-model-name&quot; | jq .\n \n# Use correct model identifier\nkano-backlog tokenizer test --adapter huggingface --model &quot;organization/model-name&quot;\n \n# For local models, use full path\nkano-backlog tokenizer test --adapter huggingface --model &quot;/path/to/local/model&quot;\nIssue: Slow Model Loading\n# First-time model download is slow\nSolutions:\n# Pre-download model\npython -c &quot;\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(&#039;your-model-name&#039;)\nprint(&#039;Model cached successfully&#039;)\n&quot;\n \n# Use smaller, faster models\n# Instead of: sentence-transformers/all-mpnet-base-v2\n# Use: sentence-transformers/all-MiniLM-L6-v2\n \n# Check model size before downloading\ncurl -s &quot;huggingface.co/api/models/your-model-name&quot; | jq &#039;.siblings[] | select(.rfilename | endswith(&quot;.bin&quot;)) | .size&#039;\nCustom Model Issues\nIssue: Unsupported Model\n# Model not in supported list\nSolutions:\n# Use heuristic adapter for unknown models\nexport KANO_TOKENIZER_ADAPTER=heuristic\nkano-backlog tokenizer test --model your-custom-model\n \n# Add custom model to configuration\n[tokenizer]\nmodel = &quot;your-custom-model&quot;\nmax_tokens = 8192  # Set appropriate limit\n \n[tokenizer.heuristic]\nchars_per_token = 4.0  # Tune for your model&#039;s tokenization\nEnvironment Problems\nDocker/Container Issues\nIssue: Dependencies Not Available in Container\nSolutions:\n# In Dockerfile, install dependencies\nRUN pip install tiktoken transformers\n \n# Or use multi-stage build\nFROM python:3.11-slim as builder\nRUN pip install tiktoken transformers\nFROM python:3.11-slim\nCOPY --from=builder /usr/local/lib/python3.11/site-packages /usr/local/lib/python3.11/site-packages\nIssue: Network Access in Container\n# Can&#039;t download HuggingFace models\nSolutions:\n# Pre-download models during build\nRUN python -c &quot;from transformers import AutoTokenizer; AutoTokenizer.from_pretrained(&#039;bert-base-uncased&#039;)&quot;\n \n# Or mount cache directory\ndocker run -v ~/.cache/huggingface:/root/.cache/huggingface your-image\nCI/CD Issues\nIssue: Tests Fail in CI Environment\nSolutions:\n# In GitHub Actions\n- name: Install dependencies\n  run: |\n    pip install tiktoken transformers\n    \n- name: Cache HuggingFace models\n  uses: actions/cache@v3\n  with:\n    path: ~/.cache/huggingface\n    key: huggingface-${{ hashFiles(&#039;requirements.txt&#039;) }}\n \n- name: Test tokenizers\n  run: |\n    export KANO_TOKENIZER_ADAPTER=heuristic  # Fallback for CI\n    kano-backlog tokenizer test\nWindows-Specific Issues\nIssue: Path Separators\n# Config file paths with backslashes\nSolutions:\n# Use forward slashes or raw strings\nkano-backlog tokenizer validate --config &quot;C:/path/to/config.toml&quot;\n \n# Or use environment variable\nset KANO_TOKENIZER_CONFIG=C:\\path\\to\\config.toml\nkano-backlog tokenizer validate\nIssue: PowerShell Environment Variables\n# Environment variables not persisting\nSolutions:\n# Set environment variables in PowerShell\n$env:KANO_TOKENIZER_ADAPTER = &quot;heuristic&quot;\nkano-backlog tokenizer test\n \n# Or use permanent setting\n[Environment]::SetEnvironmentVariable(&quot;KANO_TOKENIZER_ADAPTER&quot;, &quot;heuristic&quot;, &quot;User&quot;)\nAdvanced Troubleshooting\nDebug Mode\nEnable Verbose Logging:\n# Set log level\nexport PYTHONPATH=.\npython -c &quot;\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\nfrom kano_backlog_core.tokenizer import get_default_registry\nregistry = get_default_registry()\nadapter = registry.resolve(&#039;tiktoken&#039;, model_name=&#039;gpt-4&#039;)\nresult = adapter.count_tokens(&#039;test&#039;)\nprint(result)\n&quot;\nNetwork Debugging\nIssue: Network-Related Failures\nDiagnosis:\n# Test network connectivity\ncurl -I huggingface.co\ncurl -I pypi.org\n \n# Check DNS resolution\nnslookup huggingface.co\nnslookup pypi.org\n \n# Test with proxy\nexport HTTP_PROXY=http://proxy:port\nexport HTTPS_PROXY=http://proxy:port\nkano-backlog tokenizer test --adapter huggingface\nMemory Debugging\nIssue: Memory Leaks or High Usage\nDiagnosis:\n# Monitor memory usage\npip install memory-profiler\n \n# Profile tokenization\npython -c &quot;\nfrom memory_profiler import profile\nfrom kano_backlog_core.tokenizer import get_default_registry\n \n@profile\ndef test_tokenization():\n    registry = get_default_registry()\n    adapter = registry.resolve(&#039;huggingface&#039;, model_name=&#039;bert-base-uncased&#039;)\n    for i in range(100):\n        result = adapter.count_tokens(f&#039;Test text {i}&#039;)\n    return result\n \ntest_tokenization()\n&quot;\nPerformance Profiling\nIssue: Need Detailed Performance Analysis\nTools:\n# Install profiling tools\npip install cProfile line_profiler\n \n# Profile tokenization\npython -m cProfile -o tokenizer_profile.prof -c &quot;\nfrom kano_backlog_core.tokenizer import get_default_registry\nregistry = get_default_registry()\nadapter = registry.resolve(&#039;tiktoken&#039;, model_name=&#039;gpt-4&#039;)\nfor i in range(1000):\n    adapter.count_tokens(&#039;Sample text for profiling&#039;)\n&quot;\n \n# Analyze profile\npython -c &quot;\nimport pstats\nstats = pstats.Stats(&#039;tokenizer_profile.prof&#039;)\nstats.sort_stats(&#039;cumulative&#039;).print_stats(20)\n&quot;\nRecovery Procedures\nComplete Reset:\n# Clear all caches\nrm -rf ~/.cache/huggingface/\nrm -rf ~/.cache/pip/\n \n# Reset environment\nunset $(env | grep KANO_TOKENIZER | cut -d= -f1)\n \n# Reinstall dependencies\npip uninstall tiktoken transformers -y\npip install tiktoken transformers\n \n# Test basic functionality\nkano-backlog tokenizer test --adapter heuristic\nFactory Reset Configuration:\n# Remove custom configuration\nrm -f tokenizer_config.toml\nrm -f ~/.config/kano/tokenizer.toml\n \n# Create fresh configuration\nkano-backlog tokenizer create-example --output tokenizer_config.toml --force\n \n# Validate fresh configuration\nkano-backlog tokenizer validate --config tokenizer_config.toml\nGetting Support\nCollect Diagnostic Information:\n# Create support bundle\nmkdir tokenizer_debug\ncd tokenizer_debug\n \n# System information\nkano-backlog tokenizer status --format json &gt; system_status.json\nkano-backlog tokenizer dependencies --verbose &gt; dependencies.txt\nkano-backlog tokenizer config --format json &gt; config.json\n \n# Test results\nkano-backlog tokenizer test --verbose &gt; test_results.txt 2&gt;&amp;1\nkano-backlog tokenizer benchmark --format json &gt; benchmark.json 2&gt;&amp;1\n \n# Environment\nenv | grep -E &quot;(KANO|PYTHON|PATH)&quot; &gt; environment.txt\npython --version &gt; python_version.txt\npip list &gt; pip_packages.txt\n \n# Create archive\ncd ..\ntar -czf tokenizer_debug.tar.gz tokenizer_debug/\necho &quot;Debug information collected in tokenizer_debug.tar.gz&quot;\nCommon Support Questions:\n\nWhat operating system and Python version are you using?\nWhat tokenizer adapter and model are you trying to use?\nWhat is the exact error message?\nCan you reproduce the issue with the heuristic adapter?\nHave you tried the diagnostic commands?\n\n\nThis troubleshooting guide covers the most common issues encountered with tokenizer adapters. For additional help, run the diagnostic commands and collect the information as shown in the ‚ÄúGetting Support‚Äù section."},"skill/docs/topic":{"title":"topic","links":["skill/docs/workset"],"tags":[],"content":"Topic (Context Grouping)\nTopics provide a higher-level grouping mechanism for rapid context switching when users change focus areas during a conversation.\r\nWhile worksets are per-item execution caches, topics group multiple items and documents into a coherent context bundle.\nGoals\n\nEnable rapid context switching between focus areas\nGroup related items and documents for coherent context loading\nSupport multi-item work sessions without losing context\nProvide deterministic context export for agent consumption\n\nPrinciples\n\nTopics are derived data, but brief.md is deterministic and can be shared/reviewed in-repo\nOne active topic per agent at a time (active pointer is local cache)\nTopics reference items by UID/ID (not copies)\nPinned documents provide additional context beyond items\nmanifest.json is primarily machine-oriented (for agents/tools); capture human decision material in notes.md (and/or pinned docs) so brief.md can remain deterministic\n\nDirectory Layout\nTopics live in the backlog tree so the deterministic brief can be shared:\n_kano/backlog/topics/&lt;topic-name&gt;/\n  manifest.json     # Topic metadata: seed_items, pinned_docs, snippet_refs, timestamps, status\n  brief.md          # Stable, human-maintained brief (do not overwrite automatically)\n  brief.generated.md# Deterministic distilled brief (generated/overwritten by `kano-backlog topic distill`)\n  notes.md          # Human-oriented notes/decision pack (freeform; intended for collaboration)\n  materials/        # Raw collected materials (treated as cache; typically gitignored)\n    clips/\n    links/\n    extracts/\n    logs/\n  synthesis/        # Derived working outputs (optional)\n  publish/          # Prepared write-backs / patch skeletons (optional)\n \n_kano/backlog/.cache/worksets/\n  active_topic.&lt;agent&gt;.txt   # Current active topic for agent\nNotes:\n\nThis demo repo ignores _kano/backlog/topics/**/materials/ by default.\nThe active-topic pointer is per-agent and is not intended to be versioned.\n\nManifest Structure\n{\n  &quot;topic&quot;: &quot;auth-refactor&quot;,\n  &quot;agent&quot;: &quot;kiro&quot;,\n  &quot;seed_items&quot;: [&quot;TASK-0042&quot;, &quot;TASK-0043&quot;, &quot;BUG-0012&quot;],\n  &quot;pinned_docs&quot;: [&quot;_kano/backlog/decisions/ADR-0015.md&quot;],\n  &quot;snippet_refs&quot;: [],\n  &quot;status&quot;: &quot;open&quot;,\n  &quot;closed_at&quot;: null,\n  &quot;created_at&quot;: &quot;2026-01-12T10:00:00Z&quot;,\n  &quot;updated_at&quot;: &quot;2026-01-12T14:30:00Z&quot;\n}\nHuman Decision Materials (Recommended)\nUse notes.md as a lightweight ‚Äúdecision pack‚Äù that is easy for humans to review, while keeping manifest.json reference-first and brief.md deterministic.\nSuggested notes.md sections:\n\nDecision to make: the specific question(s) that need a call\nOptions: candidate approaches, with pros/cons\nEvidence: links to pinned ADRs, key snippets, benchmarks/logs\nRecommendation: proposed choice + rationale + follow-ups\n\nCLI Commands\nAll topic commands are accessed via kano-backlog topic &lt;subcommand&gt;.\nCreate a Topic\nkano-backlog topic create &lt;topic-name&gt; --agent &lt;agent-name&gt; [--no-notes] [--format plain|json]\nCreates a new topic:\n\nValidates topic name (alphanumeric, hyphens, underscores)\nCreates manifest.json with empty seed_items/pinned_docs/snippet_refs\nCreates brief.md template and topic subfolders\nCreates spec/ structure if --with-spec is used (requirements.md, design.md, tasks.md)\n\nCreate a Topic with Spec\nkano-backlog topic create complex-feature --agent kiro --with-spec\nThis generates the Spec Triad (Requirements, Design, Tasks) in a specific spec/ subdirectory, enabling rigorous feature definition (Medium-Term Memory).\nAdd Items to Topic\nkano-backlog topic add &lt;topic-name&gt; --item &lt;id&gt; [--format plain|json]\nAdds a backlog item to the topic:\n\nVerifies item exists in backlog\nAdds item UID to seed_items array\nSkips if item already in topic (idempotent)\nUpdates updated_at timestamp\n\nPin Documents\nkano-backlog topic pin &lt;topic-name&gt; --doc &lt;path&gt; [--format plain|json]\nPins a document to the topic:\n\nVerifies document exists\nAdds path to pinned_docs array\nSkips if already pinned (idempotent)\nSupports relative paths from workspace root\n\nSwitch Active Topic\nkano-backlog topic switch &lt;topic-name&gt; --agent &lt;agent-name&gt; [--format plain|json]\nSwitches the active topic for an agent:\n\nUpdates active_topic.&lt;agent&gt;.txt\nReturns summary (item count, pinned doc count)\nShows previous topic if any\n\nCollect a Code Snippet\nkano-backlog topic add-snippet &lt;topic-name&gt; --file &lt;path&gt; --start &lt;line&gt; --end &lt;line&gt; [--agent &lt;agent&gt;] [--snapshot]\nCollects a reference-first code snippet into the topic manifest:\n\nStores file path + line range + content hash\nOptional --snapshot caches the text in the manifest (still treated as derived)\n\nDistill Deterministic Brief\nkano-backlog topic distill &lt;topic-name&gt;\nGenerates/overwrites brief.generated.md deterministically from the manifest + materials index.\nbrief.md is a stable, human-maintained brief. Use it for curated summaries that should not be overwritten by automation.\nDecision Audit and Write-back\nTopics are a good place to capture and distill decisions, but the durable record must also live in the relevant work items.\r\nThis workflow helps humans verify that topic decisions were written back.\n\nGenerate a decision audit report for a topic:\n\nkano-backlog topic decision-audit &lt;topic-name&gt;\nkano-backlog topic decision-audit &lt;topic-name&gt; --format json\nThis writes a deterministic report to:\n_kano/backlog/topics/&lt;topic-name&gt;/publish/decision-audit.md\n\nWrite back a decision to a work item:\n\nkano workitem add-decision &lt;ITEM_ID_OR_PATH&gt; \\\n  --decision &quot;&lt;English decision text&gt;&quot; \\\n  --source &quot;_kano/backlog/topics/&lt;topic-name&gt;/synthesis/&lt;file&gt;.md&quot; \\\n  --agent &lt;agent-id&gt; \\\n  --product &lt;product&gt;\nThis appends the decision under a ## Decisions section in the item body (creating it if needed) and adds a Worklog entry.\nClose and Cleanup\nkano-backlog topic close &lt;topic-name&gt; --agent &lt;agent-name&gt;\nkano-backlog topic cleanup --ttl-days 14\nkano-backlog topic cleanup --ttl-days 14 --apply\nClosing marks the topic as closed; cleanup removes raw materials after TTL (and may optionally delete closed topics depending on implementation flags).\nExport Context Bundle\nkano-backlog topic export-context &lt;topic-name&gt; [--format markdown|json]\nExports topic context as a bundle:\n\nLoads summaries of all seed items (title, state, type)\nIncludes content from pinned documents\nOutput is deterministic (sorted, consistent formatting)\nUse --format json for machine parsing\n\nList Topics\nkano-backlog topic list [--agent &lt;agent-name&gt;] [--format plain|json]\nLists all topics:\n\nShows item count and pinned doc count\nMarks active topic (if --agent specified)\nShows last updated timestamp\n\nCommon Workflows\nAgent-first (Conversational) Workflows\nThis repo is conversational-first: docs should include both CLI usage and copy/paste prompts that a human can say to an AI agent.\nUse this consistent format:\n\nSay this to your agent\nThe agent will do\nExpected output\n\nCreate a Topic and Gather Context\nSay this to your agent:\r\n‚ÄúCreate a topic named  for , add items &lt;ITEM_1&gt;, &lt;ITEM_2&gt;, pin &lt;DOC_PATH&gt;, then distill a generated brief.‚Äù\nThe agent will do:\n\nkano-backlog topic create &lt;topic-name&gt; --agent &lt;agent-id&gt;\nkano-backlog topic add &lt;topic-name&gt; --item &lt;ITEM_1&gt; (repeat for other items)\nkano-backlog topic pin &lt;topic-name&gt; --doc &lt;DOC_PATH&gt; (optional)\nkano-backlog topic distill &lt;topic-name&gt;\n\nExpected output:\n\n_kano/backlog/topics/&lt;topic-name&gt;/manifest.json updated with seed items/pinned docs\n_kano/backlog/topics/&lt;topic-name&gt;/brief.generated.md regenerated deterministically\n\nAudit Decision Write-back for a Topic\nSay this to your agent:\r\n‚ÄúRun a decision write-back audit for topic  and show which work items are missing decisions. Save the report under publish/.‚Äù\nThe agent will do:\n\nkano-backlog topic decision-audit &lt;topic-name&gt; --format plain\n\nExpected output:\n\n_kano/backlog/topics/&lt;topic-name&gt;/publish/decision-audit.md\n\nWrite Back a Decision to a Work Item\nSay this to your agent:\r\n‚ÄúWrite back this decision to &lt;ITEM_ID&gt; and cite the synthesis file as the source: ‚Äò&lt;DECISION_TEXT&gt;‚Äô.‚Äù\nThe agent will do:\n\nkano workitem add-decision &lt;ITEM_ID&gt; \\ --decision &quot;&lt;DECISION_TEXT&gt;&quot; \\ --source &quot;_kano/backlog/topics/&lt;topic-name&gt;/synthesis/&lt;file&gt;.md&quot; \\ --agent &lt;agent-id&gt; \\ --product &lt;product&gt;\n\nExpected output:\n\nThe work item markdown is updated with a ## Decisions section (created if missing)\nA new Worklog entry is appended documenting the decision write-back\n\nSetting Up a Focus Area\n# 1. Create topic for your work area\nkano-backlog topic create auth-refactor --agent kiro\n \n# 2. Add related items\nkano-backlog topic add auth-refactor --item TASK-0042\nkano-backlog topic add auth-refactor --item TASK-0043\nkano-backlog topic add auth-refactor --item BUG-0012\n \n# 3. Pin relevant documents\nkano-backlog topic pin auth-refactor --doc _kano/backlog/decisions/ADR-0015.md\nkano-backlog topic pin auth-refactor --doc docs/auth-design.md\n \n# 4. Switch to the topic\nkano-backlog topic switch auth-refactor --agent kiro\nContext Switching\n# Check current topics\nkano-backlog topic list --agent kiro\n \n# Switch to different focus area\nkano-backlog topic switch payment-flow --agent kiro\n \n# Export context for agent consumption\nkano-backlog topic export-context payment-flow --format json\nLoading Context into Agent\n# Export as markdown for human review\nkano-backlog topic export-context auth-refactor\n \n# Export as JSON for programmatic use\nkano-backlog topic export-context auth-refactor --format json\nIntegration with Worksets\nTopics and worksets work together:\n\nTopic: Groups related items/docs/snippets and provides a deterministic brief\nWorkset: Per-item execution cache (plan/notes/deliverables) while implementing a specific Task/Bug\n\nTypical flow:\n# Switch to topic\nkano-backlog topic switch auth-refactor --agent kiro\n \n# Initialize workset for specific item\nkano-backlog workset init --item TASK-0042 --agent kiro\n \n# Work on item with workset\nkano-backlog workset next --item TASK-0042\n \n# When done, switch topic or continue with next item\nGit Ignore\nWorksets are stored in .cache/ and should be ignored.\nTopic raw materials are treated as cache; in this demo repo we ignore:\n_kano/backlog/**/.cache/\n_kano/backlog/topics/**/materials/\nRelated\n\nWorkset - Per-item execution cache\nWorkset docs: docs/workset.md\n"},"skill/docs/workset":{"title":"workset","links":["skill/docs/topic"],"tags":[],"content":"Workset (Execution Cache)\nWorkset is a derived, ephemeral execution cache used while an agent is working on a Task/Bug.\r\nIt is not the source of truth: canonical work items and ADRs remain the SSOT.\nGoals\n\nPrevent ‚Äúagent drift‚Äù during longer tasks\nProvide an execution checklist and a place to capture notes/deliverables\nMake promotion back to canonical artifacts explicit (worklog, ADRs, attachments)\n\nPrinciples\n\nWorkset data is derived and discardable (TTL cleanup is expected)\nGit must not track workset files\nPromote load-bearing information back to canonical items/ADRs\n\nDirectory Layout\nPer ADR-0011:\n_kano/backlog/.cache/worksets/items/&lt;item-id&gt;/\n  meta.json       # Metadata: agent, timestamps, TTL, source paths\n  plan.md         # Checklist derived from acceptance criteria\n  notes.md        # Working notes (use Decision: markers for ADR promotion)\n  deliverables/   # Files to promote to canonical artifacts\nCLI Commands\nAll workset commands are accessed via kano-backlog workset &lt;subcommand&gt;.\nInitialize a Workset\nkano-backlog workset init --item &lt;id&gt; --agent &lt;agent-name&gt; [--ttl-hours 72] [--format plain|json]\nCreates a workset for the specified item:\n\nGenerates meta.json with agent, timestamps, and TTL\nCreates plan.md from item‚Äôs acceptance criteria\nCreates notes.md with Decision: marker guidance\nCreates empty deliverables/ directory\nAppends worklog entry to source item\n\nIf workset already exists, returns existing path (idempotent).\nRefresh from Canonical\nkano-backlog workset refresh --item &lt;id&gt; --agent &lt;agent-name&gt; [--format plain|json]\nUpdates workset metadata from canonical files:\n\nVerifies source item still exists\nUpdates refreshed_at timestamp in meta.json\nAppends worklog entry to source item\n\nGet Next Action\nkano-backlog workset next --item &lt;id&gt; [--format plain|json]\nReturns the next unchecked step from plan.md:\n\nParses checkbox items (- [ ] and - [x])\nReturns step number and description\nReturns completion message if all steps are checked\n\nPromote Deliverables\nkano-backlog workset promote --item &lt;id&gt; --agent &lt;agent-name&gt; [--dry-run] [--format plain|json]\nPromotes files from deliverables/ to canonical artifacts:\n\nCopies files to _kano/backlog/products/&lt;product&gt;/artifacts/&lt;item-id&gt;/\nAppends worklog entry summarizing promoted files\nUse --dry-run to preview without making changes\n\nCleanup Expired Worksets\nkano-backlog workset cleanup [--ttl-hours 72] [--agent &lt;agent-name&gt;] [--dry-run] [--format plain|json]\nDeletes worksets older than TTL:\n\nOnly affects worksets under .cache/worksets/items/\nReports count and space reclaimed\nUse --dry-run to preview without deleting\n\nList Worksets\nkano-backlog workset list [--format plain|json]\nLists all item worksets with metadata:\n\nItem ID and agent\nAge and size\nTTL setting\n\nDetect ADR Candidates\nkano-backlog workset detect-adr --item &lt;id&gt; [--format plain|json]\nScans notes.md for Decision: markers:\n\nExtracts decision text\nSuggests ADR title\nUse --format json for automation\n\nCommon Workflows\nStarting Work on a Task\n# 1. Initialize workset\nkano-backlog workset init --item TASK-0042 --agent kiro\n \n# 2. Check what to do first\nkano-backlog workset next --item TASK-0042\n \n# 3. Work on the task, updating plan.md as you go\n# 4. Add notes with Decision: markers for important decisions\nCompleting a Task\n# 1. Check all steps are done\nkano-backlog workset next --item TASK-0042\n# Output: &quot;‚úì All steps complete!&quot;\n \n# 2. Promote any deliverables\nkano-backlog workset promote --item TASK-0042 --agent kiro --dry-run\nkano-backlog workset promote --item TASK-0042 --agent kiro\n \n# 3. Check for ADR candidates\nkano-backlog workset detect-adr --item TASK-0042\nMaintenance\n# List all worksets\nkano-backlog workset list\n \n# Preview cleanup\nkano-backlog workset cleanup --ttl-hours 48 --dry-run\n \n# Run cleanup\nkano-backlog workset cleanup --ttl-hours 48\nADR Promotion\nWhen Decision: markers are found in notes.md:\n\nRun kano-backlog workset detect-adr --item &lt;id&gt; to find candidates\nCreate ADR using kano adr create ...\nLink ADR to item via decisions: frontmatter\nWorklog entry is appended automatically\n\nGit Ignore\nEnsure cache paths are ignored:\n_kano/**/.cache/\n_kano/backlog/**/.cache/\nRelated\n\nTopic Context - Higher-level context grouping\nADR-0011: Workset vs GraphRAG separation\nADR-0012: Workset DB uses canonical schema\n"},"skill/references/bases":{"title":"bases","links":[],"tags":[],"content":"Obsidian Bases (plugin-free views)\nThis skill currently ships Dataview examples and a plugin-free view generator. If you want to reduce plugin dependencies, Obsidian Bases can replace many Dataview table-style dashboards.\nNote: Bases is an Obsidian core feature, but it is newer than Dataview and may have limitations depending on your Obsidian version and platform.\nWhat Bases can cover well\n\nTable-style lists of items from a folder (e.g. _kano/backlog/items)\nFiltering by simple frontmatter fields (e.g. type, state, priority, area, iteration)\nSorting and grouping (where supported by your Bases version)\n\nWhat might still require alternatives\n\nMore complex computed fields\nParent/child tree rendering from parent links (Dataview is still better here)\nCustom JS logic (DataviewJS-only)\n\nFor tree/MOC, keep Epic .index.md files and Obsidian wikilinks in the body.\nRecommended frontmatter conventions for Bases\nTo keep Bases filtering predictable, prefer:\n\nScalars: id, type, title, state, priority, parent, area, iteration, created, updated\nArrays: tags, decisions\n\nAvoid relying on nested objects for filtering. Keep nested objects only for external references if needed.\nCreate a Base for ‚ÄúInProgress Work‚Äù\nIn Obsidian:\n\nCreate a new Base\nSet the source folder to _kano/backlog/items\nAdd columns: id, type, title, state, priority, parent, area, iteration, updated\nAdd a filter:\n\nstate is one of: Proposed, Planned, Ready, InProgress, Review, Blocked\n(Optionally hide Done and Dropped)\n\n\nSave the Base in your vault\n\nKeep a zero-plugin fallback\nEven if you adopt Bases, keep these generator commands as a no-plugin fallback for sharing/CI artifacts:\n\npython skills/kano-agent-backlog-skill/scripts/backlog/view_generate.py --groups &quot;New,InProgress&quot; --title &quot;InProgress Work&quot; --output _kano/backlog/views/Dashboard_PlainMarkdown_Active.md\npython skills/kano-agent-backlog-skill/scripts/backlog/view_generate.py --groups &quot;New&quot; --title &quot;New Work&quot; --output _kano/backlog/views/Dashboard_PlainMarkdown_New.md\npython skills/kano-agent-backlog-skill/scripts/backlog/view_generate.py --groups &quot;Done&quot; --title &quot;Done Work&quot; --output _kano/backlog/views/Dashboard_PlainMarkdown_Done.md\n"},"skill/references/context_graph":{"title":"context_graph","links":[],"tags":[],"content":"Context Graph (Derived) and Graph-assisted retrieval\nThis project is file-first: Markdown work items and ADRs are the canonical source of truth.\nA Context Graph is a derived representation of how those artifacts relate, designed to help agents retrieve and assemble the right context without prompt bloat.\nWhat we mean by ‚ÄúContext Graph‚Äù\nA Context Graph is a directed, typed graph:\n\nNodes: artifacts (WorkItem, ADR, Worklog entry, chunks, code files, commits)\nEdges: explicit relationships (parent, decision_ref, relates, blocks, blocked_by, mentions, etc.)\n\nThis is a weak graph first approach:\n\nv1 uses only explicit, structured relationships already present in frontmatter or known file conventions\nno LLM-based entity extraction\nno server/MCP requirement (local-first)\n\nWhy it matters (vs vector-only RAG)\nVector/FTS retrieval finds ‚Äúsimilar‚Äù text, but can miss:\n\nthe parent chain (task ‚Üí story ‚Üí feature ‚Üí epic)\nthe ADR that explains the decision\nthe blockers/depends chain\n\nGraph-assisted retrieval uses vector/FTS to find seed nodes, then expands along known edges to pull in the load-bearing neighbors.\nMinimal Graph-assisted retrieval flow\n\nSeed retrieval\n\nFTS/embeddings return top-N chunks/nodes (seed set)\n\n\nGraph expansion\n\nexpand via allowlisted edge types\nlimit traversal depth (k-hop) and fanout\n\n\nRe-rank\n\nprioritize ADR decision sections and item title/acceptance\ndownweight noisy worklog-only matches\n\n\nContext packing\n\nassemble ‚Äúseed + neighbors‚Äù into a compact, traceable context pack\n\n\n\nSuggested node / edge model (v1)\nNode types (initial):\n\nwork_item\nadr\nchunk (optional, for embedding/fts indexing)\n\nEdge types (initial):\n\nparent (child ‚Üí parent)\ndecision_ref (work_item ‚Üí adr)\nrelates (work_item ‚Üí work_item)\nblocks / blocked_by\n\nStorage (derived)\nTwo equivalent ways to store the derived graph:\n\nReuse the SQLite index: materialize edges into a links-like table and query it for traversal\nSidecar JSONL: graph_nodes.jsonl + graph_edges.jsonl under &lt;backlog-root&gt;/_index/\n\nEither way:\n\ngraph artifacts must be safe to delete\nthe build must be repeatable from canonical Markdown (or from the SQLite index derived from Markdown)\n\nConfiguration knobs\nRecommended config keys (names indicative; see product ADR for final schema):\n\nretrieval.mode: file_scan | sqlite | hybrid\nretrieval.graph.enabled: boolean\nretrieval.graph.k_hop: int\nretrieval.graph.edge_allowlist: list\nretrieval.weights: doctype/section/state weights\n\nReferences\n\nProduct ADR (planned): Graph-assisted retrieval with a derived Context Graph\nreferences/indexing.md for the derived indexing layer\n"},"skill/references/embedding_pipeline":{"title":"embedding_pipeline","links":[],"tags":[],"content":"Embedding Pipeline Configuration\nThe embedding pipeline enables semantic search and similarity analysis across backlog items through vector embeddings. This document describes the TOML configuration schema for the pipeline components.\nConfiguration Structure\nThe embedding pipeline configuration consists of four main sections:\n\n[chunking] - Text segmentation settings\n[tokenizer] - Token counting and budget management\n[embedding] - Vector embedding generation\n[vector] - Vector storage and retrieval\n\n[chunking] Section\nControls how documents are split into chunks for processing.\n[chunking]\ntarget_tokens = 256      # Target tokens per chunk\nmax_tokens = 512         # Maximum tokens per chunk (hard limit)\noverlap_tokens = 32      # Token overlap between adjacent chunks\nversion = &quot;chunk-v1&quot;     # Chunking algorithm version\nParameters\n\ntarget_tokens (integer, default: 256): Preferred number of tokens per chunk. The chunker aims for this size but may create smaller chunks at natural boundaries.\nmax_tokens (integer, default: 512): Hard limit on chunk size. Chunks exceeding this limit will be truncated.\noverlap_tokens (integer, default: 32): Number of tokens to overlap between consecutive chunks to preserve context across boundaries.\nversion (string, default: ‚Äúchunk-v1‚Äù): Algorithm version identifier for reproducible chunking behavior.\n\nConstraints\n\noverlap_tokens must be less than target_tokens\ntarget_tokens must be less than or equal to max_tokens\nAll values must be positive integers\n\n[tokenizer] Section\nConfigures token counting for budget management and chunking.\n[tokenizer]\nadapter = &quot;heuristic&quot;              # Tokenizer adapter type\nmodel = &quot;text-embedding-3-small&quot;   # Model for token counting\nmax_tokens = 8192                  # Maximum tokens per model context (optional)\nParameters\n\nadapter (string, default: ‚Äúheuristic‚Äù): Tokenizer implementation\n\n&quot;heuristic&quot;: Fast approximation based on character counts and language detection\n&quot;tiktoken&quot;: Precise tokenization using OpenAI‚Äôs tiktoken library\n\n\nmodel (string, default: ‚Äútext-embedding-3-small‚Äù): Model identifier for token counting rules\nmax_tokens (integer, optional): Override model‚Äôs default context limit\n\nAvailable Models\nCommon models and their default token limits:\n\ntext-embedding-3-small: 8,192 tokens\ntext-embedding-3-large: 8,192 tokens\ntext-embedding-ada-002: 8,192 tokens\n\n[embedding] Section\nConfigures vector embedding generation.\n[embedding]\nprovider = &quot;noop&quot;           # Embedding provider\nmodel = &quot;noop-embedding&quot;    # Embedding model\ndimension = 1536            # Vector dimension\nParameters\n\nprovider (string, default: ‚Äúnoop‚Äù): Embedding service provider\n\n&quot;noop&quot;: Testing provider that generates random vectors\n&quot;openai&quot;: OpenAI embedding API\n\n\nmodel (string): Model identifier for embedding generation\ndimension (integer, default: 1536): Vector dimension size\n\nProvider-Specific Models\nNoOp Provider (for testing):\n\nnoop-embedding: Generates consistent random vectors\n\nOpenAI Provider:\n\ntext-embedding-3-small: 1536 dimensions\ntext-embedding-3-large: 3072 dimensions\ntext-embedding-ada-002: 1536 dimensions\n\nAdditional Options\nProvider-specific options can be added under [embedding.options]:\n[embedding.options]\napi_key = &quot;sk-...&quot;          # OpenAI API key (for openai provider)\nbase_url = &quot;https://...&quot;    # Custom API endpoint\ntimeout = 30                # Request timeout in seconds\n[vector] Section\nConfigures vector storage and retrieval.\n[vector]\nbackend = &quot;sqlite&quot;      # Vector storage backend\npath = &quot;.cache/vector&quot;  # Storage path (relative to product root)\ncollection = &quot;backlog&quot;  # Collection/table name\nmetric = &quot;cosine&quot;       # Distance metric for similarity\nParameters\n\nbackend (string, default: ‚Äúsqlite‚Äù): Vector storage implementation\n\n&quot;noop&quot;: In-memory storage for testing\n&quot;sqlite&quot;: SQLite-based persistent storage\n\n\npath (string, default: ‚Äú.cache/vector‚Äù): Storage location\n\nRelative paths are resolved from the product root\nAbsolute paths are used as-is\n\n\ncollection (string, default: ‚Äúbacklog‚Äù): Collection or table name for organizing vectors\nmetric (string, default: ‚Äúcosine‚Äù): Distance metric for similarity calculations\n\n&quot;cosine&quot;: Cosine similarity (recommended for most text embeddings)\n&quot;l2&quot;: Euclidean distance\n&quot;ip&quot;: Inner product\n\n\n\nBackend-Specific Options\nAdditional options can be configured under [vector.options]:\n[vector.options]\n# SQLite-specific options\ntimeout = 10.0              # Connection timeout\njournal_mode = &quot;WAL&quot;        # SQLite journal mode\nConfiguration Examples\nTesting/Development Profile\nMinimal configuration for local development and testing:\n[chunking]\ntarget_tokens = 256\nmax_tokens = 512\noverlap_tokens = 32\n \n[tokenizer]\nadapter = &quot;heuristic&quot;\nmodel = &quot;text-embedding-3-small&quot;\n \n[embedding]\nprovider = &quot;noop&quot;\nmodel = &quot;noop-embedding&quot;\ndimension = 1536\n \n[vector]\nbackend = &quot;sqlite&quot;\npath = &quot;.cache/vector&quot;\ncollection = &quot;backlog&quot;\nmetric = &quot;cosine&quot;\nProduction Profile with OpenAI\nConfiguration for production use with OpenAI embeddings:\n[chunking]\ntarget_tokens = 512\nmax_tokens = 1024\noverlap_tokens = 64\n \n[tokenizer]\nadapter = &quot;tiktoken&quot;\nmodel = &quot;text-embedding-3-small&quot;\nmax_tokens = 8192\n \n[embedding]\nprovider = &quot;openai&quot;\nmodel = &quot;text-embedding-3-small&quot;\ndimension = 1536\n \n[embedding.options]\napi_key = &quot;${OPENAI_API_KEY}&quot;\ntimeout = 30\n \n[vector]\nbackend = &quot;sqlite&quot;\npath = &quot;.cache/vector&quot;\ncollection = &quot;backlog&quot;\nmetric = &quot;cosine&quot;\nHigh-Capacity Configuration\nFor large documents and detailed analysis:\n[chunking]\ntarget_tokens = 1024\nmax_tokens = 2048\noverlap_tokens = 128\n \n[tokenizer]\nadapter = &quot;tiktoken&quot;\nmodel = &quot;text-embedding-3-large&quot;\nmax_tokens = 8192\n \n[embedding]\nprovider = &quot;openai&quot;\nmodel = &quot;text-embedding-3-large&quot;\ndimension = 3072\n \n[vector]\nbackend = &quot;sqlite&quot;\npath = &quot;.cache/vector&quot;\ncollection = &quot;backlog&quot;\nmetric = &quot;cosine&quot;\nConfiguration Loading\nThe pipeline configuration is loaded through the standard config system:\n\nBase configuration: Default values from PipelineConfig class\nProduct config: Values from _kano/backlog/products/{product}/config.toml\nEnvironment overrides: Environment variables (if supported)\n\nConfig File Location\nPlace embedding configuration in your product‚Äôs config file:\n_kano/backlog/products/{product}/config.toml\n\nValidation\nThe configuration is validated when loaded:\n\nRequired fields must be present\nNumeric values must be within valid ranges\nProvider and model combinations must be supported\nTokenizer and embedding models must be compatible\n\nDebugging Configuration\nUse the CLI to inspect the effective configuration:\nkano-backlog config show --product {product}\nCLI Integration\nThe embedding pipeline integrates with the CLI through these commands:\n# Build index for entire product\nkano-backlog embedding build --product {product}\n \n# Index specific file\nkano-backlog embedding build /path/to/file.md --product {product}\n \n# Index raw text\nkano-backlog embedding build --text &quot;content&quot; --source-id &quot;doc-1&quot; --product {product}\n \n# Query the index\nkano-backlog embedding query &quot;search terms&quot; --product {product}\n \n# Check index status\nkano-backlog embedding status --product {product}\nPerformance Considerations\nChunking Strategy\n\nSmaller chunks (256-512 tokens): Better for precise matching, more chunks to process\nLarger chunks (1024+ tokens): Better for context preservation, fewer API calls\n\nToken Budgets\n\nSet max_tokens based on your embedding model‚Äôs context limit\nUse overlap_tokens to preserve context across chunk boundaries\nMonitor token usage to optimize costs with paid embedding providers\n\nVector Storage\n\nSQLite backend scales to millions of vectors for local-first use\nUse appropriate metric for your embedding model (cosine for most text embeddings)\nConsider storage location (path) for performance and backup requirements\n\nTroubleshooting\nCommon Issues\n\nConfiguration validation errors: Check parameter types and ranges\nModel compatibility: Ensure tokenizer and embedding models are compatible\nAPI authentication: Verify API keys and endpoints for external providers\nStorage permissions: Ensure write access to the configured vector path\nToken limits: Verify chunk sizes don‚Äôt exceed model context limits\n\nDebug Commands\n# Validate configuration\nkano-backlog config validate --product {product}\n \n# Test embedding pipeline\nkano-backlog embedding build --text &quot;test&quot; --source-id &quot;test&quot; --product {product}\n \n# Check vector backend status\nkano-backlog embedding status --product {product}"},"skill/references/indexing":{"title":"indexing","links":[],"tags":[],"content":"Indexing (Optional)\nThis skill is file-first: the source of truth is Markdown files under _kano/backlog/.\nIndexing is an optional, rebuildable layer that exists to make agents and humans faster at:\n\nfinding relevant items quickly (filters, sorting, parent/child traversal),\nproducing views/reports reproducibly,\npowering later retrieval workflows (e.g. embeddings/RAG).\n\nThe index must never become a write-path requirement for normal operation.\nWhat is indexed\nThe SQLite index is derived from:\n\nItems: _kano/backlog/items/**/*.md\nDecisions/ADRs: _kano/backlog/decisions/**/*.md (linked via item frontmatter decisions)\n\nSchema references:\n\nreferences/indexing_schema.sql\nreferences/indexing_schema.json\n\nArtifacts (where generated files live)\nGenerated artifacts should be treated as build outputs:\n\nDefault location: &lt;product-root&gt;/.cache/ (e.g. _kano/backlog/products/&lt;product&gt;/.cache/)\nTest/experiments: _kano/backlog_sandbox/\n\nThis demo repo gitignores index artifacts:\n\n_kano/backlog/products/*/.cache/\n_kano/backlog_sandbox/\n\nConfig keys (index.*)\nIndexing is disabled by default (file-first remains the default workflow).\nIn _kano/backlog/products/&lt;product&gt;/_config/config.toml:\n[index]\nenabled = false\nbackend = &quot;sqlite&quot;\npath = null\nmode = &quot;rebuild&quot;\n\nindex.enabled: feature flag (default false)\nindex.backend: sqlite (default) or postgres (optional/future)\nindex.path: DB file path override; null uses &lt;product-root&gt;/.cache/index.sqlite3\nindex.mode: rebuild or incremental (best-effort)\n\nBuild / rebuild workflow (SQLite)\nBuild the SQLite index from files:\n# Build index for specific product\nkano-backlog admin index build --product &lt;product-name&gt;\n \n# Build all product indexes\nkano-backlog admin index build\n \n# Force rebuild even if exists\nkano-backlog admin index build --product &lt;product-name&gt; --force\n \n# Build with vector index\nkano-backlog admin index build --product &lt;product-name&gt; --vectors\nRefresh (incremental update):\n# Refresh specific product\nkano-backlog admin index refresh --product &lt;product-name&gt;\n \n# Refresh all products\nkano-backlog admin index refresh\nCheck index status:\n# Status for specific product\nkano-backlog admin index status --product &lt;product-name&gt;\n \n# Status for all products\nkano-backlog admin index status\nSafety guarantees:\n\nThe indexer must not modify any source Markdown files.\nDeleting the DB is safe; it can always be rebuilt.\n\nVector Search Integration\nThe index system integrates with the embedding pipeline for semantic search:\n# Build vector index alongside SQLite index\nkano-backlog admin index build --product &lt;product-name&gt; --vectors\n \n# Search using vector similarity\nkano-backlog search query &quot;your search text&quot; --product &lt;product-name&gt;\n \n# Check embedding status\nkano-backlog embedding status --product &lt;product-name&gt;\nObsidian views (file-first mode)\nFile-first dashboards continue to work normally (Dataview or generated Markdown views), because items remain Markdown files.\nOptional (planned): generate Markdown dashboards from DB queries to reduce dependence on Obsidian plugins.\nIf you enable index.enabled=true, kano-backlog view refresh --source auto can use the SQLite index\r\nwhen present, and falls back to file scan when the DB is missing.\nContext graph (Graph-assisted retrieval)\nA context graph is a derived, structured view of how artifacts relate (items, ADRs, dependencies, refs).\r\nIt enables Graph-assisted retrieval: retrieve seed nodes via FTS/embeddings, then expand via graph edges (k-hop)\r\nto assemble a higher-quality context pack.\n\nConcept/spec: references/context_graph.md\n\nDB-first is out of scope\nUsing a database as the source of truth would require new UI/export tooling and changes to the human-in-the-loop workflow.\r\nThis skill intentionally keeps DB usage as an optional, derived index layer."},"skill/references/logging":{"title":"logging","links":[],"tags":[],"content":"Audit Logging (Agent Tool Invocations)\nLog location (default)\n\nRoot: _kano/backlog/_shared/logs/agent_tools/ (cross-product shared)\nFile: tool_invocations.jsonl\n\nLog format (JSONL)\nEach line is a JSON object:\n{\r\n  &quot;version&quot;: 1,\r\n  &quot;timestamp&quot;: &quot;2026-01-04T10:50:12Z&quot;,\r\n  &quot;tool&quot;: &quot;shell_command&quot;,\r\n  &quot;cwd&quot;: &quot;D:/_work/_Kano/kano-agent-backlog-skill-demo&quot;,\r\n  &quot;status&quot;: &quot;ok&quot;,\r\n  &quot;exit_code&quot;: 0,\r\n  &quot;duration_ms&quot;: 123,\r\n  &quot;command_args&quot;: [&quot;python&quot;, &quot;script.py&quot;, &quot;--flag&quot;, &quot;***&quot;],\r\n  &quot;replay_command&quot;: &quot;python script.py --flag ***&quot;,\r\n  &quot;notes&quot;: &quot;optional&quot;\r\n}\n\nRequired fields\n\nversion\ntimestamp\ntool\ncwd\nstatus\ncommand_args\nreplay_command\n\nOptional fields\n\nexit_code\nduration_ms\nnotes\nerror\n\nRedaction rules\nRedact any value that is likely sensitive. Defaults include:\n\nFlags: --token, --api-key, --secret, --password, --passwd, --pwd,\r\n--client-secret, --access-key, --authorization, --bearer, --cookie\nKey-value pairs: token=..., api_key=..., secret=..., password=...\nEnv-style keys (case-insensitive): *_TOKEN, *_KEY, *_SECRET, *_PASSWORD\n\nRedaction replaces the value with *** while leaving the flag/key intact.\nRotation and retention (defaults)\n\nRotate when file size exceeds 5 MB.\nKeep the last 10 rotated files.\nFile naming: tool_invocations.jsonl, tool_invocations.1.jsonl, ‚Ä¶\n\nThese values can be made configurable later, but the defaults must exist.\nScript integration\nBacklog and filesystem scripts call scripts/logging/audit_runner.py at\r\nentrypoint so every invocation appends an audit log entry.\nEnvironment overrides\nSet these environment variables to adjust logging without code changes:\n\nKANO_AUDIT_LOG_DISABLED=1 to disable audit logging.\nKANO_AUDIT_LOG_ROOT to override the log directory.\nKANO_AUDIT_LOG_FILE to override the log filename.\nKANO_AUDIT_LOG_MAX_BYTES to override rotation size (bytes).\nKANO_AUDIT_LOG_MAX_FILES to override the number of rotated files kept.\n\nConfig defaults\nLogging scripts read defaults from _kano/backlog/_config/config.json:\n{\n  &quot;log&quot;: {\n    &quot;verbosity&quot;: &quot;info&quot;,\n    &quot;debug&quot;: false\n  }\n}\nVerbosity behavior:\n\ndebug: log all tool invocations\ninfo: log successful + failed invocations\nwarn / warning: log only warnings/errors (currently: failures)\nerror: log only failures\noff / none / disabled: disable audit logging\n\nPrecedence: environment overrides &gt; config defaults."},"skill/references/processes":{"title":"processes","links":[],"tags":[],"content":"Process Profiles\nProcess profiles define work item types, states, and transitions for the local\r\nbacklog. They are intended to be human-readable and easy to adjust for agent\r\nworkflows.\nSuggested schema (TOML)\n# Built-in Azure Boards Agile profile for kano-agent-backlog-skill.\nid = &quot;builtin/azure-boards-agile&quot;\nname = &quot;Azure Boards Agile&quot;\ndescription = &quot;Default Agile-like workflow for agent-managed backlog items.&quot;\ndefault_state = &quot;Proposed&quot;\nstates = [\n    &quot;Proposed&quot;,\n    &quot;Planned&quot;,\n    &quot;Ready&quot;,\n    &quot;InProgress&quot;,\n    &quot;Review&quot;,\n    &quot;Blocked&quot;,\n    &quot;Done&quot;,\n    &quot;Dropped&quot;\n]\nterminal_states = [&quot;Done&quot;, &quot;Dropped&quot;]\n \n[[work_item_types]]\ntype = &quot;Epic&quot;\nslug = &quot;epic&quot;\n \n[[work_item_types]]\ntype = &quot;Feature&quot;\nslug = &quot;feature&quot;\n \n[[work_item_types]]\ntype = &quot;UserStory&quot;\nslug = &quot;userstory&quot;\n \n[[work_item_types]]\ntype = &quot;Task&quot;\nslug = &quot;task&quot;\n \n[[work_item_types]]\ntype = &quot;Bug&quot;\nslug = &quot;bug&quot;\n \n[transitions]\nProposed = [&quot;Planned&quot;, &quot;Dropped&quot;]\nPlanned = [&quot;Ready&quot;, &quot;Dropped&quot;]\nReady = [&quot;InProgress&quot;, &quot;Dropped&quot;]\nInProgress = [&quot;Review&quot;, &quot;Blocked&quot;, &quot;Dropped&quot;]\nReview = [&quot;Done&quot;, &quot;InProgress&quot;]\nBlocked = [&quot;InProgress&quot;, &quot;Dropped&quot;]\nDone = []\nDropped = []\nNotes\n\nwork_item_types should align with item frontmatter type.\nstates should align with state values used in items.\nKeep transitions permissive for agent autonomy; tighten only if needed.\n\nState semantics (standard set)\nThese semantics apply to built-ins that use the default KABSD state set:\n\nProposed: not ready to start; needs more discovery/confirmation.\nPlanned: approved for the plan; detail refinement can proceed, but not started.\nReady: Ready gate passed (typically for Task/Bug before start).\nInProgress: work started.\nBlocked: work started but blocked.\nReview: work complete pending review/verification.\nDone: work complete and accepted.\nDropped: work intentionally stopped.\n\nConfig selection\nUse process.profile and process.path in _kano/backlog/_config/config.toml\r\nto choose a built-in profile or a custom file.\nUse scripts/backlog/process_linter.py to verify item folders match the active\r\nprocess profile and optionally create missing folders.\nBuilt-in profiles\n\nreferences/processes/azure-boards-agile.toml ‚Üí builtin/azure-boards-agile\nreferences/processes/scrum.toml ‚Üí builtin/scrum\nreferences/processes/cmmi.toml ‚Üí builtin/cmmi\nreferences/processes/jira-default.toml ‚Üí builtin/jira-default\n\nCustom profiles\nUse the template as a starting point and store it under your backlog config\r\narea (recommended: _kano/backlog/_config/processes/), then point\r\nprocess.path to that file.\nTemplate:\n\nreferences/processes/template.toml\n\nExample config:\n[process]\npath = &quot;_kano/backlog/_config/processes/custom.toml&quot;"},"skill/references/schema":{"title":"schema","links":[],"tags":[],"content":"Backlog Schema\nProcess-defined types and states\nItem types and states come from the active process profile. See\r\nreferences/processes.md and the profile selected via\r\n_kano/backlog/_config/config.json (process.profile or process.path).\nWhen scripts or docs need workflow details, they should load the profile\r\nspecified in config (built-in or custom) instead of hardcoding the list.\nParent rules (default)\n\nEpic ‚Üí Feature\nFeature ‚Üí UserStory\nUserStory ‚Üí Task or Bug\nFeature ‚Üí Bug (allowed)\nTask ‚Üí Task (optional sub-task)\nEpic has no parent\n\nThese defaults align with built-in profiles; custom processes may define\r\ndifferent parent relationships.\nParent state sync (forward-only)\nWhen a child item state changes, parents can auto-advance forward-only:\n\nNever downgrade parent state automatically.\nNever change child states based on parent edits.\nReady/Planned children advance parents to Planned (not Ready).\nAny InProgress/Review/Blocked child advances parent to InProgress.\nAll Done ‚áí parent Done; all Dropped ‚áí parent Dropped; mix Done/Dropped ‚áí parent Done.\n\nReady gate (required, non-empty)\nTo move to Ready, each item must include:\n\nContext\nGoal\nApproach\nAcceptance Criteria\nRisks / Dependencies\n\nFile naming\n\n&lt;ID&gt;_&lt;slug&gt;.md\nSlug: ASCII, hyphen-separated\nID prefixes:\n\nKABSD-EPIC-\nKABSD-FTR-\nKABSD-USR-\nKABSD-TSK-\nKABSD-BUG-\n\n\nPrefix derivation:\n\nSource: config/profile.env ‚Üí PROJECT_NAME.\nSplit on non-alphanumeric separators and camel-case boundaries, take first letters.\nIf only one letter, use the first letter plus the next consonant (A/E/I/O/U skipped).\nIf still short, use the first two letters.\nUppercase the result (example: kano-agent-backlog-skill-demo ‚Üí KABSD).\n\n\nStore files under _kano/backlog/items/&lt;type&gt;/&lt;bucket&gt;/ by item type.\nBucket names use the lower bound of each 100 range:\n\n0000, 0100, 0200, ‚Ä¶\n\n\nFor Epic, create &lt;ID&gt;_&lt;slug&gt;.index.md in the same folder.\n\nFrontmatter (minimum)\n---\r\nid: KABSD-TSK-0001\r\ntype: Task\r\ntitle: &quot;Short title&quot;\r\nstate: Proposed\r\npriority: P2\r\nparent: KABSD-USR-0001\r\narea: general\r\niteration: null\r\ntags: []\r\ncreated: 2026-01-02\r\nupdated: 2026-01-02\r\nowner: null\r\nexternal:\r\n  azure_id: null\r\n  jira_key: null\r\nlinks:\r\n  relates: []\r\n  blocks: []\r\n  blocked_by: []\r\ndecisions: []\r\n---\n\nImmutable fields\n\nid, type, created must not be changed after creation.\n\nConfig defaults (baseline)\nBaseline config lives at _kano/backlog/_config/config.json and defaults to:\n{\r\n  &quot;log&quot;: { &quot;verbosity&quot;: &quot;info&quot;, &quot;debug&quot;: false },\r\n  &quot;process&quot;: { &quot;profile&quot;: &quot;builtin/azure-boards-agile&quot;, &quot;path&quot;: null },\r\n  &quot;sandbox&quot;: { &quot;root&quot;: &quot;_kano/backlog_sandbox&quot; },\r\n  &quot;index&quot;: { &quot;enabled&quot;: false, &quot;backend&quot;: &quot;sqlite&quot;, &quot;path&quot;: null, &quot;mode&quot;: &quot;rebuild&quot; }\r\n}\n\nConfig overrides (environment)\n\nKANO_BACKLOG_CONFIG_PATH: override config file path (must be under _kano/backlog or _kano/backlog_sandbox).\nAudit log env overrides (highest precedence over config defaults):\n\nKANO_AUDIT_LOG_DISABLED\nKANO_AUDIT_LOG_ROOT\nKANO_AUDIT_LOG_FILE\nKANO_AUDIT_LOG_MAX_BYTES\nKANO_AUDIT_LOG_MAX_FILES\n\n\n\nParent reference format (collision-safe)\n\nSame-product parent: use the display ID in parent (e.g., KABSD-FTR-0002).\nParent is intentionally intra-product: it powers hierarchy and parent state sync; cross-product ‚Äúparent‚Äù relationships are usually not desired.\nCross-product relationships should be expressed via links.relates, links.blocks, links.blocked_by instead of parent.\n\nFor collision-safe references in links/content, use id@uidshort (e.g., KABSD-FTR-0002@019b8f52) or full uid (019b8f52-9fde-7162-bd19-e9b8310526fc).\n\n\n\nValidation\n\nThe workitem_update_state.py script validates that parent resolves uniquely within the current product.\nIf unresolved or ambiguous, the script aborts with guidance to keep parent intra-product and use links.* (with id@uidshort / full uid) for cross-product relationships.\n\nRationale\n\nDisplay IDs are human-friendly and stable for day-to-day work within a product.\nParent drives hierarchy and state propagation; limiting it to a product keeps behavior predictable.\nCross-product collisions are expected in a multi-product platform; disambiguated refs in links ensure correctness without sacrificing readability in the common case.\n"},"skill/references/templates":{"title":"templates","links":[],"tags":[],"content":"Templates\nWork item template\nPlace the file under _kano/backlog/items/&lt;type&gt;/.\r\nUse the ID prefix derived from config/profile.env ‚Üí PROJECT_NAME (example: kano-agent-backlog-skill-demo ‚Üí KABSD).\n---\r\nid: KABSD-TSK-0001\r\ntype: Task\r\ntitle: &quot;Short title&quot;\r\nstate: Proposed\r\npriority: P2\r\nparent: KABSD-USR-0001\r\narea: general\r\niteration: null\r\ntags: []\r\ncreated: 2026-01-02\r\nupdated: 2026-01-02\r\nowner: null\r\nexternal:\r\n  azure_id: null\r\n  jira_key: null\r\nlinks:\r\n  relates: []\r\n  blocks: []\r\n  blocked_by: []\r\ndecisions: []\r\n---\r\n\r\n# Context\r\n\r\n# Goal\r\n\r\n# Non-Goals\r\n\r\n# Approach\r\n\r\n# Alternatives\r\n\r\n# Acceptance Criteria\r\n\r\n# Risks / Dependencies\r\n\r\n# Worklog\r\n\r\n2026-01-02 10:00 [agent=&lt;AGENT_NAME&gt;] Created from discussion: &lt;summary&gt;.\n\nImportant: Replace &lt;AGENT_NAME&gt; with your actual agent identity. See SKILL.md ‚ÄúAgent Identity Determination‚Äù for how to determine your identity. NEVER use example values like codex, antigravity, or auto.\nWorklog line format\nYYYY-MM-DD HH:MM [agent=&lt;AGENT_NAME&gt;] &lt;message&gt;\r\nYYYY-MM-DD HH:MM [agent=&lt;AGENT_NAME&gt;] [model=&lt;MODEL_NAME&gt;] &lt;message&gt;\n\nAgent Identity: Provide the actual runtime agent identity explicitly in Worklog entries; do not copy placeholders or examples. See SKILL.md for details.\nModel (Optional): When available, include the model used by the agent (e.g., claude-sonnet-4.5, gpt-5.1, gemini-3.0-high). This provides additional context for audit trails and debugging.\nADR template\n---\r\nid: ADR-0001\r\ntitle: &quot;Decision title&quot;\r\nstatus: Proposed\r\ndate: 2026-01-02\r\nrelated_items: []\r\nsupersedes: null\r\nsuperseded_by: null\r\n---\r\n\r\n# Decision\r\n\r\n# Context\r\n\r\n# Options Considered\r\n\r\n# Pros / Cons\r\n\r\n# Consequences\r\n\r\n# Follow-ups\n"},"skill/references/views":{"title":"views","links":[],"tags":[],"content":"Views (Obsidian Dataview)\nIf you want fewer plugin dependencies, consider Obsidian Bases as a plugin-free alternative for table-style dashboards. See references/bases.md.\nTree by parent\nUse Dataview or DataviewJS to build parent/child views from parent fields.\r\nAvoid encoding hierarchy in folders.\r\nQueries should target _kano/backlog/items to include all item subfolders.\r\nHide Done/Dropped items in views by default (view-level archive).\nExample: List Epic items\ntable id, state, priority, iteration\nfrom &quot;_kano/backlog/items&quot;\nwhere type = &quot;Epic&quot; and state != &quot;Done&quot; and state != &quot;Dropped&quot;\nsort created asc\nExample: Items by iteration\ntable id, type, state, priority\nfrom &quot;_kano/backlog/items&quot;\nwhere iteration != null and state != &quot;Done&quot; and state != &quot;Dropped&quot;\nsort iteration asc, priority asc\nInProgress view (no Dataview required)\nGenerate plain Markdown lists (no Dataview required).\nscripts/backlog/view_generate.py supports two data sources:\n\nFile scan (default file-first behavior)\nSQLite index (when index.enabled=true and the DB exists)\n\nWhen --source auto is used, it prefers SQLite when available, otherwise falls back to scanning files.\npython scripts/backlog/view_generate.py --source auto --groups &quot;New,InProgress&quot; --title &quot;InProgress Work&quot; --output _kano/backlog/views/Dashboard_PlainMarkdown_Active.md\npython scripts/backlog/view_generate.py --source auto --groups &quot;New&quot; --title &quot;New Work&quot; --output _kano/backlog/views/Dashboard_PlainMarkdown_New.md\npython scripts/backlog/view_generate.py --source auto --groups &quot;Done&quot; --title &quot;Done Work&quot; --output _kano/backlog/views/Dashboard_PlainMarkdown_Done.md\nOr refresh all standard dashboards (and optionally refresh the SQLite index first):\npython scripts/backlog/view_refresh_dashboards.py --backlog-root _kano/backlog --agent &lt;agent-name&gt;\nOutputs:\n\n_kano/backlog/views/Dashboard_PlainMarkdown_Active.md (New + InProgress/Review/Blocked)\n_kano/backlog/views/Dashboard_PlainMarkdown_New.md (New only)\n_kano/backlog/views/Dashboard_PlainMarkdown_Done.md (Done + Dropped)\n\nDemo: DBIndex vs NoDBIndex\nIf you want to show the difference between:\n\nNoDBIndex: file scan only\nDBIndex: query SQLite index (optional, derived)\n\nGenerate both variants by running the same view generator with different --source values:\npython scripts/backlog/view_generate.py --source files --groups &quot;New,InProgress&quot; --title &quot;InProgress Work (Demo: NoDBIndex)&quot; --output _kano/backlog/views/_demo/Dashboard_Demo_NoDBIndex_Active.md\npython scripts/backlog/view_generate.py --source sqlite --groups &quot;New,InProgress&quot; --title &quot;InProgress Work (Demo: DBIndex)&quot; --output _kano/backlog/views/_demo/Dashboard_Demo_DBIndex_Active.md\nOr use the dedicated generator (preferred):\r\npython scripts/backlog/view_generate_demo.py --backlog-root _kano/backlog --agent &lt;agent-name&gt;\nIf you only want a tag view:\npython scripts/backlog/view_generate_tag.py --backlog-root _kano/backlog --source auto --tags \\&quot;versioning,release\\&quot; --output _kano/backlog/views/_demo/Dashboard_Demo_Tags_Versioning.md --agent &lt;agent-name&gt;"},"skill/references/workflow":{"title":"workflow","links":[],"tags":[],"content":"Workflow SOP\nA) Planning (discussion ‚Üí tickets)\n\nCreate or update Epic for the milestone.\nSplit into Features (capabilities).\nSplit into UserStories (user perspective).\nSplit into Tasks/Bugs (single focused coding sessions).\nFill Ready gate sections for each Task/Bug.\nAppend Worklog entry: ‚ÄúCreated from discussion: ‚Ä¶‚Äù (scripts require --agent).\n\nB) Ready gate\n\nMove to Ready only after required sections are complete.\nNo code changes until the item is Ready.\n\nC) Execution\n\nSet state to InProgress.\nAppend Worklog for important decisions or changes.\nIf a decision is architectural, create ADR and link it:\n\nAdd ADR id to item decisions: []\nAppend Worklog entry referencing the ADR\n\n\n\nConflict Guard\n\nOwner Locking: Items in InProgress are locked to their owner.\nAuto-Assignment: When moving to InProgress, if no owner is set, you become the owner.\nCollaboration: To hand off work, the current owner must change the owner field or move the item out of InProgress (e.g. to Review or Planned).\n\nD) Completion\n\nMove state to Review ‚Üí Done.\nAppend a Worklog summary with:\n\nWhat changed\nRelated items and ADRs\n\n\n\nD.1) Parent sync (forward-only)\n\nWhen a child state changes, parents can be auto-advanced forward-only.\nParent edits never force child states.\nUse --no-sync-parent if you need to keep parent state unchanged for a manual re-plan.\n\nE) Scope change\n\nDo not rewrite a ticket into a different task.\nSplit into a new ticket and link via links.relates.\nAppend a Worklog entry explaining the split.\n\nF) File operations\n\nUse scripts/backlog/* or scripts/fs/* for backlog/skill artifacts.\nScripts only operate under _kano/backlog/ or _kano/backlog_sandbox/ to keep audit logs clean.\n"},"skill/templates/AGENTS.block":{"title":"AGENTS.block","links":[],"tags":[],"content":"\nProject backlog discipline (kano-agent-backlog-skill)\n\nUse {{SKILL_ROOT}}/SKILL.md for any planning/backlog work.\nBacklog root is {{BACKLOG_ROOT}} (items are file-first; index/logs are derived).\nBefore any code change, create/update items in {{BACKLOG_ROOT}}/items/ (Epic ‚Üí Feature ‚Üí UserStory ‚Üí Task/Bug).\nEnforce the Ready gate on Task/Bug before starting; Worklog is append-only.\nUse the kano CLI (not ad-hoc edits) so audit logs capture actions:\n\nBootstrap: python {{SKILL_ROOT}}/scripts/kano-backlog admin init --product &lt;name&gt; --agent &lt;agent-name&gt;\nCreate/update: python {{SKILL_ROOT}}/scripts/kano-backlog workitem create|update-state ... --agent &lt;agent-name&gt;\nViews: python {{SKILL_ROOT}}/scripts/kano-backlog view refresh --agent &lt;agent-name&gt; --product &lt;name&gt;\n\n\nDashboards auto-refresh after item changes by default (views.auto_refresh=true); use --no-refresh or set it to false if needed.\n\n"},"skill/templates/CLAUDE.block":{"title":"CLAUDE.block","links":[],"tags":[],"content":"\nBacklog workflow (kano-agent-backlog-skill)\n\nSkill entrypoint: {{SKILL_ROOT}}/SKILL.md\nBacklog root: {{BACKLOG_ROOT}}\nBefore coding, create/update backlog items and meet the Ready gate.\nWorklog is append-only; record decisions and state changes.\nPrefer running the kano CLI so actions are auditable (and dashboards stay current):\n\npython {{SKILL_ROOT}}/scripts/kano-backlog admin init --product &lt;name&gt; --agent &lt;agent-name&gt;\npython {{SKILL_ROOT}}/scripts/kano-backlog workitem create|update-state ... --agent &lt;agent-name&gt; [--product &lt;name&gt;]\npython {{SKILL_ROOT}}/scripts/kano-backlog view refresh --agent &lt;agent-name&gt; --product &lt;name&gt;\n\n\nDashboards auto-refresh after item changes by default (views.auto_refresh=true); use --no-refresh or set it to false if needed.\n\n"},"skill/templates/snapshot_report_developer":{"title":"snapshot_report_developer","links":[],"tags":[],"content":"Developer Snapshot Report: {{scope}}\nScope: {{meta.scope}}\r\nVCS: branch={{meta.vcs.branch}}, revno={{meta.vcs.revno}}, hash={{meta.vcs.hash}}, dirty={{meta.vcs.dirty}}, provider={{meta.vcs.provider}}\nImplementation Status (Capabilities)\nThis section maps backlog features to their implementation evidence.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeatureStatusEvidence{{#each capabilities}}{{feature}}{{status}}{{#each evidence_refs}}- {{this}}{{/each}}{{/each}}\nTechnical Debt &amp; Stubs\nThis section lists known incomplete implementations (TODOs, FIXMEs, NotImplementedError).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTypeLocationMessageTicket{{#each stub_inventory}}{{type}}{{file}}:{{line}}{{message}}{{ticket_ref}}{{/each}}\nCLI Surface\nRoot Command: {{cli_tree.[0].name}}\n\n\n                  \n                  NOTE\n                  \n                \n\nAll status claims above are backed by repo evidence. partial status indicates presence of stubs or work-in-progress markers linked to the feature.\n\n"},"skill/templates/snapshot_report_pm":{"title":"snapshot_report_pm","links":[],"tags":[],"content":"Product Manager Snapshot Report: {{scope}}\nScope: {{meta.scope}}\r\nVCS: branch={{meta.vcs.branch}}, revno={{meta.vcs.revno}}, hash={{meta.vcs.hash}}, dirty={{meta.vcs.dirty}}, provider={{meta.vcs.provider}}\nFeature Delivery Status\nOverview of feature implementation based on repository evidence.\nDone / In Review\n{{#each capabilities}}\r\n{{#if (eq status ‚Äúdone‚Äù)}}\n\n {{feature}}\n\nEvidence: {{#each evidence_refs}}{{this}}; {{/each}}\r\n{{/if}}\r\n{{/each}}\n\n\n\nIn Progress / Partial\n{{#each capabilities}}\r\n{{#if (eq status ‚Äúpartial‚Äù)}}\n\n[/] {{feature}}\n\nStatus: Partial / In Progress\nEvidence: {{#each evidence_refs}}{{this}}; {{/each}}\r\n{{/if}}\r\n{{/each}}\n\n\n\nNot Started / Missing\n{{#each capabilities}}\r\n{{#if (eq status ‚Äúmissing‚Äù)}}\n\n {{feature}}\r\n{{/if}}\r\n{{/each}}\n\nKnown Risks (Stubs)\nThe following items have explicit code markers indicating incomplete work:\n{{#each stub_inventory}}\n\n{{type}} in {{file}}: ‚Äú{{message}}‚Äù {{#if ticket_ref}}(Ticket: {{ticket_ref}}){{/if}}\r\n{{/each}}\n"},"skill/templates/snapshot_report_qa":{"title":"snapshot_report_qa","links":[],"tags":[],"content":"QA Snapshot Report: {{scope}}\nScope: {{meta.scope}}\r\nVCS: branch={{meta.vcs.branch}}, revno={{meta.vcs.revno}}, hash={{meta.vcs.hash}}, dirty={{meta.vcs.dirty}}, provider={{meta.vcs.provider}}\nTestability &amp; Evidence\nFeatures that report ‚ÄúDone‚Äù status and their associated evidence.\n{{#each capabilities}}\n{{feature}}\n\nStatus: {{status}}\nTest Evidence References:\r\n{{#each evidence_refs}}\n\n{{this}}\r\n{{/each}}\r\n{{#unless evidence_refs}}\nNo specific evidence found.\r\n{{/unless}}\r\n{{/each}}\n\n\n\nCLI Surface (Test Scope)\nThe following command structure is exposed in the CLI and requires testing:\nRoot: {{cli_tree.[0].name}} ({{cli_tree.[0].help}})\n(Note: Recursive tree listing would go here in fully expanded report)\nHealth Check\nEnvironment health status:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheckPassedMessage{{#each health}}{{name}}{{passed}}{{message}}{{/each}}"}}